[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About\nBook\nWorkshops\nTalks\nR-Ladies\nPapers\n\n\n\nSTATISTICIAN | ACTUARY | DATA SCIENTIST\n‚ÄúI‚Äôve always loved maths and science, I meant to be a Veterinary, I wanted to do ballet classes, teach gymnastic, be an economist, a psychologist‚Ä¶ Many years passed since I started the journey of my career as an investigator, it turned out to be an everyday challenge with tasks to solve, and most importantly a continuous learning path. Each day brings with it a fresh set of challenges to solve and opportunities for growth. It‚Äôs a path marked by curiosity, resilience, and a relentless pursuit of excellence‚Äîa journey that I‚Äôm grateful to be on.‚Äù\nFederica Gazzelloni is an Actuary, and a Data Scientist with a focus on health metrics, machine learning, and data visualization. With experience spanning corporate, academic, and research roles, Federica has developed a robust skill set that bridges actuarial science, statistical modeling, and public health.\nFederica began her career as an actuary, working in corporate and academic settings where she focused on quantitative analysis, risk modeling, and compliance in the insurance and pensions industry. As a research-oriented actuary, she not only applied advanced actuarial principles but also developed a deep understanding of the statistical methods. Federica taught mathematics to high school students and instructed university students in computer science, helping to cultivate the next generation of data-driven professionals.\nIn recent years, Federica has expanded her focus to health data modeling, particularly in the context of infectious disease research. As the world faced the Covid-19 pandemic, her expertise in statistical methods and health metrics became even more relevant. Collaborating with the Institute for Health Metrics and Evaluation (IHME), Federica contributed to global health studies that aimed to understand the spread and impact of Covid-19, as well as other pressing health issues. This experience inspired her to create a practical manual for health data analysis, which serves as a resource for professionals and students alike who wish to explore health metrics and epidemiological modeling in-depth. Her book, to be published by CRC Press, combines foundational knowledge with practical applications, including R code for real-world case studies in health metrics.\nFederica‚Äôs role in the pre-publication stages of GBD research enables her to directly contribute to some of the most impactful global health research being conducted today. This work not only requires technical acumen in statistical modeling but also a keen eye for detail and a commitment to accuracy, given the far-reaching implications of public health data.\nBeyond her work in health metrics, Federica is an active member of the open-source community, where she contributes to several organizations dedicated to education and software development. She is a Certified Carpentries Instructor, delivering workshops that empower learners with foundational data science skills in R, Python, and other tools. Federica has taught workshops for prestigious organizations, including the Helmholtz Information & Data Science Academy, the University of Washington, and the Centers for Disease Control and Prevention (CDC), where she introduced participants to key concepts in programming, data wrangling, and statistical analysis. Her teaching style is characterized by clarity, engagement, and inclusivity, making technical content accessible to learners of all backgrounds and skill levels.\nFederica also serves as the Lead Organizer for R-Ladies Rome, a chapter of the global R-Ladies organization that promotes gender diversity in the R programming and data science communities. Since its founding in 2023, R-Ladies Rome has grown significantly under her leadership, reaching a large and engaged audience through online and in-person events. Through R-Ladies, Federica has organized a range of activities, from tutorials on data visualization to workshops on advanced R packages, all aimed at fostering a supportive and inclusive community for women and underrepresented groups in data science. Notable events include a session with Hadley Wickham, Chief Scientist at Posit PBC, and an upcoming workshop on building reproducible data pipelines, which reflects the group‚Äôs commitment to high-quality, practical learning opportunities.\nIn addition to her work with R-Ladies Rome, Federica has collaborated with other R user groups globally, including R-Ladies New York, R-Ladies Paris, and the TunisR User Group. These partnerships have expanded the reach of her initiatives, creating a more interconnected and supportive global R community. Federica is also involved with Bioconductor and the R Consortium, furthering her commitment to open-source development and collaborative learning.\nOne of Federica‚Äôs key strengths is her ability to use data visualization as a tool for communication and insight. Her background in data visualization allows her to create compelling visual representations of complex health data, making it easier for diverse audiences to grasp intricate statistical relationships. Whether through static graphs or interactive dashboards, Federica‚Äôs visualizations aim to tell a story, uncover trends, and empower decision-makers with actionable insights.\nThroughout her career, Federica has consistently demonstrated a commitment to knowledge sharing and community building. Her work is informed by a belief in the power of data to drive positive change, especially in fields like public health where informed decision-making can save lives. Her contributions to education, open-source software, and health metrics research underscore her dedication to making data science an inclusive and impactful field.\n\n\n\nHealth Metrics and the Spread of Infectious Diseases: Machine Learning Applications and Spatial Modelling Analysis with R\nüîó https://bookdown.org/fede_gazzelloni/hmsidR/\nEditor CRC Press - Chapman & Hall\n\nDiscover the fascinating world of health metrics and infectious disease analysis with our upcoming manual and textbook! Whether you‚Äôre new to the field or a seasoned practitioner, this resource offers valuable insights into key concepts and practical applications.\nLearn about essential public health metrics like DALYs, YLL, and YLD, which provide crucial indicators of a population‚Äôs health status. Dive into recent infectious disease outbreaks, including the impact of Covid19, and explore the most affected locations.\nWith a mix of explanatory text and hands-on R code examples, you‚Äôll become proficient in analyzing real-world case studies. We‚Äôll compare health metrics across different locations and test predictive models using tidymodels and INLA. Plus, you‚Äôll discover the power of spatial visualization techniques using ggplot2, leaflet, sf, rgdal, and other popular R packages. Whether you‚Äôre a beginner or an advanced practitioner, this book is designed to enhance your understanding and skills in health data analysis.\nIdeal for early-stage practitioners and graduate students in public health, this resource is your guide to unlocking the full potential of R for spatial and health metrics analysis.\n\n\n\nCDC Centers for Disease Control\nData Analysis and Visualization with R\nDate: 2024-08-19 to 2024-08-20 | Centers for Disease Control\nWebsite: https://rrlove-cdc.github.io/2024-08-19-cdc-online/\nEtherPad: https://pad.carpentries.org/2024-08-19-cdc-online\n\nData Carpentry Genomics\nDate: 2024-06-10 to 2024-06-13 | Centers for Disease Control\nWebsite: https://fgazzelloni.github.io/2024-06-10-cdc-online/\nEtherPad: https://pad.carpentries.org/2024-06-10-cdc-online\nUniversity of Washington\nNetwork of the National Library of Medicine (NNLM) Region 5 Library Carpentry\nDate: March 19-21 2024 | Network of the National Library of Medicine (NNLM)\nWebsite: https://nnlm-ncds.github.io/2024-03-19-nnlm-uw-online/\nEtherPad: https://pad.carpentries.org/2024-03-19-nnlm-uw-online\nHelmholtz Information & Data Science Academy\nSoftware Carpentry (Shell, Git, and programming with R)\nDate: February 12-13 2024 | Helmholtz Online\nWebsite: https://macrobiotus.github.io/2024-02-12-helmholtz-online/\nEtherPad: https://pad.carpentries.org/2024-02-12-helmholtz-online\n\n\n\n\nList of my talks\n\nShinyConf2024 - Shiny4Good Talk\nDate: April 2024\nAbstract: This talk will offer insights gained from active participation in the Data Science Learning Community through participation in Shiny book clubs. By exploring the unique dynamics of these forums, I will highlight the collaborative and knowledge-sharing atmosphere that the Data Science Learning Community cultivates. The talk will draw examples from the book clubs to illustrate how practical discussions and exercises amplify the learning experience.\n#beginner #Shiny4Good #shinyapp #showcase #rstats #datascience\n\n\n\n\n\n\nSharing the mission with the open source community\n\nR-Ladies DC\nDate: September 2022\nAbstract: This tutorial is meant for people new to spatial analysis and modeling with RStudio but comfortable in making simple data visualization with ggplot2. In this video, you will learn how to make a map with RStudio, and how to use data modeling for making spatial model analysis.\nMaterial: It is helpful to have the following R packages installed beforehand: {tidyverse}, {ggthemes}, {maptools}, {ggmap}, {sf}, {spocc}, {dismo}, {SpatialEpi}, and {oregonfrogs} dataset from\nremotes::install_github(\"fgazzelloni/oregonfrogs\")\n\nGitHub Repo: https://github.com/Fgazzelloni/How-to-Spatial-Modeling-with-R\n\nBook: https://fgazzelloni.github.io/How-to-Spatial-Modeling-with-R\n\n\n\nR-Ladies NYC & R-Ladies Rome\nDate: April 2023\nAbstract: In this video, you will learn about Modeling infectious diseases with R using both deterministic and Bayesian SIR model methods. We will explore both Deterministic and Bayesian SIR model methods, and learn how to use the well-known SIR model to understand how epidemics unfold and how to prevent their spread.\nThis video is perfect for students of science, healthcare professionals, or anyone who wants to gain a deeper understanding of how to use R for Modeling infectious diseases.\nAgenda:\n\nIntroduction presentation of the Chapters and R-Ladies Global action\nSIR model with R - quick intro assessment (Speaker Federica Gazzelloni)\nBayesian workflow for disease transmission (Speaker Jacqueline Buros)\nQ&A session\n\nMaterial:\n\nGitHub Repo: https://github.com/Fgazzelloni/sir-model-with-R/\n\n\n\nR-Ladies Cambridge\nDate: March 2024\nAbstract: In this video, you‚Äôll learn how to use ggplot2 to replicate one of the ongoing #DuboisChallenge2024 plates. The original plates are part of W.E.B. Du Bois‚Äôs legacy from back in 1900, showcased at the Paris Exposition. I‚Äôll be using modern tools such as R. We‚Äôll explore the colors used in the plates and delve into the intricacies of the challenge by understanding the perspective of hand-made graphs in the ‚ÄòDuboisian‚Äô style.\nMaterial:\n\nGitHub Repo: https://github.com/Fgazzelloni/R-Ladies-Cambridge-Dataviz-lunch-Replicating-Du-Bois-with-R\n\nWebSite Collection: https://fgazzelloni.quarto.pub/unlocking-the-power-of-data-visualization-with-r/duboischallenge/\n\n\n\nR-Ladies NYC & R-Ladies Rome\nDate: June 2024\nAbstract: In this video, you‚Äôll learn how to make a website in R with Quarto. We‚Äôll explore the basics of Quarto and how to publish a website. The tutorial is designed to be accessible to beginners, with no prior experience required. By the end of the session, you will have a fully functional website that you can use to showcase your projects and achievements.\nMaterial:\n\nGithub repository: https://github.com/Fgazzelloni/building_a_website_in_r\n\nFinal version of the Website: https://fgazzelloni.quarto.pub/my-website-in-r‚Äìquarto/\n\n\n\nResources:\n\nQuarto Docs: https://quarto.org/docs/websites/\n\nCSS customization for your website: https://www.w3schools.com/css/\n\nBackground image (change - ‚Äúpink‚Äù with your favorite pick): https://www.pexels.com/search/pink/\n\n\n\n\nCo-authored Publications\n\n\nWarning: package 'ggplot2' was built under R version 4.4.1\n\n\nWarning: package 'lubridate' was built under R version 4.4.1\n\n\n\n\n\nList of Co-Authored Publications\n\nyear\njournal\ntitle\nauthor\npubid\n\n\n\n2025\nThe Lancet Public Health\nChanging life expectancy in European countries 1990‚Äì2021: a subanalysis of causes and risk factors from the Global Burden of Disease Study 2021\nN Steel, CMM Bauer-Staeb, JA Ford, C Abbafati, MA Abdalla, ...\naqlVkmm33-oC\n\n\n2025\nElsevier\nChanging life expectancy in European countries 1990‚Äì2021: a subanalysis of causes and risk factors from the Global Burden of Disease Study 2021\nS Nicholas, CMM Bauer-Staeb, JA Ford, C Abbafati, MA Abdalla, ...\nWp0gIr-vW9MC\n\n\n2024\nThe Lancet\nGlobal age-sex-specific mortality, life expectancy, and population estimates in 204 countries and territories and 811 subnational locations, 1950‚Äì2021, and the impact of the¬†‚Ä¶\nAE Schumacher, HH Kyu, A Aali, C Abbafati, J Abbas, ...\n8k81kl-MbHgC\n\n\n2024\nThe lancet\nGlobal fertility in 204 countries and territories, 1950‚Äì2021, with forecasts to 2100: a comprehensive demographic analysis for the Global Burden of Disease Study 2021\nNV Bhattacharjee, AE Schumacher, A Aali, YH Abate, R Abbasgholizadeh, ...\n5nxA0vEk-isC\n\n\n2024\nThe Lancet\nGlobal, regional, and national stillbirths at 20 weeks' gestation or longer in 204 countries and territories, 1990‚Äì2021: findings from the Global Burden of Disease Study 2021\nH Comfort, TA McHugh, AE Schumacher, A Harris, EA May, KR Paulson, ...\nM3ejUd6NZC8C\n\n\n2024\nBMC public health\nThe state of health in the European Union (EU-27) in 2019: a systematic analysis for the Global Burden of Disease study 2019\nJV Santos, A Padron-Monedero, B Bikbov, DA Grad, D Plass, EA Mechili, ...\nkNdYIx-mwKoC\n\n\n2024\nEuropean Journal of Public Health\nExploring Burden of Disease Metrics in EU legislation: A Systematic Analysis\nJ Chen-Xu, DA Grad, N Mahrouseh, S Cuschieri, F Gazzelloni, EA Mechili, ...\n4TOpqqG69KYC\n\n\n2024\nBritish Actuarial Journal\nLoss modelling from first principles\nP Parodi, D Thrumble, P Watson, Z Ji, A Wang, I Bhatia, J Lees, S Mealy, ...\nUebtZRa9Y70C\n\n\n2023\nEpidemiology & Infection\nBurden of infectious disease studies in Europe and the United Kingdom: a review of methodological design choices\nP Charalampous, JA Haagsma, LS Jakobsen, V Gorasso, I Noguer, ...\nufrVoPGSRksC\n\n\n2023\nArchives of public health\nBurden of disease attributable to risk factors in European countries: a scoping literature review\nV Gorasso, JN Morgado, P Charalampous, SM Pires, JA Haagsma, ...\n0EnyYjriUFMC\n\n\n2023\nArchives of Public Health\nBurden of disease attributable to risk factors in European countries: a scoping literature review\nV Gorasso, JN Morgado, P Charalampous, SM Pires, JA Haagsma, ...\nhqOjcs7Dif8C\n\n\n2022\nEuropean Journal of Public Health\nBurden of non-communicable disease studies in Europe: a systematic review of data sources and methodological choices\nP Charalampous, V Gorasso, D Plass, SM Pires, E Von Der Lippe, ...\nW7OEmFMy1HYC\n\n\n2022\nBMC public health\nMethodological considerations in injury burden of disease studies across Europe: a systematic literature review\nP Charalampous, E Pallari, V Gorasso, E Von der Lippe, ...\nWF5omc3nYNoC\n\n\n2021\nEuropean Journal of Public Health\nThe state of health in the European Union in 2019\nJ Vasco Santos, A Padron Monedero, B Bikbov, DA Grad, D Plass, ...\neQOLeE2rZwMC\n\n\n2021\nEuropean Journal of Public Health\nThe state of health in the European Union in 2019\nJV Santos, AP Monedero, B Bikbov, DA Grad, D Plass, EA Mechili, ...\nYsMSGLbcyi4C\n\n\nNA\nChapman and Hall/CRC\nHealth Metrics and the Spread of Infectious Diseases: Machine Learning Applications and Spatial Modelling Analysis with R\nF Gazzelloni\n4DMP91E08xMC\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "about/book/index.html",
    "href": "about/book/index.html",
    "title": "Federica Gazzelloni's Website",
    "section": "",
    "text": "Health Metrics and the Spread of Infectious Diseases: Machine Learning Applications and Spatial Modelling Analysis with R\nüîó https://bookdown.org/fede_gazzelloni/hmsidR/\n\nEditor CRC Press - Chapman & Hall\n\n\nDiscover the fascinating world of health metrics and infectious disease analysis with our upcoming manual and textbook! Whether you‚Äôre new to the field or a seasoned practitioner, this resource offers valuable insights into key concepts and practical applications.\nLearn about essential public health metrics like DALYs, YLL, and YLD, which provide crucial indicators of a population‚Äôs health status. Dive into recent infectious disease outbreaks, including the impact of Covid19, and explore the most affected locations.\nWith a mix of explanatory text and hands-on R code examples, you‚Äôll become proficient in analyzing real-world case studies. We‚Äôll compare health metrics across different locations and test predictive models using tidymodels and INLA. Plus, you‚Äôll discover the power of spatial visualization techniques using ggplot2, leaflet, sf, rgdal, and other popular R packages. Whether you‚Äôre a beginner or an advanced practitioner, this book is designed to enhance your understanding and skills in health data analysis.\nIdeal for early-stage practitioners and graduate students in public health, this resource is your guide to unlocking the full potential of R for spatial and health metrics analysis.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "about/talks/index.html",
    "href": "about/talks/index.html",
    "title": "Federica Gazzelloni's Website",
    "section": "",
    "text": "ShinyConf2024 - Shiny4Good Talk\nDate: April 2024\nAbstract: This talk will offer insights gained from active participation in the Data Science Learning Community through participation in Shiny book clubs. By exploring the unique dynamics of these forums, I will highlight the collaborative and knowledge-sharing atmosphere that the Data Science Learning Community cultivates. The talk will draw examples from the book clubs to illustrate how practical discussions and exercises amplify the learning experience.\n#beginner #Shiny4Good #shinyapp #showcase #rstats #datascience\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "about/papers/index.html",
    "href": "about/papers/index.html",
    "title": "Federica Gazzelloni's Website",
    "section": "",
    "text": "Co-authored Publications\n\n\nWarning: package 'ggplot2' was built under R version 4.4.1\n\n\nWarning: package 'lubridate' was built under R version 4.4.1\n\n\n\n\n\nList of Co-Authored Publications\n\nyear\njournal\ntitle\nauthor\npubid\n\n\n\n2025\nThe Lancet Public Health\nChanging life expectancy in European countries 1990‚Äì2021: a subanalysis of causes and risk factors from the Global Burden of Disease Study 2021\nN Steel, CMM Bauer-Staeb, JA Ford, C Abbafati, MA Abdalla, ...\naqlVkmm33-oC\n\n\n2025\nElsevier\nChanging life expectancy in European countries 1990‚Äì2021: a subanalysis of causes and risk factors from the Global Burden of Disease Study 2021\nS Nicholas, CMM Bauer-Staeb, JA Ford, C Abbafati, MA Abdalla, ...\nWp0gIr-vW9MC\n\n\n2024\nThe Lancet\nGlobal age-sex-specific mortality, life expectancy, and population estimates in 204 countries and territories and 811 subnational locations, 1950‚Äì2021, and the impact of the¬†‚Ä¶\nAE Schumacher, HH Kyu, A Aali, C Abbafati, J Abbas, ...\n8k81kl-MbHgC\n\n\n2024\nThe lancet\nGlobal fertility in 204 countries and territories, 1950‚Äì2021, with forecasts to 2100: a comprehensive demographic analysis for the Global Burden of Disease Study 2021\nNV Bhattacharjee, AE Schumacher, A Aali, YH Abate, R Abbasgholizadeh, ...\n5nxA0vEk-isC\n\n\n2024\nThe Lancet\nGlobal, regional, and national stillbirths at 20 weeks' gestation or longer in 204 countries and territories, 1990‚Äì2021: findings from the Global Burden of Disease Study 2021\nH Comfort, TA McHugh, AE Schumacher, A Harris, EA May, KR Paulson, ...\nM3ejUd6NZC8C\n\n\n2024\nBMC public health\nThe state of health in the European Union (EU-27) in 2019: a systematic analysis for the Global Burden of Disease study 2019\nJV Santos, A Padron-Monedero, B Bikbov, DA Grad, D Plass, EA Mechili, ...\nkNdYIx-mwKoC\n\n\n2024\nEuropean Journal of Public Health\nExploring Burden of Disease Metrics in EU legislation: A Systematic Analysis\nJ Chen-Xu, DA Grad, N Mahrouseh, S Cuschieri, F Gazzelloni, EA Mechili, ...\n4TOpqqG69KYC\n\n\n2024\nBritish Actuarial Journal\nLoss modelling from first principles\nP Parodi, D Thrumble, P Watson, Z Ji, A Wang, I Bhatia, J Lees, S Mealy, ...\nUebtZRa9Y70C\n\n\n2023\nEpidemiology & Infection\nBurden of infectious disease studies in Europe and the United Kingdom: a review of methodological design choices\nP Charalampous, JA Haagsma, LS Jakobsen, V Gorasso, I Noguer, ...\nufrVoPGSRksC\n\n\n2023\nArchives of public health\nBurden of disease attributable to risk factors in European countries: a scoping literature review\nV Gorasso, JN Morgado, P Charalampous, SM Pires, JA Haagsma, ...\n0EnyYjriUFMC\n\n\n2023\nArchives of Public Health\nBurden of disease attributable to risk factors in European countries: a scoping literature review\nV Gorasso, JN Morgado, P Charalampous, SM Pires, JA Haagsma, ...\nhqOjcs7Dif8C\n\n\n2022\nEuropean Journal of Public Health\nBurden of non-communicable disease studies in Europe: a systematic review of data sources and methodological choices\nP Charalampous, V Gorasso, D Plass, SM Pires, E Von Der Lippe, ...\nW7OEmFMy1HYC\n\n\n2022\nBMC public health\nMethodological considerations in injury burden of disease studies across Europe: a systematic literature review\nP Charalampous, E Pallari, V Gorasso, E Von der Lippe, ...\nWF5omc3nYNoC\n\n\n2021\nEuropean Journal of Public Health\nThe state of health in the European Union in 2019\nJ Vasco Santos, A Padron Monedero, B Bikbov, DA Grad, D Plass, ...\neQOLeE2rZwMC\n\n\n2021\nEuropean Journal of Public Health\nThe state of health in the European Union in 2019\nJV Santos, AP Monedero, B Bikbov, DA Grad, D Plass, EA Mechili, ...\nYsMSGLbcyi4C\n\n\nNA\nChapman and Hall/CRC\nHealth Metrics and the Spread of Infectious Diseases: Machine Learning Applications and Spatial Modelling Analysis with R\nF Gazzelloni\n4DMP91E08xMC\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "content/stats/index.html",
    "href": "content/stats/index.html",
    "title": "Statistics",
    "section": "",
    "text": "Understanding Plain Vanilla - from Scratch\n\n\n\nrstats\n\nmodeling\n\nmachine-learning\n\nartificial-neural-networks\n\n\n\n\n\n\n\n\n\nJun 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nR-Ladies Events Stats\n\n\n\ntext-analysis\n\n\n\n\n\n\n\n\n\nMar 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHow does geom_smooth() make predictions\n\n\n\nrstats\n\nmodeling\n\nlinear regression\n\ngeom_smooth\n\n\n\n\n\n\n\n\n\nFeb 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Simpson‚Äôs Paradox: A Simple Explanation\n\n\n\nrstats\n\nmodeling\n\n\n\n\n\n\n\n\n\nJan 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Statistics model comparison\n\n\n\nrstats\n\nmodeling\n\nbayesian\n\n\n\n\n\n\n\n\n\nMar 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nPredictive modeling\n\n\n\nmodeling\n\n\n\n\n\n\n\n\n\nJun 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nMultiple Hypothesis Testing\n\n\n\n\n\n\n\n\nMay 20, 2022\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "content/stats/statistics/multiple_testing/index.html",
    "href": "content/stats/statistics/multiple_testing/index.html",
    "title": "Multiple Hypothesis Testing",
    "section": "",
    "text": "Multiple testing, what does it means?\nSometimes answering a research question means evaluating the different combinations of predictors, and this is the case when multiple hypothesis testing is made.\nThe following are the fundamental steps for understanding how to proceed when multiple hypothesis testing is needed.\nImage credits: Hypothesis Testing On Linear Regression-medium.com"
  },
  {
    "objectID": "content/stats/statistics/multiple_testing/index.html#overview",
    "href": "content/stats/statistics/multiple_testing/index.html#overview",
    "title": "Multiple Hypothesis Testing",
    "section": "",
    "text": "Multiple testing, what does it means?\nSometimes answering a research question means evaluating the different combinations of predictors, and this is the case when multiple hypothesis testing is made.\nThe following are the fundamental steps for understanding how to proceed when multiple hypothesis testing is needed.\nImage credits: Hypothesis Testing On Linear Regression-medium.com"
  },
  {
    "objectID": "content/stats/statistics/multiple_testing/index.html#research-questions",
    "href": "content/stats/statistics/multiple_testing/index.html#research-questions",
    "title": "Multiple Hypothesis Testing",
    "section": "Research questions",
    "text": "Research questions\nIn general, hypothesis testing is made for comparing the expected values of two predictors which are the key drivers for explaining the trend of certain variables that are depending on the levels of those predictors.\nGoing into more detail, making a real-life example, we might need to evaluate the influence of gender and age on the increase of heart diseases. Under this specification, simple questions to be answered would be:\n\nto be a female can be more or less risky in incurring heart disease?\nor, what is the age on average at which heart diseases start increasing?\n\nTo answer these questions we need to make some assumptions:\nthere is no gender difference in heart diseases increasing age is not a cause of heart diseases"
  },
  {
    "objectID": "content/stats/statistics/multiple_testing/index.html#hypothesis-testing",
    "href": "content/stats/statistics/multiple_testing/index.html#hypothesis-testing",
    "title": "Multiple Hypothesis Testing",
    "section": "Hypothesis testing",
    "text": "Hypothesis testing\nThe first hypothesis is namely, the null hypothesis \\(H_{0}\\), and in general, it assumes that there would be no difference in mean (expected value) between the levels of the predictors. In case the null hypothesis fails, an alternative hypothesis is considered, such as there is a difference.\n\nNull hypothesis \\(H_{0}\\): the difference in mean equals zero\nAlternative hypothesis \\(H_{a}\\): the difference in mean is not zero\n\nThis is the starting point of making a hypothesis testing, then more variables can be tested, excluded, or kept whether there is a difference in the mean or not. And, so we are talking of multiple testing of hypothesis.\nSteps to hypothesis testing\n\nDefine the null and alternative hypothesis\nConstruct a test statistic against the null hypothesis\nCompute a p-value to quantify the probability to obtain a value that is the same or more extreme than the test statistic under the null hypothesis\nDecide whether to reject the null hypothesis\nStep back to modeling\nLet‚Äôs hypothesize that we are investigating the root cause of heart diseases. To be simplistic we hypothesize that only gender and age are the key drivers for high blood pressure, the cause of the disease.\nHow would you make a model for answering this question?\nAs an example, we use the heart_disease dataset from the {cheese} package, and to begin our investigation we consider two predictors Age and Sex.\nModel function: \\(y=\\beta_{0}+\\beta_{1}x_{1}+\\beta_{2}x_{2}\\epsilon\\)\nHypothesis:\n\n\n\\(\\beta_{i}\\) equals to zero\nthere is no difference between the mean blood pressure in female and male groups.\n\n\nlibrary(tidyverse)\nlibrary(cheese)\ncheese::heart_disease %&gt;%head\n\n# A tibble: 6 √ó 9\n    Age Sex    ChestPain           BP Cholesterol BloodSugar MaximumHR\n  &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;            &lt;dbl&gt;       &lt;dbl&gt; &lt;lgl&gt;          &lt;dbl&gt;\n1    63 Male   Typical angina     145         233 TRUE             150\n2    67 Male   Asymptomatic       160         286 FALSE            108\n3    67 Male   Asymptomatic       120         229 FALSE            129\n4    37 Male   Non-anginal pain   130         250 FALSE            187\n5    41 Female Atypical angina    130         204 FALSE            172\n6    56 Male   Atypical angina    120         236 FALSE            178\n# ‚Ñπ 2 more variables: ExerciseInducedAngina &lt;fct&gt;, HeartDisease &lt;fct&gt;\n\n\n\nheart_disease%&gt;%\n  ggplot(aes(y=HeartDisease,fill=factor(Sex)),color=\"white\") +\n  geom_histogram(stat=\"count\",position = \"dodge\")+\n  labs(fill=\"Sex\")+\n  scale_fill_brewer()+\n  theme_test()\n\n\n\n\n\n\n\n\nResources\n\n\nHeartDisease dataset\nMultiple Testing - Washington"
  },
  {
    "objectID": "content/stats/statistics/plain_vanilla/index.html",
    "href": "content/stats/statistics/plain_vanilla/index.html",
    "title": "Understanding Plain Vanilla - from Scratch",
    "section": "",
    "text": "In the world of artificial intelligence, understanding the fundamentals of neural networks is essential. One of the simplest yet powerful architectures is the Artificial Neural Network (ANN) with a single hidden layer, often referred to as a ‚Äúplain vanilla‚Äù network. This blog post aims to provide a comprehensive guide to building such a network from scratch, focusing on key concepts like activation functions, weights, and the backpropagation algorithm outside the library boxes."
  },
  {
    "objectID": "content/stats/statistics/plain_vanilla/index.html#overview",
    "href": "content/stats/statistics/plain_vanilla/index.html#overview",
    "title": "Understanding Plain Vanilla - from Scratch",
    "section": "",
    "text": "In the world of artificial intelligence, understanding the fundamentals of neural networks is essential. One of the simplest yet powerful architectures is the Artificial Neural Network (ANN) with a single hidden layer, often referred to as a ‚Äúplain vanilla‚Äù network. This blog post aims to provide a comprehensive guide to building such a network from scratch, focusing on key concepts like activation functions, weights, and the backpropagation algorithm outside the library boxes."
  },
  {
    "objectID": "content/stats/statistics/plain_vanilla/index.html#example-of-construction-of-plain-vanilla-network-architecture",
    "href": "content/stats/statistics/plain_vanilla/index.html#example-of-construction-of-plain-vanilla-network-architecture",
    "title": "Understanding Plain Vanilla - from Scratch",
    "section": "Example of construction of Plain vanilla network architecture",
    "text": "Example of construction of Plain vanilla network architecture\nANN‚Äôs (Artificial Neural Networks) is the simplest implementation of deep learning model architectures that mimic the human brain‚Äôs neural network. The simplest form of ANN is a single-layer network, also known as a ‚Äúplain vanilla‚Äù network. This network consists of an input layer, a hidden layer, and an output layer. The hidden layer transforms the input data into a new set of features, which are then used to predict the response variable.\n\n\n\nIn the image above, we have some predictors \\(x_i\\) that are fed into the hidden layer, which then transforms them into a new set of features \\(h_k(x)\\), which are then used to predict the response variable \\(y\\).\n\nrm(list=ls())\nsuppressMessages(library(tidyverse))\n\nWarning: package 'ggplot2' was built under R version 4.4.1\n\n\nWarning: package 'lubridate' was built under R version 4.4.1\n\ntheme_set(theme_minimal())\n\nBuild synthetic data\nLet‚Äôs create a synthetic dataset to demonstrate the construction of a plain vanilla network. We will generate a dataset with 60 observations and two predictors, x and y, using the following steps:\n\n\nPredictors as Uniform distributed variables ranging between [-2, 2]:\n\n\nset.seed(100)\nx &lt;- runif(60, min=-2, max=2)\n\n\n\nResponse Variable as function of the predictors:\n\n\ny &lt;- function(x) {\n  Y = (cos(2*x + 1))\n  return(Y)\n}\n\n\ndata &lt;- tibble(y=y(x),x)\nhead(data)\n\n# A tibble: 6 √ó 2\n       y       x\n   &lt;dbl&gt;   &lt;dbl&gt;\n1  0.859 -0.769 \n2  0.591 -0.969 \n3  0.152  0.209 \n4 -0.829 -1.77  \n5  0.733 -0.126 \n6  0.645 -0.0649\n\n\n\ndata %&gt;% summary()\n\n       y                  x           \n Min.   :-0.99978   Min.   :-1.77447  \n 1st Qu.:-0.59934   1st Qu.:-0.89194  \n Median : 0.15559   Median :-0.04092  \n Mean   : 0.02628   Mean   : 0.01437  \n 3rd Qu.: 0.62358   3rd Qu.: 0.88138  \n Max.   : 0.99930   Max.   : 1.95826  \n\n\nEDA - Exploratory Data Analysis\nLet‚Äôs visualize our synthetic data:\n\ndata %&gt;%\n  ggplot(aes(x, y)) +\n  geom_line() +\n  geom_point() +\n  geom_hline(yintercept=0, \n             linetype=\"dashed\", color=\"grey\")\n\n\n\nSynthetic Data\n\n\n\nParameters estimation\nNow that we have the data, we attempt to replicate the distribution of this data with a model using artificial neural network technique with a single hidden layer. The model will have the following parameters:\nModel Formula:\n\\[f(X)=\\beta_0+\\sum_{k=1}^K{\\beta_kh_k(X)}\\]\nWhere, \\(h_k\\) is the hidden layer, \\(k=1,...,K\\) the number of activations, \\(\\beta_0,\\beta_1,...,\\beta_K\\) the coefficients , and \\(w_{10},...,w_{kp}\\) the weights.\nThe hidden layer computes a number of activations.\nNumber of activations\nInitialize the number of hidden neurons \\(k\\), which is the number of activations in the hidden layer:\n\nhidden_neurons = 5\n\nThe number of hidden neurons is a hyperparameter that needs to be tuned. The more neurons, the more complex the model, but it can also lead to overfitting. You can think of a neuron as a connection between the input and output layers. The more neurons, the more connections, and the more complex the model.\nWe have set to have 5 hidden neurons in this example. This means that the hidden layer will compute 5 different linear combinations of the input \\(X\\). This linear combination is then squashed through an activation function \\(g(¬∑)\\) to transform it.\nThe function that takes the input \\(X\\) and produces an output \\(A_k\\), the activation.\nActivation Function\nThe activation function is a non-linear transformation of the input layers \\(X_1,X_2,...,X_p\\) which transform to \\(h_k(X)\\) while learning during the training of the network. It is a function that decides whether a neuron should be activated or not by calculating the weighted sum and further adding bias with it. The purpose of the activation function is to introduce non-linearity into the output of a neuron.\n\\[A_k=h_k(X)=g(z)\\]\n\\(g(z)\\) is a function used in logistic regression to convert a linear function into probabilities between zero and one.\nTo be more explicit, the activation function is a function that takes the input signal and generates an output signal, but takes into account a threshold, meaning that it will only be activated if the signal is above a certain threshold.\nWe have specified 5 different activation functions to compare their performance, and we will use the sigmoid function as the activation function in this example.\nType of Activation functions\nThere are several types of activation functions, each with its own characteristics, but all have in common that they introduce non-linearity into the first level of output provided.\nSome of the most common types of activation functions are:\n\n\nSigmoid function:\n\n\\[g(z)=\\frac{e^z}{1+e^z}=\\frac{1}{1+ e^{-z}}\\]\n\nsigmoid &lt;- function(x) {\n  z = (1 / (1 + exp(-x)))\n  return(z)\n}\n\n\n\nReLU (Rectified Linear Unit) function:\n\n\\[g(z) = max(0, z)\\]\n\nrelu &lt;- function(x) {\n  z = ifelse(x &lt; 0, 0, x)\n  return(z)\n}\n\n\n\nSoftPlus Function\n\n\\[g(z) = \\log(1 + e^z)\\]\nThis is a smooth approximation to the ReLU function. Firstly introduced in 2001, Softplus is an alternative to traditional functions because it is differentiable and its derivative is easy to demonstrate (see source: https://sefiks.com/2017/08/11/softplus-as-a-neural-networks-activation-function/).\n\nsoftplus &lt;- function(x) {\n  z = log(1 + exp(x))\n  return(z)\n}\n\n\nOther types are:\n\nPolynomials/Splines: \\(x^2\\)\n\nHyperbolic tanh: \\(tanh(x) = (e^x ‚Äì e^-x) / (e^x + e^-x)\\)\n\n\n\n\nLet‚Äôs compare the activation functions:\ndata %&gt;%\n  mutate(z=sigmoid(x)) %&gt;%\n  ggplot() +\n  geom_line(aes(x, z)) +\n  ylim(0, 1)\ndata %&gt;%\n  ggplot() +\n  geom_line(aes(x, relu(x) * 1/2.4))\ndata %&gt;%\n  ggplot() +\n  geom_line(aes(x, softplus(x)))\n\n\n\n\n\nSigmoid function\n\n\n\n\n\nReLU function\n\n\n\n\n\nSoftPlus function\n\n\n\n\n\nAnd now look at how the Sigmoid differs from the ReLU function:\n\n\ndata %&gt;%\n  ggplot() +\n  geom_line(aes(x, sigmoid(x))) +\n  # relu resized for comparison\n  geom_line(aes(x, relu(x) * 1/2.4))+\n  labs(y=\"Sigmoid vs ReLU\")\n\n\n\n\n\n\nSigmoid vs ReLU\n\n\n\n\n\nOur model is a model in the model:\n\\[f(X)=\\beta_0+\\sum_{k=1}^K{\\beta_kh_k(X)}\\] \\[A_k=h_k(X)=g(z)=g(w_{k0}+\\sum_{j=1}^p{w_{kj}X_j})\\]\n\\[f(X)=\\beta_0+\\sum_{k=1}^K{\\beta_kg(w_{k0}+\\sum_{wkj}^p{X_j})}\\]\nAs you might have noticed in the formula above, the model is a linear combination of the input \\(X\\) and the weights \\(w_{kj}\\), which are adjusted during the training process. The activation function \\(g(z)\\) is applied to the linear combination of the input and weights to transform the output.\nLet‚Äôs have a look at the weights and how they are initialized.\nWeights Initialization\nThe weights are the parameters of the model that are adjusted during the training process. They can be considered as the coefficients of the hidden layer model.\nThey are initialized randomly, and the model is trained to adjust these weights during the training process. The weights are adjusted using the backpropagation algorithm, which computes the gradient of the loss function with respect to the weights. Then, the weights are updated using the gradient descent algorithm. We will see how this is done in the next section.\nThe weights are initialized randomly to break the symmetry and prevent the model from getting stuck in a local minimum. In this case we use a normal distribution with a mean of 0 and a standard deviation of 1 to initialize the weights.\nRandomly initializing the weights as i.i.d. \\(W \\sim N(0,1)\\):\n\nw1 = matrix(rnorm(2*hidden_neurons), \n            nrow=hidden_neurons, \n            ncol=2)\nw2 = matrix(rnorm(hidden_neurons + 1), \n            nrow=1, \n            ncol=(hidden_neurons + 1))\n\nThe constant term \\(w_{k0}\\) will shift the inflection point, and transform a linear function to a non-linear one. The weights are adjusted during the training process to minimize the error between the predicted and actual values.\nThe model derives five new features by computing five different linear combinations of \\(X\\), and then squashes each through an activation function \\(g(¬∑)\\) to transform it.\n\ndata %&gt;%\n  ggplot(aes(x, y)) +\n  geom_point(shape=21, \n             stroke=0.5, \n             fill=\"grey\", \n             color=\"grey20\") +\n  geom_line(linewidth=0.2) +\n  geom_smooth(method = \"lm\", \n              color=\"steelblue\", \n              se=F) +\n  geom_line(aes(x, sigmoid(y)), \n            linetype=\"dashed\", \n            color=\"steelblue\")\n\n\n\nIn this figure is shown the attempt of the ‚Äòlinear‚Äô and ‚Äòsigmoid‚Äô functions to fit our original data. The final model function, able to replicate the original pattern is the result of a continous adaptation and re-calibration of the coeffcients in the model.\n\n\n\nFeedForward\nThe meaning of feedforward is used to describe the process of moving the input data through the network to obtain the predicted output. The feedforward process is the first step in the training process of the neural network.\nHere is a function that computes the output of the model given the inputs: data, weights, and number of activations. It computes the output by multiplying the input data by the weights and applying the activation function to the result. It is a matrix multiplication (%*%), which is a common operation in unsupervised learning algorithms.\n\nfeedForward &lt;- function(x, w1, w2, activation) {\n  output &lt;- rep(0, length(x))\n\n  for (i in 1:length(x)) {\n    a1 = w1 %*% matrix(rbind(1, x[i]), ncol=1)\n    z1 = activation(a1)\n    a2 = w2 %*% matrix(rbind(1, z1), ncol=1)\n    output[i] = a2\n  }\n\n  return(output)\n}\n\nDerivative Activation Function\nNow, that we have the feedforward function, we need to compute the derivative of the activation function. The backpropagation algorithm multiplies the derivative of the activation function.\n\nBackpropagation algorithm multiplies the derivative of the activation function.\n\nHere is a recap of the definition of derivative formula, which is applied any time the output released by the activation function is met in the network. And so, a new minimum is found. It will be more clear through the end of the post.\n\n\n\nSo, it is fundamental to define the derivative of the activation function needed for computing the gradient. For this example, we will use the derivative of the sigmoid function.\n\nderivativeActivation &lt;- function(x) {\n  g = (sigmoid(x) * (1 - sigmoid(x)))\n  return(g)\n}\n\nModel Error\nFunction for computing model error is the sum of squared errors (SSE) between the predicted and actual values.\n\nmodelError &lt;- function(x, y, w1, w2, activation) {\n  # Predictions\n  preds &lt;- feedForward(x, w1, w2, activation)\n  # Error calculation\n  SSE &lt;- sum((y - preds) ** 2)\n  return (SSE)\n}\n\nBack-Propagation\nSo, this is the time for computing the gradients.\n\nWhat are the gradients?\n\nThe gradients are the derivatives of the cost function with respect to the weights. The backpropagation algorithm computes the gradient of the loss function with respect to the weights.\nThe gradients are then used to update the weights using the gradient descent algorithm.\n\nbackPropagation &lt;- function(x, y, w1, w2, \n                            activation, derivativeActivation) {\n  #predicted values\n  preds &lt;- feedForward(x, w1, w2, activation) \n  #Derivative of the cost function (first term)\n  derivCost &lt;- -2 * (y - preds) \n  #Gradients for the weights\n  dW1 &lt;- matrix(0, ncol=2, nrow=nrow(w1)) \n  dW2 &lt;- matrix(rep(0, length(x) * (dim(w2)[2])), nrow=length(x)) \n\n  # Computing the Gradient for W2\n  for (i in 1:length(x)) {\n    a1 = w1 %*% matrix(rbind(1, x[i]), ncol=1)\n    da2dW2 = matrix(rbind(1, activation(a1)), nrow=1)\n    dW2[i,] = derivCost[i] * da2dW2\n  }\n\n  # Computing the gradient for W1\n  for (i in 1:length(x)) {\n    a1 = w1 %*% matrix(rbind(1, x[i]), ncol=1)\n    da2da1 = derivativeActivation(a1) * matrix(w2[,-1], ncol=1)\n    da2dW1 = da2da1 %*% matrix(rbind(1, x[i]), nrow=1)\n    dW1 = dW1 + derivCost[i] * da2dW1\n  }\n\n  # Storing gradients for w1, w2 in a list\n  gradient &lt;- list(dW1, colSums(dW2))\n\n  return (gradient)\n}\n\nStochastic Gradient Descent\nDefining our Stochastic Gradient Descent algorithm which will adjust our weight matrices.\n\nSGD &lt;- function(x, y, w1, w2, activation, derivative, learnRate, epochs) {\n  SSEvec &lt;- rep(NA, epochs) # Empty array to store SSE values after each epoch\n  SSEvec[1] = modelError(x, y, w1, w2, activation)\n\n  for (j in 1:epochs) {\n    for (i in 1:length(x)) {\n      gradient &lt;- backPropagation(x[i], y[i], w1, w2, activation, derivative)\n      # Adjusting model parameters for a given number of epochs\n      w1 &lt;- w1 - learnRate * gradient[[1]]\n      w2 &lt;- w2 - learnRate * gradient[[2]]\n    }\n    SSEvec[j+1] &lt;- modelError(x, y, w1, w2, activation) \n    # Storing SSE values after each iteration\n    }\n    # Beta vector holding model parameters\n    B &lt;- list(w1, w2)\n    result &lt;- list(B, SSEvec)\n    return(result)\n}\n\nModeling\nRunning the SGD function to obtain our optimized model and parameters:\n\nmodel &lt;- SGD(x, y(x), w1, w2, \n             activation = sigmoid, \n             derivative = derivativeActivation,\n             learnRate = 0.01, \n             epochs = 200)\n\nObtaining our adjusted SSE‚Äôs for each epoch:\n\nSSE &lt;- model[[2]]\n\nModel Visualization\nPlotting the SSE from each epoch vs number of epochs\n\nmodel_data &lt;- tibble(x=seq(0, 200, 1), SSE)\n\nggplot(model_data,aes(x, SSE)) +\n  geom_line(linewidth=0.1)+\n  geom_point(shape=21, \n             stroke=0.2, \n             fill=alpha(\"steelblue\", 0.3),\n             color=\"brown\") +\n  labs(title=\"Model SSE by Number of Epochs\",\n       x = \"Epochs\", y = \"Error\")\n\n\n\n\n\n\n\nParameters optimization\nExtracting our new parameters from our model.\n\nnew_w1 &lt;- model[[1]][[1]]\nnew_w2 &lt;- model[[1]][[2]]\n\nComparing our old weight matrices against the new ones.\n\npar(mfrow=c(1,2))\nplot(w1,new_w1)\nabline(0,1)\n\nplot(w2,new_w2)\nabline(0,1)\n\n\n\n\n\n\n\nNew Predictions\nObtaining our new predictions using our optimized parameters.\n\ny_pred &lt;- feedForward(x, new_w1, new_w2, sigmoid)\n\nPlotting training data against our model predictions\n\ndata %&gt;%\n  mutate(y_pred=y_pred) %&gt;%\n  pivot_longer(cols = c(y, y_pred)) %&gt;%\n  ggplot(aes(x, value, group=name, color=name)) +\n  geom_point(shape=21, stroke=0.5) +\n  geom_line() +\n  scale_color_discrete(type = c(\"steelblue\", \"red\")) +\n  labs(title= \"Target Response vs. Predictions\",\n       x=\"Observations\", \n       y=\"Responses\")"
  },
  {
    "objectID": "content/stats/statistics/plain_vanilla/index.html#resources",
    "href": "content/stats/statistics/plain_vanilla/index.html#resources",
    "title": "Understanding Plain Vanilla - from Scratch",
    "section": "Resources",
    "text": "Resources\n\nThe code used for this example is customized from tristanoprofetto github repository: https://github.com/tristanoprofetto/neural-networks/blob/main/ANN/Regressor/feedforward.R\n\nStatQuest: Neural Networks Pt. 1: Inside the Black Box https://www.youtube.com/watch?v=CqOfi41LfDw\n\nOther-Resources: https://towardsdatascience.com/the-mostly-complete-chart-of-neural-networks-explained-3fb6f2367464"
  },
  {
    "objectID": "content/stats/statistics/predictive_modeling/index.html",
    "href": "content/stats/statistics/predictive_modeling/index.html",
    "title": "Predictive modeling",
    "section": "",
    "text": "This post is dedicated to make a comparison between Caret and TidyModels R packages. Data modeling with R pass through data preprocessing and parameters assessments to predicting an outcome. Both set of packages can be used to acheive same results, with the purpose of finding the best predictive performance for data specifc models.\nThe Caret package is the starting point for understanding how to manage models and produce unbiases predictions with R. As well as TidyModels meta package, it gives the opportunity to contruct a multivariate model syntax to manage several models to be applied on same set of data. TidyModels allows the use of a set of concatenated functions in partership with the TidyVerse grammar to build a structural model base which blends different models as one global model.\nThe following is an attempt to a comparison between the two predictive model structures."
  },
  {
    "objectID": "content/stats/statistics/predictive_modeling/index.html#overview",
    "href": "content/stats/statistics/predictive_modeling/index.html#overview",
    "title": "Predictive modeling",
    "section": "",
    "text": "This post is dedicated to make a comparison between Caret and TidyModels R packages. Data modeling with R pass through data preprocessing and parameters assessments to predicting an outcome. Both set of packages can be used to acheive same results, with the purpose of finding the best predictive performance for data specifc models.\nThe Caret package is the starting point for understanding how to manage models and produce unbiases predictions with R. As well as TidyModels meta package, it gives the opportunity to contruct a multivariate model syntax to manage several models to be applied on same set of data. TidyModels allows the use of a set of concatenated functions in partership with the TidyVerse grammar to build a structural model base which blends different models as one global model.\nThe following is an attempt to a comparison between the two predictive model structures."
  },
  {
    "objectID": "content/stats/statistics/predictive_modeling/index.html#caret-package",
    "href": "content/stats/statistics/predictive_modeling/index.html#caret-package",
    "title": "Predictive modeling",
    "section": "Caret package",
    "text": "Caret package\nThe most important functions for this package, grouped by steps to modeling, are:\n\n\nPreprocessing (data cleaning/wrangling)\n\npreProcess()\n\n\n\nData splitting and resampling\n\ncreateDataPartition()\n\n\ncreateResample()\ncreateTimeSlices()\n\n\n\nModel fit and prediction\n\ntrain()\n\n\npredict()\n\n\n\nModel comparison\n\nconfusionMatrix()"
  },
  {
    "objectID": "content/stats/statistics/predictive_modeling/index.html#machine-learning-algorithms-in-r",
    "href": "content/stats/statistics/predictive_modeling/index.html#machine-learning-algorithms-in-r",
    "title": "Predictive modeling",
    "section": "Machine learning algorithms in R",
    "text": "Machine learning algorithms in R\n\nLinear discriminant analysis\nRegression\nNaive Bayes\nSupport vector machines\nClassification and regression trees\nRandom forests\nBoosting\netc.\n\nResource: Practical Machine Learning"
  },
  {
    "objectID": "content/stats/statistics/predictive_modeling/index.html#caret-or-tidymodels",
    "href": "content/stats/statistics/predictive_modeling/index.html#caret-or-tidymodels",
    "title": "Predictive modeling",
    "section": "Caret or TidyModels?",
    "text": "Caret or TidyModels?\nCaret Tidymodels"
  },
  {
    "objectID": "content/stats/statistics/predictive_modeling/index.html#caret-example-with-spam-data",
    "href": "content/stats/statistics/predictive_modeling/index.html#caret-example-with-spam-data",
    "title": "Predictive modeling",
    "section": "Caret Example with SPAM Data",
    "text": "Caret Example with SPAM Data\n\nlibrary(caret); library(kernlab); data(spam)\ninTrain &lt;- createDataPartition(y=spam$type,\n                              p=0.75, list=FALSE)\ntraining &lt;- spam[inTrain,]\ntesting &lt;- spam[-inTrain,]\n# dim(training)\n\nset.seed(32343)\nmodelFit &lt;- train(type ~.,data=training, method=\"glm\")\n# modelFit\n\npredictions &lt;- predict(modelFit,newdata=testing)\n# predictions\n\ncm &lt;- confusionMatrix(predictions,testing$type)\ncm\n\nplot(cm$table,main=\"Table\")"
  },
  {
    "objectID": "content/stats/statistics/predictive_modeling/index.html#tidymodels-example-with-spam-data",
    "href": "content/stats/statistics/predictive_modeling/index.html#tidymodels-example-with-spam-data",
    "title": "Predictive modeling",
    "section": "TidyModels Example with SPAM Data",
    "text": "TidyModels Example with SPAM Data\n\nlibrary(tidymodels)\ntidymodels_prefer()\nset.seed(123)\nsplit &lt;- initial_split(spam,0.75,strata=type)\ntraining &lt;- training(split)\ntesting &lt;- testing(split)\n\nmodelFit &lt;- logistic_reg() %&gt;% \n  set_engine(\"glm\") %&gt;%\n  fit(type~.,data=spam)\n\n# tidy(modelFit)\n\npredictions &lt;- predict(modelFit,new_data=testing)\n# predictions\n\ntesting$pred &lt;- predictions$.pred_class\ncm &lt;- yardstick::conf_mat(data = testing, truth = type, estimate = pred)\ncm\nautoplot(cm)"
  },
  {
    "objectID": "content/blog/index.html",
    "href": "content/blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Writing a book in R\n\n\n\nbook\n\n\n\n\n\n\n\n\n\nJun 23, 2025\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "content/dataviz/data_visualization/usecases/hiv/index.html",
    "href": "content/dataviz/data_visualization/usecases/hiv/index.html",
    "title": "Epidemic transition metrics of HIV infections estimation",
    "section": "",
    "text": "HIV infections data is from the aidsinfo.unaids.org. The data contains Global values from 2010 to 2022 for HIV estimate, HIV incidence of mortality, HIV prevalence, and HIV deaths.\n\nNational data for the 2010 and 2020 are included in this dataset.\nThe following is estimating magnitude of change along 10 years time-frame from 2010 to 2020 of HIV infections for all countries with available data.\nLoad necessary libraries.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\ntidymodels_prefer()\nlibrary(spatialsample)\nlibrary(sf)\nlibrary(tmap)\ndata(\"World\")\n\n\nThis first dataset contains the average values, obtained by averaging the lower and upper bounds of 2010 and 2020 HIV infections for 173 countries.\n\naids_avg &lt;- read.csv(\"data/aids_avg.csv\")\naids_avg &lt;- aids_avg%&gt;%\n  filter(!country==\"Global\")%&gt;%\n  filter(!country==\"India\")%&gt;%\n  rename(aids_cc = aids_change2)\n\naids_avg%&gt;%dim\n\nThe percent change relative to the sum of changes for all countries is given by:\n\\[\\text{Percent Change (Relative to Sum)}=(\\frac{\\text{(Final Value‚àíInitial Value)}}{‚àë(Final Value‚àíInitialValue)})√ó100\\] This will give you the percent change for each country relative to the sum of all changes. As well as the percentage contribution of each country's change to the total change can be obtained.\n\n\nprevalence_rt &lt;- read.csv(\"data/Epidemic transition metrics_Incidence_prevalence ratio.csv\")\n\naids_prevalence_rt&lt;-prevalence_rt%&gt;%\n  filter(!Country==\"Global\")%&gt;%\n  select(!contains(\"Footnote\"))%&gt;%\n  select(contains(c(\"Country\",\"2010\",\"2020\")))%&gt;%\n  mutate(avg_2010=(as.numeric(X2010_upper)-as.numeric(X2010_lower))/2,\n         avg_2020=(as.numeric(X2020_upper)-as.numeric(X2020_lower))/2,\n         aids_change=(avg_2020-avg_2010),\n         country_change=round(aids_change/avg_2010,5),\n         aids_prev_cc=round(aids_change/sum(aids_change,na.rm = T),5))\n\n\n\ninc_mort_ratio &lt;- read_csv(\"data/Epidemic transition metrics_Incidence_mortality ratio.csv\")\n\naids_inc_mort_ratio &lt;- inc_mort_ratio%&gt;%\n  janitor::clean_names()%&gt;%\n    filter(!country==\"Global\")%&gt;%\n  select(!contains(\"Footnote\"))%&gt;%\n  select(contains(c(\"Country\",\"2010\",\"2020\")))%&gt;%\n  mutate(avg_2010=(as.numeric(x2010_upper)-as.numeric(x2010_lower))/2,\n         avg_2020=(as.numeric(x2020_upper)-as.numeric(x2020_lower))/2,\n         aids_change=(avg_2020-avg_2010),\n         country_change=round(aids_change/avg_2010,5),\n         aids_imr_cc=round(aids_change/sum(aids_change,na.rm = T),5))\n\n\n\ndeaths &lt;- read_csv(\"data/Epidemic transition metrics_Trend of AIDS-related deaths.csv\")\n\naids_deaths&lt;- deaths%&gt;%\n  filter(!Country==\"Global\")%&gt;%\n  filter(!Country==\"India\")%&gt;%\n  select(!contains(\"Footnote\"))%&gt;%\n  select(contains(c(\"Country\",\"2010\",\"2020\")))%&gt;%\n  janitor::clean_names()%&gt;%\n  mutate(x2010_upper=as.numeric(str_extract(x2010_upper,\"([0-9]+)\")),\n         x2010_lower=as.numeric(str_extract(x2010_lower,\"([0-9]+)\")),\n         x2020_upper=as.numeric(str_extract(x2020_upper,\"([0-9]+)\")),\n         x2020_lower=as.numeric(str_extract(x2010_lower,\"([0-9]+)\")),\n         avg_2010=(x2010_upper-x2010_lower)/2,\n         avg_2020=(x2020_upper-x2020_lower)/2,\n         aids_change=(avg_2020-avg_2010),\n         country_change=round(aids_change/avg_2010,5),\n         aids_d_cc=round(aids_change/sum(aids_change,na.rm = T),5))\n\n\n\ndat&lt;- aids_avg%&gt;%\n  select(country,aids_cc)%&gt;%\n  left_join(aids_prevalence_rt%&gt;%select(Country,aids_prev_cc),\n            by=c(\"country\"=\"Country\"))%&gt;%\n  left_join(aids_inc_mort_ratio%&gt;%select(country,aids_imr_cc),\n            by=c(\"country\"))%&gt;%\n  left_join(aids_deaths%&gt;%select(country,aids_d_cc),\n            by=c(\"country\"))\ndat%&gt;%head\n\n\ndat%&gt;%\n  dim()\n\n\n\ndat%&gt;%\n  pivot_longer(cols = contains(\"cc\"))%&gt;%\n  mutate(value=scale(value),\n         country=as.factor(country))%&gt;%\n  drop_na()%&gt;%\n  ggplot(aes(x=fct_reorder(country,value),y=value,\n             group=name,fill=name),color=\"grey24\")+\n  geom_col(position = \"stack\")+\n  scale_y_log10(expand=c(0,0),label=scales::comma_format())+\n  labs(title=\"HIV Distributions (2010-2020)\",\n       x=\"Country\",\n       fill=\"\",\n       caption = \"Graphic: @fgazzelloni\")+\n  theme(text=element_text(size=14),\n        axis.text.x = element_text(angle = 90,size=4,hjust=1),\n        panel.grid = element_blank(),\n        panel.background = element_rect(color = \"grey24\",fill=\"grey24\"))\n\n\n\nWorld &lt;- World %&gt;%\n  select(country=name,geometry)%&gt;%\n  filter(!country==\"Antarctica\")\n\naids_map &lt;- dat%&gt;%\n    pivot_longer(cols = contains(\"cc\"))%&gt;%\n  mutate(value=scale(value),\n         country=as.factor(country))%&gt;%\n  drop_na()%&gt;%\n  left_join(World,by=\"country\")%&gt;%\n  st_as_sf()%&gt;%\n  st_transform(crs=\"ESRI:54030\")\n\n\nlabels=c(\"aids_cc\"=\"HIV Country Contribution\",\n         \"aids_d_cc\"=\"HIV Deaths Country Contribution\",\n         \"aids_imr_cc\"=\"HIV incidence mortality rate Country Contribution\",\n         \"aids_prev_cc\"=\"HIV Prevalence Country Contribution\")\n\n\n\nggplot()+\n  geom_sf(data=World,color=\"grey25\",fill=\"grey75\")+\n  geom_sf(data=aids_map,\n          mapping=aes(geometry=geometry,fill=value),\n          color=\"red\")+\n  coord_sf(crs=\"ESRI:54030\",clip = \"off\")+\n  facet_wrap(~name,labeller = labeller(name=labels))+\n  scale_fill_viridis_c()+\n  labs(caption=\"Map: @fgazzelloni\")\n\n\n\ndata &lt;- dat%&gt;%\n  drop_na()%&gt;%\n  inner_join(World,by=c(\"country\"))%&gt;%\n  sf::st_as_sf(crs = 4326)\n\n\nset.seed(11132023)\nsplit &lt;- initial_split(data,prop = 0.8)\ntrain&lt;- training(split)\ntest &lt;- testing(split)\n\n\n\nfolds &lt;- spatial_clustering_cv(train, v = 5)\n\n\n\nautoplot(folds)+\n  labs(title=\"HIV Spatial Clustering Cross Validation\",\n       caption=\"DataSource: aidsinfo.unaids.org | Map: @fgazzelloni\")+\n  ggthemes::theme_map(base_size = 14)+\n  theme(plot.title = element_text(hjust=0.5),\n        plot.caption = element_text(hjust = 0.5))\n\n\nsource: https://spatialsample.tidymodels.org/articles/spatialsample.html\n\n# `splits` will be the `rsplit` object\ncompute_preds &lt;- function(splits) {\n  # fit the model to the analysis set\n  mod &lt;- lm(aids_cc ~ aids_prev_cc+aids_imr_cc+aids_d_cc,\n    data = analysis(split)\n  )\n  # identify the assessment set\n  holdout &lt;- assessment(split)\n  # return the assessment set, with true and predicted price\n  tibble::tibble(\n    geometry = holdout$geometry,\n    aids_cc = log10(holdout$aids_cc),\n    .pred = predict(mod, holdout)\n  )\n}\n\n\n\ncluster_folds &lt;- spatial_clustering_cv(data, v = 15)\nblock_folds &lt;- spatial_block_cv(data, v = 15)\n\n\ncluster_folds$type &lt;- \"cluster\"\nblock_folds$type &lt;- \"block\"\n\n\nresamples &lt;-\n  dplyr::bind_rows(\n    cluster_folds,\n    block_folds\n  )\n\n\ncv_res &lt;- resamples %&gt;%\n  mutate(.preds = map(splits, compute_preds))\n\n\ncv_rmse &lt;- cv_res %&gt;%\n  unnest(.preds) %&gt;%\n  drop_na()%&gt;%\n  filter(!aids_cc==-Inf)%&gt;%\n  group_by(id, type) %&gt;%\n  rmse(aids_cc, .pred)\n\n\ncv_res %&gt;%\n  unnest(.preds) %&gt;%\n  ggplot(aes(fill = .pred)) +\n  geom_sf(data=World,mapping=aes(geometry = geometry),inherit.aes = F)+\n  geom_sf(aes(geometry = geometry)) +\n  labs() +\n  scale_fill_viridis_c() +\n  facet_wrap(~type)\n\n\ncv_res %&gt;%\n  unnest(.preds) %&gt;%\n  ggplot(aes(fill = aids_cc)) +\n  geom_sf(data=World,mapping=aes(geometry = geometry),inherit.aes = F)+\n  geom_sf(aes(geometry = geometry)) +\n  labs() +\n  scale_fill_viridis_c() \n\n\n\nSpatialSample\nr-spatial.org\nhttps://www.tmwr.org/"
  },
  {
    "objectID": "content/dataviz/data_visualization/usecases/hiv/index.html#hiv-average-values-2010-2020",
    "href": "content/dataviz/data_visualization/usecases/hiv/index.html#hiv-average-values-2010-2020",
    "title": "Epidemic transition metrics of HIV infections estimation",
    "section": "",
    "text": "This first dataset contains the average values, obtained by averaging the lower and upper bounds of 2010 and 2020 HIV infections for 173 countries.\n\naids_avg &lt;- read.csv(\"data/aids_avg.csv\")\naids_avg &lt;- aids_avg%&gt;%\n  filter(!country==\"Global\")%&gt;%\n  filter(!country==\"India\")%&gt;%\n  rename(aids_cc = aids_change2)\n\naids_avg%&gt;%dim\n\nThe percent change relative to the sum of changes for all countries is given by:\n\\[\\text{Percent Change (Relative to Sum)}=(\\frac{\\text{(Final Value‚àíInitial Value)}}{‚àë(Final Value‚àíInitialValue)})√ó100\\] This will give you the percent change for each country relative to the sum of all changes. As well as the percentage contribution of each country's change to the total change can be obtained."
  },
  {
    "objectID": "content/dataviz/data_visualization/usecases/hiv/index.html#hiv-prevalence-ratio-2010-2020",
    "href": "content/dataviz/data_visualization/usecases/hiv/index.html#hiv-prevalence-ratio-2010-2020",
    "title": "Epidemic transition metrics of HIV infections estimation",
    "section": "",
    "text": "prevalence_rt &lt;- read.csv(\"data/Epidemic transition metrics_Incidence_prevalence ratio.csv\")\n\naids_prevalence_rt&lt;-prevalence_rt%&gt;%\n  filter(!Country==\"Global\")%&gt;%\n  select(!contains(\"Footnote\"))%&gt;%\n  select(contains(c(\"Country\",\"2010\",\"2020\")))%&gt;%\n  mutate(avg_2010=(as.numeric(X2010_upper)-as.numeric(X2010_lower))/2,\n         avg_2020=(as.numeric(X2020_upper)-as.numeric(X2020_lower))/2,\n         aids_change=(avg_2020-avg_2010),\n         country_change=round(aids_change/avg_2010,5),\n         aids_prev_cc=round(aids_change/sum(aids_change,na.rm = T),5))"
  },
  {
    "objectID": "content/dataviz/data_visualization/usecases/hiv/index.html#hiv-incidence-mortality-ratio-2010-2020",
    "href": "content/dataviz/data_visualization/usecases/hiv/index.html#hiv-incidence-mortality-ratio-2010-2020",
    "title": "Epidemic transition metrics of HIV infections estimation",
    "section": "",
    "text": "inc_mort_ratio &lt;- read_csv(\"data/Epidemic transition metrics_Incidence_mortality ratio.csv\")\n\naids_inc_mort_ratio &lt;- inc_mort_ratio%&gt;%\n  janitor::clean_names()%&gt;%\n    filter(!country==\"Global\")%&gt;%\n  select(!contains(\"Footnote\"))%&gt;%\n  select(contains(c(\"Country\",\"2010\",\"2020\")))%&gt;%\n  mutate(avg_2010=(as.numeric(x2010_upper)-as.numeric(x2010_lower))/2,\n         avg_2020=(as.numeric(x2020_upper)-as.numeric(x2020_lower))/2,\n         aids_change=(avg_2020-avg_2010),\n         country_change=round(aids_change/avg_2010,5),\n         aids_imr_cc=round(aids_change/sum(aids_change,na.rm = T),5))"
  },
  {
    "objectID": "content/dataviz/data_visualization/usecases/hiv/index.html#hiv-deaths-2010-2020",
    "href": "content/dataviz/data_visualization/usecases/hiv/index.html#hiv-deaths-2010-2020",
    "title": "Epidemic transition metrics of HIV infections estimation",
    "section": "",
    "text": "deaths &lt;- read_csv(\"data/Epidemic transition metrics_Trend of AIDS-related deaths.csv\")\n\naids_deaths&lt;- deaths%&gt;%\n  filter(!Country==\"Global\")%&gt;%\n  filter(!Country==\"India\")%&gt;%\n  select(!contains(\"Footnote\"))%&gt;%\n  select(contains(c(\"Country\",\"2010\",\"2020\")))%&gt;%\n  janitor::clean_names()%&gt;%\n  mutate(x2010_upper=as.numeric(str_extract(x2010_upper,\"([0-9]+)\")),\n         x2010_lower=as.numeric(str_extract(x2010_lower,\"([0-9]+)\")),\n         x2020_upper=as.numeric(str_extract(x2020_upper,\"([0-9]+)\")),\n         x2020_lower=as.numeric(str_extract(x2010_lower,\"([0-9]+)\")),\n         avg_2010=(x2010_upper-x2010_lower)/2,\n         avg_2020=(x2020_upper-x2020_lower)/2,\n         aids_change=(avg_2020-avg_2010),\n         country_change=round(aids_change/avg_2010,5),\n         aids_d_cc=round(aids_change/sum(aids_change,na.rm = T),5))"
  },
  {
    "objectID": "content/dataviz/data_visualization/usecases/hiv/index.html#all-data",
    "href": "content/dataviz/data_visualization/usecases/hiv/index.html#all-data",
    "title": "Epidemic transition metrics of HIV infections estimation",
    "section": "",
    "text": "dat&lt;- aids_avg%&gt;%\n  select(country,aids_cc)%&gt;%\n  left_join(aids_prevalence_rt%&gt;%select(Country,aids_prev_cc),\n            by=c(\"country\"=\"Country\"))%&gt;%\n  left_join(aids_inc_mort_ratio%&gt;%select(country,aids_imr_cc),\n            by=c(\"country\"))%&gt;%\n  left_join(aids_deaths%&gt;%select(country,aids_d_cc),\n            by=c(\"country\"))\ndat%&gt;%head\n\n\ndat%&gt;%\n  dim()\n\n\n\ndat%&gt;%\n  pivot_longer(cols = contains(\"cc\"))%&gt;%\n  mutate(value=scale(value),\n         country=as.factor(country))%&gt;%\n  drop_na()%&gt;%\n  ggplot(aes(x=fct_reorder(country,value),y=value,\n             group=name,fill=name),color=\"grey24\")+\n  geom_col(position = \"stack\")+\n  scale_y_log10(expand=c(0,0),label=scales::comma_format())+\n  labs(title=\"HIV Distributions (2010-2020)\",\n       x=\"Country\",\n       fill=\"\",\n       caption = \"Graphic: @fgazzelloni\")+\n  theme(text=element_text(size=14),\n        axis.text.x = element_text(angle = 90,size=4,hjust=1),\n        panel.grid = element_blank(),\n        panel.background = element_rect(color = \"grey24\",fill=\"grey24\"))"
  },
  {
    "objectID": "content/dataviz/data_visualization/usecases/hiv/index.html#world-polygons",
    "href": "content/dataviz/data_visualization/usecases/hiv/index.html#world-polygons",
    "title": "Epidemic transition metrics of HIV infections estimation",
    "section": "",
    "text": "World &lt;- World %&gt;%\n  select(country=name,geometry)%&gt;%\n  filter(!country==\"Antarctica\")\n\naids_map &lt;- dat%&gt;%\n    pivot_longer(cols = contains(\"cc\"))%&gt;%\n  mutate(value=scale(value),\n         country=as.factor(country))%&gt;%\n  drop_na()%&gt;%\n  left_join(World,by=\"country\")%&gt;%\n  st_as_sf()%&gt;%\n  st_transform(crs=\"ESRI:54030\")\n\n\nlabels=c(\"aids_cc\"=\"HIV Country Contribution\",\n         \"aids_d_cc\"=\"HIV Deaths Country Contribution\",\n         \"aids_imr_cc\"=\"HIV incidence mortality rate Country Contribution\",\n         \"aids_prev_cc\"=\"HIV Prevalence Country Contribution\")\n\n\n\nggplot()+\n  geom_sf(data=World,color=\"grey25\",fill=\"grey75\")+\n  geom_sf(data=aids_map,\n          mapping=aes(geometry=geometry,fill=value),\n          color=\"red\")+\n  coord_sf(crs=\"ESRI:54030\",clip = \"off\")+\n  facet_wrap(~name,labeller = labeller(name=labels))+\n  scale_fill_viridis_c()+\n  labs(caption=\"Map: @fgazzelloni\")"
  },
  {
    "objectID": "content/dataviz/data_visualization/usecases/hiv/index.html#spending-data",
    "href": "content/dataviz/data_visualization/usecases/hiv/index.html#spending-data",
    "title": "Epidemic transition metrics of HIV infections estimation",
    "section": "",
    "text": "data &lt;- dat%&gt;%\n  drop_na()%&gt;%\n  inner_join(World,by=c(\"country\"))%&gt;%\n  sf::st_as_sf(crs = 4326)\n\n\nset.seed(11132023)\nsplit &lt;- initial_split(data,prop = 0.8)\ntrain&lt;- training(split)\ntest &lt;- testing(split)"
  },
  {
    "objectID": "content/dataviz/data_visualization/usecases/hiv/index.html#spatial-cross-validation",
    "href": "content/dataviz/data_visualization/usecases/hiv/index.html#spatial-cross-validation",
    "title": "Epidemic transition metrics of HIV infections estimation",
    "section": "",
    "text": "folds &lt;- spatial_clustering_cv(train, v = 5)\n\n\n\nautoplot(folds)+\n  labs(title=\"HIV Spatial Clustering Cross Validation\",\n       caption=\"DataSource: aidsinfo.unaids.org | Map: @fgazzelloni\")+\n  ggthemes::theme_map(base_size = 14)+\n  theme(plot.title = element_text(hjust=0.5),\n        plot.caption = element_text(hjust = 0.5))"
  },
  {
    "objectID": "content/dataviz/data_visualization/usecases/hiv/index.html#function-for-calculating-predictions",
    "href": "content/dataviz/data_visualization/usecases/hiv/index.html#function-for-calculating-predictions",
    "title": "Epidemic transition metrics of HIV infections estimation",
    "section": "",
    "text": "source: https://spatialsample.tidymodels.org/articles/spatialsample.html\n\n# `splits` will be the `rsplit` object\ncompute_preds &lt;- function(splits) {\n  # fit the model to the analysis set\n  mod &lt;- lm(aids_cc ~ aids_prev_cc+aids_imr_cc+aids_d_cc,\n    data = analysis(split)\n  )\n  # identify the assessment set\n  holdout &lt;- assessment(split)\n  # return the assessment set, with true and predicted price\n  tibble::tibble(\n    geometry = holdout$geometry,\n    aids_cc = log10(holdout$aids_cc),\n    .pred = predict(mod, holdout)\n  )\n}"
  },
  {
    "objectID": "content/dataviz/data_visualization/usecases/hiv/index.html#spatial-clustering-and-spatial-block-cross-validations",
    "href": "content/dataviz/data_visualization/usecases/hiv/index.html#spatial-clustering-and-spatial-block-cross-validations",
    "title": "Epidemic transition metrics of HIV infections estimation",
    "section": "",
    "text": "cluster_folds &lt;- spatial_clustering_cv(data, v = 15)\nblock_folds &lt;- spatial_block_cv(data, v = 15)\n\n\ncluster_folds$type &lt;- \"cluster\"\nblock_folds$type &lt;- \"block\"\n\n\nresamples &lt;-\n  dplyr::bind_rows(\n    cluster_folds,\n    block_folds\n  )\n\n\ncv_res &lt;- resamples %&gt;%\n  mutate(.preds = map(splits, compute_preds))\n\n\ncv_rmse &lt;- cv_res %&gt;%\n  unnest(.preds) %&gt;%\n  drop_na()%&gt;%\n  filter(!aids_cc==-Inf)%&gt;%\n  group_by(id, type) %&gt;%\n  rmse(aids_cc, .pred)\n\n\ncv_res %&gt;%\n  unnest(.preds) %&gt;%\n  ggplot(aes(fill = .pred)) +\n  geom_sf(data=World,mapping=aes(geometry = geometry),inherit.aes = F)+\n  geom_sf(aes(geometry = geometry)) +\n  labs() +\n  scale_fill_viridis_c() +\n  facet_wrap(~type)\n\n\ncv_res %&gt;%\n  unnest(.preds) %&gt;%\n  ggplot(aes(fill = aids_cc)) +\n  geom_sf(data=World,mapping=aes(geometry = geometry),inherit.aes = F)+\n  geom_sf(aes(geometry = geometry)) +\n  labs() +\n  scale_fill_viridis_c()"
  },
  {
    "objectID": "content/dataviz/data_visualization/usecases/hiv/index.html#references",
    "href": "content/dataviz/data_visualization/usecases/hiv/index.html#references",
    "title": "Epidemic transition metrics of HIV infections estimation",
    "section": "",
    "text": "SpatialSample\nr-spatial.org\nhttps://www.tmwr.org/"
  },
  {
    "objectID": "content/dataviz/data_visualization/dogbreeds/index.html",
    "href": "content/dataviz/data_visualization/dogbreeds/index.html",
    "title": "Dog breeds",
    "section": "",
    "text": "This #TidyTuesday post is all about drawing with R. I have replicated the dogs‚Äô prints (or paws) using R. For solving this kind of tasks you need to be very ‚Äúpractice‚Äù with functions outcomes. And this means to have in mind the shape the function will take. For example we know that \\(y = x^2\\) would shape a parabolic function.\nSo that, I decided to get some extra inspiration and used the ggforce::geom_ellipse() function to produce ellipses of different sizes.\n\nThe data set for this week #TidyTuesday contains Dog breeds information, in three datasets.To load the data:\n\nbreed_traits &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-02-01/breed_traits.csv')\ntrait_description &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-02-01/trait_description.csv')\nbreed_rank_all &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-02-01/breed_rank.csv')\n\n\nA bit of data wrangling to make a rank dataset and see which are the 2020 favourite breeds.\n\nrank&lt;-breed_rank_all%&gt;%\n  count(Breed,`2020 Rank`,Image)%&gt;%\n  arrange(`2020 Rank`)%&gt;%\n  slice(1:10)%&gt;%\n  relocate(`2020 Rank`)%&gt;%\n  select(-n)\n\nrank\n\nSelect favourite breeds, and set the images\n\ndogs&lt;-c(\"Bulldogs\",\"Poodles\",\"Beagles\",\"Rottweilers\",\"Dachshunds\")\n\nimages &lt;-rank%&gt;%filter(Breed%in%dogs)%&gt;%select(Breed,Image)\n\nmy_df &lt;-breed_traits%&gt;%\n  select(-\"Coat Type\",-\"Coat Length\")%&gt;%\n  filter(Breed%in%dogs)%&gt;%\n  mutate(id=row_number())%&gt;%\n  relocate(id)%&gt;%\n  pivot_longer(cols=3:16,names_to=\"factor\",values_to=\"value\")%&gt;%\n  inner_join(images,by=\"Breed\")\n\n\n\nlibrary(extrafont)\nlibrary(showtext)\nshowtext::showtext_auto()\nshowtext::showtext_opts(dpi=320)\nlibrary(sysfonts)\n#font_families_google()\nfont_add_google(name=\"Barlow Condensed\",family=\"dogs\")\n\nfamily = \"dogs\"\n\n\n\nggforce::geom_ellipse to make the paws\nggimage::geom_image to add the images\n\n\ndog_prints_plot &lt;-\n  \n  ggplot(my_df,aes(id,value,group=Breed))+\n  \n  # this is the largest part of the paw\n  ggforce::geom_ellipse(aes(x0=id,y0=value-0.2,a=0.2,b=0.25,\n                            angle=50,color=Breed),fill=\"#393d36\",size=0.5,alpha=0.7)+\n  # the following four ellipses are small extrems of the paw\n  ggforce::geom_ellipse(aes(x0=id+0.22,y0=value+0.2,a=0.1,b=0.12, \n                            angle=50,color=Breed),fill=\"#393d36\",size=0.5,alpha=0.7)+\n  ggforce::geom_ellipse(aes(x0=id-0.2,y0=value+0.15,a=0.1,b=0.12,\n                            angle=50,color=Breed),fill=\"#393d36\",size=0.5,alpha=0.7)+\n  ggforce::geom_ellipse(aes(x0=id+0.01,y0=value+0.25,a=0.1,b=0.1,\n                            angle=50,color=Breed),fill=\"#393d36\",size=0.5,alpha=0.7)+\n  ggforce::geom_ellipse(aes(x0=id-0.32,y0=value-0.15,a=0.1,b=0.1,\n                            angle=50,color=Breed),fill=\"#393d36\",size=0.5,alpha=0.7)+\n  # this is the grey part of the paw  \n  ggforce::geom_ellipse(aes(x0=id,y0=value-0.3,a=0.08,b=0.08,\n                            angle=50),color=\"grey65\",size=0.02,fill=\"grey65\",alpha=0.1) +\n  # The segment is to make the arrow up\n  geom_segment(x=0.43,xend=0.43,y=0,yend=6, size=3,color=\"grey65\",\n               arrow = arrow(length = unit(c(0, 0, 0.4, 0.4), 'cm')))+\n  \n  # description of the factors\n  geom_text(aes(label=factor),check_overlap = T, vjust=5.5, hjust=0.5, size=2.5, color=\"midnightblue\",family=family)+\n  \n  # The breed's names\n  geom_text(aes(x=id,y=6.5,label=Breed),color=\"black\",size=5.5,family=family,face=\"bold\")+\n  \n  # Images of the breeds\n  ggimage::geom_image(aes(x=id,y=-1.4,image=Image),size=0.2)+\n  \n  # expand the canvas to make the paws standing out\n  scale_y_continuous(expand = c(0,1))+\n  ylim(-1.5,7)+\n  coord_cartesian()+\n  labs(title=\"Top 5 selected Breeds: Was Goofy a dog?\", \n       caption=\"Datasource: American Kennel Club | #TidyTuesday 2022-w5 | Viz: Federica Gazzelloni\")+\n  ggthemes::theme_fivethirtyeight()+\n  theme(text = element_text(family = family,size=14),\n        plot.title = element_text(size=18,hjust=0.1),\n        plot.caption = element_text(face = \"bold\",vjust=0.5),\n        panel.grid.major.x = element_line(size=38),\n        panel.grid.major.y = element_blank(),\n        axis.text.y = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks.x = element_line(size=4),\n        plot.background = element_rect(color=\"white\",fill=\"white\"),\n        panel.background = element_rect(color=\"white\",fill=\"white\"),\n        legend.position = \"none\")\n\n\n\nlibrary(cowplot)\n\nfinal &lt;- ggdraw()+\n  draw_plot(dog_prints_plot)+\n  draw_image(\"print.png\",x=0.3,y=0.43,scale=0.16)+\n  draw_image(\"print.png\",x=0.4,y=0.43,scale=0.16)\n\nggsave(\"dog_prints_plot.png\",final)"
  },
  {
    "objectID": "content/dataviz/data_visualization/dogbreeds/index.html#overview",
    "href": "content/dataviz/data_visualization/dogbreeds/index.html#overview",
    "title": "Dog breeds",
    "section": "",
    "text": "This #TidyTuesday post is all about drawing with R. I have replicated the dogs‚Äô prints (or paws) using R. For solving this kind of tasks you need to be very ‚Äúpractice‚Äù with functions outcomes. And this means to have in mind the shape the function will take. For example we know that \\(y = x^2\\) would shape a parabolic function.\nSo that, I decided to get some extra inspiration and used the ggforce::geom_ellipse() function to produce ellipses of different sizes.\n\nThe data set for this week #TidyTuesday contains Dog breeds information, in three datasets.To load the data:\n\nbreed_traits &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-02-01/breed_traits.csv')\ntrait_description &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-02-01/trait_description.csv')\nbreed_rank_all &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-02-01/breed_rank.csv')\n\n\nA bit of data wrangling to make a rank dataset and see which are the 2020 favourite breeds.\n\nrank&lt;-breed_rank_all%&gt;%\n  count(Breed,`2020 Rank`,Image)%&gt;%\n  arrange(`2020 Rank`)%&gt;%\n  slice(1:10)%&gt;%\n  relocate(`2020 Rank`)%&gt;%\n  select(-n)\n\nrank\n\nSelect favourite breeds, and set the images\n\ndogs&lt;-c(\"Bulldogs\",\"Poodles\",\"Beagles\",\"Rottweilers\",\"Dachshunds\")\n\nimages &lt;-rank%&gt;%filter(Breed%in%dogs)%&gt;%select(Breed,Image)\n\nmy_df &lt;-breed_traits%&gt;%\n  select(-\"Coat Type\",-\"Coat Length\")%&gt;%\n  filter(Breed%in%dogs)%&gt;%\n  mutate(id=row_number())%&gt;%\n  relocate(id)%&gt;%\n  pivot_longer(cols=3:16,names_to=\"factor\",values_to=\"value\")%&gt;%\n  inner_join(images,by=\"Breed\")\n\n\n\nlibrary(extrafont)\nlibrary(showtext)\nshowtext::showtext_auto()\nshowtext::showtext_opts(dpi=320)\nlibrary(sysfonts)\n#font_families_google()\nfont_add_google(name=\"Barlow Condensed\",family=\"dogs\")\n\nfamily = \"dogs\"\n\n\n\nggforce::geom_ellipse to make the paws\nggimage::geom_image to add the images\n\n\ndog_prints_plot &lt;-\n  \n  ggplot(my_df,aes(id,value,group=Breed))+\n  \n  # this is the largest part of the paw\n  ggforce::geom_ellipse(aes(x0=id,y0=value-0.2,a=0.2,b=0.25,\n                            angle=50,color=Breed),fill=\"#393d36\",size=0.5,alpha=0.7)+\n  # the following four ellipses are small extrems of the paw\n  ggforce::geom_ellipse(aes(x0=id+0.22,y0=value+0.2,a=0.1,b=0.12, \n                            angle=50,color=Breed),fill=\"#393d36\",size=0.5,alpha=0.7)+\n  ggforce::geom_ellipse(aes(x0=id-0.2,y0=value+0.15,a=0.1,b=0.12,\n                            angle=50,color=Breed),fill=\"#393d36\",size=0.5,alpha=0.7)+\n  ggforce::geom_ellipse(aes(x0=id+0.01,y0=value+0.25,a=0.1,b=0.1,\n                            angle=50,color=Breed),fill=\"#393d36\",size=0.5,alpha=0.7)+\n  ggforce::geom_ellipse(aes(x0=id-0.32,y0=value-0.15,a=0.1,b=0.1,\n                            angle=50,color=Breed),fill=\"#393d36\",size=0.5,alpha=0.7)+\n  # this is the grey part of the paw  \n  ggforce::geom_ellipse(aes(x0=id,y0=value-0.3,a=0.08,b=0.08,\n                            angle=50),color=\"grey65\",size=0.02,fill=\"grey65\",alpha=0.1) +\n  # The segment is to make the arrow up\n  geom_segment(x=0.43,xend=0.43,y=0,yend=6, size=3,color=\"grey65\",\n               arrow = arrow(length = unit(c(0, 0, 0.4, 0.4), 'cm')))+\n  \n  # description of the factors\n  geom_text(aes(label=factor),check_overlap = T, vjust=5.5, hjust=0.5, size=2.5, color=\"midnightblue\",family=family)+\n  \n  # The breed's names\n  geom_text(aes(x=id,y=6.5,label=Breed),color=\"black\",size=5.5,family=family,face=\"bold\")+\n  \n  # Images of the breeds\n  ggimage::geom_image(aes(x=id,y=-1.4,image=Image),size=0.2)+\n  \n  # expand the canvas to make the paws standing out\n  scale_y_continuous(expand = c(0,1))+\n  ylim(-1.5,7)+\n  coord_cartesian()+\n  labs(title=\"Top 5 selected Breeds: Was Goofy a dog?\", \n       caption=\"Datasource: American Kennel Club | #TidyTuesday 2022-w5 | Viz: Federica Gazzelloni\")+\n  ggthemes::theme_fivethirtyeight()+\n  theme(text = element_text(family = family,size=14),\n        plot.title = element_text(size=18,hjust=0.1),\n        plot.caption = element_text(face = \"bold\",vjust=0.5),\n        panel.grid.major.x = element_line(size=38),\n        panel.grid.major.y = element_blank(),\n        axis.text.y = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks.x = element_line(size=4),\n        plot.background = element_rect(color=\"white\",fill=\"white\"),\n        panel.background = element_rect(color=\"white\",fill=\"white\"),\n        legend.position = \"none\")\n\n\n\nlibrary(cowplot)\n\nfinal &lt;- ggdraw()+\n  draw_plot(dog_prints_plot)+\n  draw_image(\"print.png\",x=0.3,y=0.43,scale=0.16)+\n  draw_image(\"print.png\",x=0.4,y=0.43,scale=0.16)\n\nggsave(\"dog_prints_plot.png\",final)"
  },
  {
    "objectID": "content/dataviz/data_visualization/dogbreeds/index.html#resources",
    "href": "content/dataviz/data_visualization/dogbreeds/index.html#resources",
    "title": "Dog breeds",
    "section": "Resources:",
    "text": "Resources:\n\nplotting-math-functions-in-r"
  },
  {
    "objectID": "content/dataviz/data_visualization/ggmap/index.html",
    "href": "content/dataviz/data_visualization/ggmap/index.html",
    "title": "How to get started with ggmap",
    "section": "",
    "text": "For this #30DayMapChallenge 2023 Day6 - Asia let‚Äôs explore the Population estimation for Regions and major Cities, from different sources.\nAlso, we will be looking at how to get started with ggmap to find the geocodes for the major cities in Asia.\n\n\nLet‚Äôs scrap the table of the Major Cities in Asia along with the Population level from Wikipedia.org.\nLoad the first set of libraries:\n\nlibrary(rvest)\nlibrary(stringr)\nlibrary(tidyverse)\n\n\nhtml.population &lt;- read_html('https://en.wikipedia.org/wiki/List_of_Asian_cities_by_population_within_city_limits')\n\ndf.asia_cities &lt;- html.population %&gt;%\n  html_nodes(\"table\") %&gt;%\n  .[[1]] %&gt;%\n  html_table(fill = TRUE)\n\ndf.asia_cities %&gt;% names()\n\n\ndf.asia_cities%&gt;%select(1,2,4)%&gt;%head\n\nSelect only the vectors of interest and clean data.\n\ndf.asia_cities &lt;- df.asia_cities[c(1,2,4)]\n\nasia_cities &lt;- df.asia_cities %&gt;%\n    mutate(Population = str_replace_all(Population, \"\\\\[.*\\\\]\",\"\") %&gt;% parse_number(),\n           City_full= str_c(df.asia_cities$City, df.asia_cities$Nation, sep = ', ')) %&gt;%\n    select(City, Nation, City_full, Population)%&gt;%\n    filter(!str_detect(Nation,\"Russia|Turkey\"),\n           !is.na(Population))\n\n\nasia_cities %&gt;% head()\n\n\nTo find the Asia City Geocodes we use the geocode() function from the {ggmap} package.\n\n\n\n\n\n\nIn order to get started with {ggmap} follow these steps:\n\n\nInstall the package from source:\nremotes::install_github(‚Äúdkahle/ggmap‚Äù)\n\nGet started with Google Maps Platform https://developers.google.com/maps all you need to do, if you do not have access to the platform yet, is to get started a free trial by adding your bank account information (if you do not want to continue after the trials ends you can stop it without charges)\nGo to on the left-side bar menu and select Overview then ENABLE APIs\nGo to APIs & Services to check enabled APIs\nGo to Keys and Credentials and click +CREATE CREDENTIALS on the top-side bar\n\nCopy the API key and paste it in the register_google() function, the option write = T will save the credentials for future use in your .Renviron file:\nggmap::register_google(, write = T)\n\n\n\n\n\nOnce you are all set try:\n\nlibrary(ggmap)\n\n?ggmap::geocode\n\ndata.geo &lt;- geocode(c(\"waco, texas\"))\n\ngeocode(\"waco texas\", output = \"latlona\")\n\n\ndata.geo%&gt;%head\n\n\nasia_cities_full &lt;- cbind(asia_cities, data.geo)\n# inspect\nasia_cities_full %&gt;% head() \n\n\nLet‚Äôs have a look at the map of Asia with {ggmap}.\n\nmap.asia &lt;- get_map('Asia', zoom = 3)\nmap.asia %&gt;% ggmap()\n\nFor this challenge we will be using another package for the polygons of Asia, the {rworldmap} package.\ninstall.packages(\"rworldmap\")\n\nlibrary(rworldmap)\n\n\nworldmap &lt;- rworldmap::getMap(resolution = \"high\")\ndim(worldmap)\n\nHave a look at the regions and choose Asia.\n\nt(t(table(worldmap$REGION)))\n\n\nasia &lt;- worldmap[which(worldmap$REGION==\"Asia\"),]\nasia%&gt;%class\n\nAs it is a spatial polygon dataframe, and we‚Äôd like to use the geom_sf() function from the ggplot2 package, we transform it to a simple feature object with st_as_sf() function from the sf package.\n\nlibrary(sf)\n\n\nasia_sf &lt;- asia %&gt;%\n  st_as_sf()\n\nasia_sf %&gt;% class()\n\n\nTo map the continent with population estimation by state we can set the option fill= POP_EST.\n\nasia_sf %&gt;%\n  ggplot()+\n  geom_sf(aes(fill=POP_EST))+\n  scale_fill_continuous()\n\n\nInteresting is looking at a different classification of the population classes, and we do this by using the classIntervals() function from the classInt package for classifying the Population Estimation by quantile.\nLet‚Äôs have a look at the population quantiles first. What we can see are the min and the max levels, and the values of the three quantiles, 25%, 50% (median), and the 75%. Which estimation of population follow in each quantile class.\nThe median population estimate for Asia is around 18 million, with some regions having populations of less than 1.5 billion people.\n\nquantile(asia_sf$POP_EST, na.rm=TRUE)\n\n\nasia_sf%&gt;%\n  ggplot(aes(POP_EST))+\n  geom_histogram(aes(fill=SOVEREIGNT),bins = 20)+\n  geom_vline(aes(xintercept = mean(POP_EST)),color=\"lightblue\")+\n  geom_vline(aes(xintercept = median(POP_EST)),color=\"midnightblue\")+\n  geom_text(aes(x=9000000,y=9,label=\"median\"),size=2)+\n  geom_text(aes(x=50000000,y=9,label=\"mean\"),size=2)+\n  scale_x_log10(labels=scales::comma_format(scale = 1/1000),n.breaks =8)+\n  scale_fill_viridis_d()+\n  labs(x=\"Population Estimation (Thousands)\",\n       title=\"Asia Population Distribution\",\n       caption=\"DataSource: {rworldmap} | Graphic: @fgazzelloni\")+\n  ggthemes::theme_clean()+\n  theme(legend.text = element_text(size=5),\n        legend.key.size = unit(5,units = \"pt\"))\n\nWe use the classInt package to find custom intervals of the population. And set up a new object called brks.\n\nlibrary(classInt)\n\n\nbrks &lt;- classIntervals(asia_sf$POP_EST,\n                       n=10, \n                       style=\"quantile\")\nbrks\n\nSet the color scheme:\n\nbrks &lt;- brks$brks\ncolors &lt;- RColorBrewer::brewer.pal(length(brks), \"Spectral\")\n\nFinalize the dataset to use for the map with the population estimation interval cuts.\n\nregion_pop &lt;- asia_sf%&gt;%\n  select(POP_EST)%&gt;%\n  mutate(breaks=case_when(POP_EST &gt; 0 & POP_EST &lt; 625493.5 ~ \"[0,625493.5)\",\n                          POP_EST &gt;= 625493.5 & POP_EST &lt; 2691158 ~ \"[625493.5,2691158)\",\n                          POP_EST &gt;= 2691158 & POP_EST &lt; 4728016 ~ \"[2691158,4728016)\",\n                          POP_EST &gt;= 4728016 & POP_EST &lt; 6834942 ~ \"[4728016,6834942)\",\n                          POP_EST &gt;= 6834942 & POP_EST &lt; 17788961 ~ \"[6834942,17788961)\",\n                          POP_EST &gt;= 17788961 & POP_EST &lt; 23822783 ~ \"[17788961,23822783)\",\n                          POP_EST &gt;= 23822783 & POP_EST &lt; 28625005 ~ \"[23822783,28625005)\",\n                          POP_EST &gt;= 28625005 & POP_EST &lt; 65905410 ~ \"[28625005,65905410)\",\n                          POP_EST &gt;= 65905410 & POP_EST &lt; 141564781 ~ \"[65905410,141564781)\",\n                          POP_EST &gt;= 141564781 & POP_EST &lt;= 1338612968 ~ \"[141564781,1338612968]\"))\n\nSet some information about Asia Population on a text box with the geom_textbox() function from the ggtext package.\n\ntext &lt;- tibble(asia_text=c(\"As of 2022, Asia's 4.6B population thrives in diverse urban centers. Mumbai's density soars at 20.7K/km¬≤, while Tokyo boasts 6.3K/km¬≤. Asia's remarkable density and cultural richness make it the world's most populous and dynamic continent.\"))\n\n\n\nregion_pop %&gt;%\n  ggplot()+\n  geom_sf(aes(fill=breaks))+\n  scale_fill_manual(breaks=c(\"[0,625493.5)\",\"[625493.5,2691158)\",\n                               \"[2691158,4728016)\",\"[4728016,6834942)\",\n                             \n                             \"[6834942,17788961)\",\"[17788961,23822783)\",\n                               \"[23822783,28625005)\",\"[28625005,65905410)\",\n                               \"[65905410,141564781)\",\"[141564781,1338612968]\"),\n                      values=rev(colors))+\n  geom_point(data=asia_cities_full,\n             mapping=aes(lon,lat,size=Population),\n             shape=21,stroke=0.5,\n             alpha=0.7,\n             color=\"grey90\",\n             inherit.aes = F)+\n  scale_size_continuous(labels=scales::comma_format())+\n  geom_text(data=asia_cities_full,\n             mapping=aes(lon,lat,label=City),fontface=\"bold\",\n            check_overlap = T,\n            size=2.1,color=\"white\")+\n  ggtext::geom_textbox(data=text,\n                       mapping=aes(x=60,y=-6,label=text),\n                       size=1.8,width = 0.4,fill=\"grey90\",\n                       family = \"Gill Sans\",\n                       inherit.aes = F)+\n  geom_curve(x=50,xend=67,y=0,yend=20,\n               linewidth=0.2,curvature = -0.5,\n               arrow = arrow(angle=30, \n                             length = unit(0.1, \"inches\"),\n                             ends = \"last\", type = \"open\"),\n      color=\"white\")+\n    geom_curve(x=86,xend=140,y=-5,yend=33,\n               linewidth=0.2,\n               arrow = arrow(angle=30, \n                             length = unit(0.1, \"inches\"),\n                             ends = \"last\", type = \"open\"),\n      color=\"white\")+\n  labs(fill=\"Regions Population\",\n       size=\"Cities Population\",\n       title=\"Asia - Population Level\",\n       caption=\"#30DayMapChallenge 2023 Day6 - ASIA\\nDataSource: Wikipedia & ggmap | Map @fgazzelloni\")+\n  ggthemes::theme_map()+\n  theme(text=element_text(color=\"white\", family = \"Gill Sans\"),\n        plot.title = element_text(face=\"bold\",size=14),\n        plot.caption = element_text(hjust = 0),\n        plot.background = element_rect(fill=\"#4A4A4A\",color=\"#4A4A4A\"),\n        panel.background = element_rect(fill=\"#4A4A4A\",color=\"#4A4A4A\"),\n        legend.background = element_blank(),\n        legend.key = element_rect(color=\"#4A4A4A\",fill=\"#4A4A4A\"),\n        legend.position = \"right\",\n        legend.text = element_text(size=5.5),\n        legend.key.size = unit(5.5,units = \"pt\"))\n\n\nggsave(\"day6_asia.png\",\n       width = 7,height = 4,\n       bg=\"#4A4A4A\")\n\n\n\nhttps://cran.r-project.org/web/packages/ggmap/readme/README.html\nhttps://www.r-bloggers.com/2017/09/mapping-the-largest-cities-in-asia-using-r/"
  },
  {
    "objectID": "content/dataviz/data_visualization/ggmap/index.html#asia-cities-and-population-by-wikipedia.org",
    "href": "content/dataviz/data_visualization/ggmap/index.html#asia-cities-and-population-by-wikipedia.org",
    "title": "How to get started with ggmap",
    "section": "",
    "text": "Let‚Äôs scrap the table of the Major Cities in Asia along with the Population level from Wikipedia.org.\nLoad the first set of libraries:\n\nlibrary(rvest)\nlibrary(stringr)\nlibrary(tidyverse)\n\n\nhtml.population &lt;- read_html('https://en.wikipedia.org/wiki/List_of_Asian_cities_by_population_within_city_limits')\n\ndf.asia_cities &lt;- html.population %&gt;%\n  html_nodes(\"table\") %&gt;%\n  .[[1]] %&gt;%\n  html_table(fill = TRUE)\n\ndf.asia_cities %&gt;% names()\n\n\ndf.asia_cities%&gt;%select(1,2,4)%&gt;%head\n\nSelect only the vectors of interest and clean data.\n\ndf.asia_cities &lt;- df.asia_cities[c(1,2,4)]\n\nasia_cities &lt;- df.asia_cities %&gt;%\n    mutate(Population = str_replace_all(Population, \"\\\\[.*\\\\]\",\"\") %&gt;% parse_number(),\n           City_full= str_c(df.asia_cities$City, df.asia_cities$Nation, sep = ', ')) %&gt;%\n    select(City, Nation, City_full, Population)%&gt;%\n    filter(!str_detect(Nation,\"Russia|Turkey\"),\n           !is.na(Population))\n\n\nasia_cities %&gt;% head()"
  },
  {
    "objectID": "content/dataviz/data_visualization/ggmap/index.html#use-ggmap",
    "href": "content/dataviz/data_visualization/ggmap/index.html#use-ggmap",
    "title": "How to get started with ggmap",
    "section": "",
    "text": "To find the Asia City Geocodes we use the geocode() function from the {ggmap} package.\n\n\n\n\n\n\nIn order to get started with {ggmap} follow these steps:\n\n\nInstall the package from source:\nremotes::install_github(‚Äúdkahle/ggmap‚Äù)\n\nGet started with Google Maps Platform https://developers.google.com/maps all you need to do, if you do not have access to the platform yet, is to get started a free trial by adding your bank account information (if you do not want to continue after the trials ends you can stop it without charges)\nGo to on the left-side bar menu and select Overview then ENABLE APIs\nGo to APIs & Services to check enabled APIs\nGo to Keys and Credentials and click +CREATE CREDENTIALS on the top-side bar\n\nCopy the API key and paste it in the register_google() function, the option write = T will save the credentials for future use in your .Renviron file:\nggmap::register_google(, write = T)\n\n\n\n\n\nOnce you are all set try:\n\nlibrary(ggmap)\n\n?ggmap::geocode\n\ndata.geo &lt;- geocode(c(\"waco, texas\"))\n\ngeocode(\"waco texas\", output = \"latlona\")\n\n\ndata.geo%&gt;%head\n\n\nasia_cities_full &lt;- cbind(asia_cities, data.geo)\n# inspect\nasia_cities_full %&gt;% head()"
  },
  {
    "objectID": "content/dataviz/data_visualization/ggmap/index.html#mapping-asia-polygons",
    "href": "content/dataviz/data_visualization/ggmap/index.html#mapping-asia-polygons",
    "title": "How to get started with ggmap",
    "section": "",
    "text": "Let‚Äôs have a look at the map of Asia with {ggmap}.\n\nmap.asia &lt;- get_map('Asia', zoom = 3)\nmap.asia %&gt;% ggmap()\n\nFor this challenge we will be using another package for the polygons of Asia, the {rworldmap} package.\ninstall.packages(\"rworldmap\")\n\nlibrary(rworldmap)\n\n\nworldmap &lt;- rworldmap::getMap(resolution = \"high\")\ndim(worldmap)\n\nHave a look at the regions and choose Asia.\n\nt(t(table(worldmap$REGION)))\n\n\nasia &lt;- worldmap[which(worldmap$REGION==\"Asia\"),]\nasia%&gt;%class\n\nAs it is a spatial polygon dataframe, and we‚Äôd like to use the geom_sf() function from the ggplot2 package, we transform it to a simple feature object with st_as_sf() function from the sf package.\n\nlibrary(sf)\n\n\nasia_sf &lt;- asia %&gt;%\n  st_as_sf()\n\nasia_sf %&gt;% class()\n\n\nTo map the continent with population estimation by state we can set the option fill= POP_EST.\n\nasia_sf %&gt;%\n  ggplot()+\n  geom_sf(aes(fill=POP_EST))+\n  scale_fill_continuous()\n\n\nInteresting is looking at a different classification of the population classes, and we do this by using the classIntervals() function from the classInt package for classifying the Population Estimation by quantile.\nLet‚Äôs have a look at the population quantiles first. What we can see are the min and the max levels, and the values of the three quantiles, 25%, 50% (median), and the 75%. Which estimation of population follow in each quantile class.\nThe median population estimate for Asia is around 18 million, with some regions having populations of less than 1.5 billion people.\n\nquantile(asia_sf$POP_EST, na.rm=TRUE)\n\n\nasia_sf%&gt;%\n  ggplot(aes(POP_EST))+\n  geom_histogram(aes(fill=SOVEREIGNT),bins = 20)+\n  geom_vline(aes(xintercept = mean(POP_EST)),color=\"lightblue\")+\n  geom_vline(aes(xintercept = median(POP_EST)),color=\"midnightblue\")+\n  geom_text(aes(x=9000000,y=9,label=\"median\"),size=2)+\n  geom_text(aes(x=50000000,y=9,label=\"mean\"),size=2)+\n  scale_x_log10(labels=scales::comma_format(scale = 1/1000),n.breaks =8)+\n  scale_fill_viridis_d()+\n  labs(x=\"Population Estimation (Thousands)\",\n       title=\"Asia Population Distribution\",\n       caption=\"DataSource: {rworldmap} | Graphic: @fgazzelloni\")+\n  ggthemes::theme_clean()+\n  theme(legend.text = element_text(size=5),\n        legend.key.size = unit(5,units = \"pt\"))\n\nWe use the classInt package to find custom intervals of the population. And set up a new object called brks.\n\nlibrary(classInt)\n\n\nbrks &lt;- classIntervals(asia_sf$POP_EST,\n                       n=10, \n                       style=\"quantile\")\nbrks\n\nSet the color scheme:\n\nbrks &lt;- brks$brks\ncolors &lt;- RColorBrewer::brewer.pal(length(brks), \"Spectral\")\n\nFinalize the dataset to use for the map with the population estimation interval cuts.\n\nregion_pop &lt;- asia_sf%&gt;%\n  select(POP_EST)%&gt;%\n  mutate(breaks=case_when(POP_EST &gt; 0 & POP_EST &lt; 625493.5 ~ \"[0,625493.5)\",\n                          POP_EST &gt;= 625493.5 & POP_EST &lt; 2691158 ~ \"[625493.5,2691158)\",\n                          POP_EST &gt;= 2691158 & POP_EST &lt; 4728016 ~ \"[2691158,4728016)\",\n                          POP_EST &gt;= 4728016 & POP_EST &lt; 6834942 ~ \"[4728016,6834942)\",\n                          POP_EST &gt;= 6834942 & POP_EST &lt; 17788961 ~ \"[6834942,17788961)\",\n                          POP_EST &gt;= 17788961 & POP_EST &lt; 23822783 ~ \"[17788961,23822783)\",\n                          POP_EST &gt;= 23822783 & POP_EST &lt; 28625005 ~ \"[23822783,28625005)\",\n                          POP_EST &gt;= 28625005 & POP_EST &lt; 65905410 ~ \"[28625005,65905410)\",\n                          POP_EST &gt;= 65905410 & POP_EST &lt; 141564781 ~ \"[65905410,141564781)\",\n                          POP_EST &gt;= 141564781 & POP_EST &lt;= 1338612968 ~ \"[141564781,1338612968]\"))\n\nSet some information about Asia Population on a text box with the geom_textbox() function from the ggtext package.\n\ntext &lt;- tibble(asia_text=c(\"As of 2022, Asia's 4.6B population thrives in diverse urban centers. Mumbai's density soars at 20.7K/km¬≤, while Tokyo boasts 6.3K/km¬≤. Asia's remarkable density and cultural richness make it the world's most populous and dynamic continent.\"))"
  },
  {
    "objectID": "content/dataviz/data_visualization/ggmap/index.html#make-the-map",
    "href": "content/dataviz/data_visualization/ggmap/index.html#make-the-map",
    "title": "How to get started with ggmap",
    "section": "",
    "text": "region_pop %&gt;%\n  ggplot()+\n  geom_sf(aes(fill=breaks))+\n  scale_fill_manual(breaks=c(\"[0,625493.5)\",\"[625493.5,2691158)\",\n                               \"[2691158,4728016)\",\"[4728016,6834942)\",\n                             \n                             \"[6834942,17788961)\",\"[17788961,23822783)\",\n                               \"[23822783,28625005)\",\"[28625005,65905410)\",\n                               \"[65905410,141564781)\",\"[141564781,1338612968]\"),\n                      values=rev(colors))+\n  geom_point(data=asia_cities_full,\n             mapping=aes(lon,lat,size=Population),\n             shape=21,stroke=0.5,\n             alpha=0.7,\n             color=\"grey90\",\n             inherit.aes = F)+\n  scale_size_continuous(labels=scales::comma_format())+\n  geom_text(data=asia_cities_full,\n             mapping=aes(lon,lat,label=City),fontface=\"bold\",\n            check_overlap = T,\n            size=2.1,color=\"white\")+\n  ggtext::geom_textbox(data=text,\n                       mapping=aes(x=60,y=-6,label=text),\n                       size=1.8,width = 0.4,fill=\"grey90\",\n                       family = \"Gill Sans\",\n                       inherit.aes = F)+\n  geom_curve(x=50,xend=67,y=0,yend=20,\n               linewidth=0.2,curvature = -0.5,\n               arrow = arrow(angle=30, \n                             length = unit(0.1, \"inches\"),\n                             ends = \"last\", type = \"open\"),\n      color=\"white\")+\n    geom_curve(x=86,xend=140,y=-5,yend=33,\n               linewidth=0.2,\n               arrow = arrow(angle=30, \n                             length = unit(0.1, \"inches\"),\n                             ends = \"last\", type = \"open\"),\n      color=\"white\")+\n  labs(fill=\"Regions Population\",\n       size=\"Cities Population\",\n       title=\"Asia - Population Level\",\n       caption=\"#30DayMapChallenge 2023 Day6 - ASIA\\nDataSource: Wikipedia & ggmap | Map @fgazzelloni\")+\n  ggthemes::theme_map()+\n  theme(text=element_text(color=\"white\", family = \"Gill Sans\"),\n        plot.title = element_text(face=\"bold\",size=14),\n        plot.caption = element_text(hjust = 0),\n        plot.background = element_rect(fill=\"#4A4A4A\",color=\"#4A4A4A\"),\n        panel.background = element_rect(fill=\"#4A4A4A\",color=\"#4A4A4A\"),\n        legend.background = element_blank(),\n        legend.key = element_rect(color=\"#4A4A4A\",fill=\"#4A4A4A\"),\n        legend.position = \"right\",\n        legend.text = element_text(size=5.5),\n        legend.key.size = unit(5.5,units = \"pt\"))\n\n\nggsave(\"day6_asia.png\",\n       width = 7,height = 4,\n       bg=\"#4A4A4A\")"
  },
  {
    "objectID": "content/dataviz/data_visualization/ggmap/index.html#resources",
    "href": "content/dataviz/data_visualization/ggmap/index.html#resources",
    "title": "How to get started with ggmap",
    "section": "",
    "text": "https://cran.r-project.org/web/packages/ggmap/readme/README.html\nhttps://www.r-bloggers.com/2017/09/mapping-the-largest-cities-in-asia-using-r/"
  },
  {
    "objectID": "content/dataviz/data_visualization/yarn/index.html",
    "href": "content/dataviz/data_visualization/yarn/index.html",
    "title": "Yarn: hierarchical edge bundling visualization",
    "section": "",
    "text": "This post is all about hierarchical edge bundling visualization, the dataset comes from #TidyTuesday 2022 week 41 Ravelry data.\nThe picture below is the result of the hierarchical edge bundling visualization.\n\nFirst thing load the libraries and set the fonts:\n\nlibrary(tidyverse)\nlibrary(igraph)\nlibrary(ggraph)\nlibrary(RColorBrewer)\nlibrary(showtext)\nlibrary(sysfonts)\nlibrary(extrafont)\n\nshowtext::showtext_auto()\nshowtext::showtext_opts(dpi=320)\nfont_add_google(name=\"Pangolin\",family=\"pangolin\")\n\nHelpful tip is how to set the dpi option inside the showtext::showtext_opts function. This sets the size of your text, and it can be very useful when used in conjunction with the same option inside the ggsave function. If showtxet dpi is of a certain value, then you should set the ggsave dpi lower than that value to balance the text size outcome in your final .png file.\nA perfect result comes from a nice balance trade-off between the dpi of the two functions.\nLet‚Äôs have a look at the data, there are 100000 observation and 24 variables referring to the various types of yarns, companies, names, yardage, weights, textures, ratings, ‚Ä¶\n\nyarn &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-10-11/yarn.csv')\n\nyarn%&gt;%dim\n\n\nyarn%&gt;%names\n\nLet‚Äôs select the names, the textures and the yardage for the length of the yarn, which is different from type to type.\n\n  yarn %&gt;%\n  select(yarn_weight_name,\n         texture_clean,\n         texture_clean,yardage)%&gt;%\n  head()\n\nAnd, tidy the texture_clean a bit more, grouping for most common texture names such as merino, acrylic, cotton, nylon, aran, cashmere, wool, silk, jersey,‚Ä¶and calculate the yardage average.\nTidy data:\n\ndf &lt;- yarn%&gt;%\n  mutate(\n    texture_clean=case_when(str_detect(texture_clean,\n                                       \"merino\")~\"merino\",\n                            str_detect(texture_clean,\n                                            \"ply|plied|play|plies\")~\"ply\",\n                            str_detect(texture_clean,\n                                       \"acrylique|acrylic|polyacryl|acrilyc|acryt\")~\"acrylic\",\n                            str_detect(texture_clean,\"nylon\")~\"nylon\",\n                            str_detect(texture_clean,\"cotton\")~\"cotton\",\n                            str_detect(texture_clean,\"wool\")~\"wool\",\n                            str_detect(texture_clean,\"polyamide|polyamid\")~\"polyamid\",\n                            str_detect(texture_clean,\"angora\")~\"angora\",\n                            str_detect(texture_clean,\"cashmere\")~\"cashmere\",\n                            str_detect(texture_clean,\"aran\")~\"aran\",\n                            str_detect(texture_clean,\"silk\")~\"silk\",\n                            str_detect(texture_clean,\"jersey\")~\"jersey\",\n                            TRUE~texture_clean))%&gt;%\n  filter(str_detect(texture_clean,\n                    c(\"merino|ply|acrylic|nylon|cotton|wool|angora|cashmere|aran|silk|jersey\")))%&gt;%\n  count(texture_clean,yarn_weight_name,yardage,grams) %&gt;%\n  mutate(yarn_weight_name=case_when(yarn_weight_name==\"Aran / Worsted\"~\"Aran\",\n                                    yarn_weight_name==\"DK / Sport\"~\"DK\",\n                                    yarn_weight_name==\"Light Fingering\"~\"Fingering\",\n                                    yarn_weight_name==\"Super Bulky\"~\"Bulky\",\n                                    TRUE~yarn_weight_name))%&gt;%\n  filter(!yarn_weight_name==\"No weight specified\",!is.na(yarn_weight_name))%&gt;%\n  filter(!is.na(yardage),!is.na(grams))%&gt;%\n  select(-n)%&gt;%\n  group_by(yarn_weight_name,texture_clean)%&gt;%\n  summarise_all(.funs=mean)%&gt;%\n  select(yarn_weight_name,texture_clean,yardage)\n\n\ndf%&gt;%head\n\n\nNow, for setting the data ready for being used inside one of the ggraph functions, a new vector is created named YARN. This is done to have a central point to all the yarns‚Äô types.\nSo, what is needed is a dataframe with two columns from and to. Actually, what is needed are two dataframe hierarchy and vertices. As follow:\n\nd1&lt;- df%&gt;%\n  select(-texture_clean,-yardage)%&gt;%\n  mutate(from = \"YARN\",.before=everything())%&gt;%\n  rename(to = yarn_weight_name)\n  \nd2 &lt;- df%&gt;%\n  select(-yardage) %&gt;%\n  rename(from = yarn_weight_name, \n         to = texture_clean)\n  \n\nhierarchy &lt;- rbind(d1, d2)\nvertices &lt;- data.frame(name = unique(c(as.character(hierarchy$from), \n                                       as.character(hierarchy$to))) ) \n\nHierarchy:\n\nhierarchy%&gt;%head\n\nVertices:\n\nvertices%&gt;%head\n\nThen create the graph and the layout with graph_from_data_frame() function.\n\nmygraph &lt;- graph_from_data_frame(hierarchy, vertices=vertices )\n\n\nggraph(mygraph, layout = 'dendrogram', circular = F) + \n  geom_edge_diagonal()\n\n\nggraph(mygraph, layout = 'dendrogram', circular = T) + \n  geom_edge_diagonal()+\n  geom_node_point(color=\"navy\",size=5)\n\nI can even filter the leafs out to point just the main nodes:\n\nggraph(mygraph, layout = 'dendrogram', circular = T) + \n  geom_edge_diagonal()+\n  geom_node_point(aes(filter=!leaf),\n                  color=\"navy\",\n                  size=5)\n\nI like the circular type, and we can have a look at the inside calculation of the function with create_layout() specifying the type of layout as a dendrogram. It is a dataframe graph and it has the x, and y vectors, the leafs, and the names and other specifications.\n\ndf1 &lt;- create_layout(mygraph, layout = 'dendrogram')\ndf1%&gt;%class\n\n\ndf1%&gt;%head()\n\nTo add the labels to the leafs they would need to be oriented by a specific angle level, the reason for this is that the subgroups are not all the same.\nWhat is needed is a function for node angle adjustments and another similar function to adjust the horizontal distance of the text around the dendrogram. Likely the {ggraph} package provides a function for calculating the angles of your data:\n- node_angle(x,y)\n\nnode_angle(df1$x,df1$y,degrees = T)%&gt;%head()\n\nThese values need to be adjusted:\n\nnode_ang_adj &lt;- function(x,y) {\n  ifelse(node_angle(x,y) &gt; 90 & node_angle(x,y) &lt; 270 , \n         node_angle(x,y) + 180, node_angle(x,y))\n  }\n\nnode_hjust_adj &lt;- function(x,y) {\n  ifelse(node_angle(x,y) &gt; 90 & node_angle(x,y) &lt; 270 , 1,0)\n}\n\nFinally, we can make the hierarchical edge bundling visualization type circular dendrogram:\n\nggraph(mygraph, layout = 'dendrogram', circular = TRUE) + \n  geom_edge_diagonal(aes(color=factor(x)),\n                     alpha=0.9,\n                     show.legend = F) +\n  geom_node_point(aes(color=factor(x)),\n                  size=10,\n                  show.legend = F)+\n  geom_node_point(aes(color=factor(x)),\n                  size=10,\n                  shape=8,\n                  show.legend = F)+\n  geom_node_label(aes(filter=!leaf,label=name,color=factor(x)),\n                  label.padding = unit(0.1, \"lines\"),\n                  label.r = unit(0.1, \"lines\"),\n                  label.size = 0.1,\n                  family = \"pangolin\",\n                  fontface=\"bold\",\n                  show.legend = F,\n                  size=4, \n                  alpha=1)+\n  geom_node_text(aes(x = x*1.1, \n                     y=y*1.1, \n                     hjust = node_hjust_adj(x,y),\n                     angle=node_ang_adj(x,y),\n                     filter = leaf, \n                     label=name,\n                     color=factor(x)),\n                 family = \"pangolin\",\n                 fontface=\"bold\",\n                 show.legend = F,\n                 size=4, \n                 alpha=1)+\n  scale_color_manual(values = rep(RColorBrewer::brewer.pal(10,\"Paired\"),10))+\n  scale_x_discrete(expand = c(0,0.3))+\n  scale_y_discrete(expand = c(0,0.3))+\n  coord_fixed()+\n  labs(caption=\"What's inside your YARN?\\ntextures for each type\\n\\nDataSource: #TidyTuesday 2022 week41 Ravelry data\\nDataViz: Federica Gazzelloni (FG) Twitter: @fgazzelloni\\n\",\n       alt=\"Infographics\") +\n  theme_graph()+\n  theme(plot.margin = margin(5,5,5,5,unit = \"pt\"),\n        plot.caption = element_text(face=\"bold\",family=\"pangolin\"))\n\nSave it with setting dpi:\n\nggsave(\"featured.png\",\n      dpi=280,\n      bg=\"white\",\n      width = 9,height = 9)\n\n\n\nHierarchical edge bundling\nggplot extensions\nggraph\ntidygraph\nigraph"
  },
  {
    "objectID": "content/dataviz/data_visualization/yarn/index.html#overview",
    "href": "content/dataviz/data_visualization/yarn/index.html#overview",
    "title": "Yarn: hierarchical edge bundling visualization",
    "section": "",
    "text": "This post is all about hierarchical edge bundling visualization, the dataset comes from #TidyTuesday 2022 week 41 Ravelry data.\nThe picture below is the result of the hierarchical edge bundling visualization.\n\nFirst thing load the libraries and set the fonts:\n\nlibrary(tidyverse)\nlibrary(igraph)\nlibrary(ggraph)\nlibrary(RColorBrewer)\nlibrary(showtext)\nlibrary(sysfonts)\nlibrary(extrafont)\n\nshowtext::showtext_auto()\nshowtext::showtext_opts(dpi=320)\nfont_add_google(name=\"Pangolin\",family=\"pangolin\")\n\nHelpful tip is how to set the dpi option inside the showtext::showtext_opts function. This sets the size of your text, and it can be very useful when used in conjunction with the same option inside the ggsave function. If showtxet dpi is of a certain value, then you should set the ggsave dpi lower than that value to balance the text size outcome in your final .png file.\nA perfect result comes from a nice balance trade-off between the dpi of the two functions.\nLet‚Äôs have a look at the data, there are 100000 observation and 24 variables referring to the various types of yarns, companies, names, yardage, weights, textures, ratings, ‚Ä¶\n\nyarn &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-10-11/yarn.csv')\n\nyarn%&gt;%dim\n\n\nyarn%&gt;%names\n\nLet‚Äôs select the names, the textures and the yardage for the length of the yarn, which is different from type to type.\n\n  yarn %&gt;%\n  select(yarn_weight_name,\n         texture_clean,\n         texture_clean,yardage)%&gt;%\n  head()\n\nAnd, tidy the texture_clean a bit more, grouping for most common texture names such as merino, acrylic, cotton, nylon, aran, cashmere, wool, silk, jersey,‚Ä¶and calculate the yardage average.\nTidy data:\n\ndf &lt;- yarn%&gt;%\n  mutate(\n    texture_clean=case_when(str_detect(texture_clean,\n                                       \"merino\")~\"merino\",\n                            str_detect(texture_clean,\n                                            \"ply|plied|play|plies\")~\"ply\",\n                            str_detect(texture_clean,\n                                       \"acrylique|acrylic|polyacryl|acrilyc|acryt\")~\"acrylic\",\n                            str_detect(texture_clean,\"nylon\")~\"nylon\",\n                            str_detect(texture_clean,\"cotton\")~\"cotton\",\n                            str_detect(texture_clean,\"wool\")~\"wool\",\n                            str_detect(texture_clean,\"polyamide|polyamid\")~\"polyamid\",\n                            str_detect(texture_clean,\"angora\")~\"angora\",\n                            str_detect(texture_clean,\"cashmere\")~\"cashmere\",\n                            str_detect(texture_clean,\"aran\")~\"aran\",\n                            str_detect(texture_clean,\"silk\")~\"silk\",\n                            str_detect(texture_clean,\"jersey\")~\"jersey\",\n                            TRUE~texture_clean))%&gt;%\n  filter(str_detect(texture_clean,\n                    c(\"merino|ply|acrylic|nylon|cotton|wool|angora|cashmere|aran|silk|jersey\")))%&gt;%\n  count(texture_clean,yarn_weight_name,yardage,grams) %&gt;%\n  mutate(yarn_weight_name=case_when(yarn_weight_name==\"Aran / Worsted\"~\"Aran\",\n                                    yarn_weight_name==\"DK / Sport\"~\"DK\",\n                                    yarn_weight_name==\"Light Fingering\"~\"Fingering\",\n                                    yarn_weight_name==\"Super Bulky\"~\"Bulky\",\n                                    TRUE~yarn_weight_name))%&gt;%\n  filter(!yarn_weight_name==\"No weight specified\",!is.na(yarn_weight_name))%&gt;%\n  filter(!is.na(yardage),!is.na(grams))%&gt;%\n  select(-n)%&gt;%\n  group_by(yarn_weight_name,texture_clean)%&gt;%\n  summarise_all(.funs=mean)%&gt;%\n  select(yarn_weight_name,texture_clean,yardage)\n\n\ndf%&gt;%head\n\n\nNow, for setting the data ready for being used inside one of the ggraph functions, a new vector is created named YARN. This is done to have a central point to all the yarns‚Äô types.\nSo, what is needed is a dataframe with two columns from and to. Actually, what is needed are two dataframe hierarchy and vertices. As follow:\n\nd1&lt;- df%&gt;%\n  select(-texture_clean,-yardage)%&gt;%\n  mutate(from = \"YARN\",.before=everything())%&gt;%\n  rename(to = yarn_weight_name)\n  \nd2 &lt;- df%&gt;%\n  select(-yardage) %&gt;%\n  rename(from = yarn_weight_name, \n         to = texture_clean)\n  \n\nhierarchy &lt;- rbind(d1, d2)\nvertices &lt;- data.frame(name = unique(c(as.character(hierarchy$from), \n                                       as.character(hierarchy$to))) ) \n\nHierarchy:\n\nhierarchy%&gt;%head\n\nVertices:\n\nvertices%&gt;%head\n\nThen create the graph and the layout with graph_from_data_frame() function.\n\nmygraph &lt;- graph_from_data_frame(hierarchy, vertices=vertices )\n\n\nggraph(mygraph, layout = 'dendrogram', circular = F) + \n  geom_edge_diagonal()\n\n\nggraph(mygraph, layout = 'dendrogram', circular = T) + \n  geom_edge_diagonal()+\n  geom_node_point(color=\"navy\",size=5)\n\nI can even filter the leafs out to point just the main nodes:\n\nggraph(mygraph, layout = 'dendrogram', circular = T) + \n  geom_edge_diagonal()+\n  geom_node_point(aes(filter=!leaf),\n                  color=\"navy\",\n                  size=5)\n\nI like the circular type, and we can have a look at the inside calculation of the function with create_layout() specifying the type of layout as a dendrogram. It is a dataframe graph and it has the x, and y vectors, the leafs, and the names and other specifications.\n\ndf1 &lt;- create_layout(mygraph, layout = 'dendrogram')\ndf1%&gt;%class\n\n\ndf1%&gt;%head()\n\nTo add the labels to the leafs they would need to be oriented by a specific angle level, the reason for this is that the subgroups are not all the same.\nWhat is needed is a function for node angle adjustments and another similar function to adjust the horizontal distance of the text around the dendrogram. Likely the {ggraph} package provides a function for calculating the angles of your data:\n- node_angle(x,y)\n\nnode_angle(df1$x,df1$y,degrees = T)%&gt;%head()\n\nThese values need to be adjusted:\n\nnode_ang_adj &lt;- function(x,y) {\n  ifelse(node_angle(x,y) &gt; 90 & node_angle(x,y) &lt; 270 , \n         node_angle(x,y) + 180, node_angle(x,y))\n  }\n\nnode_hjust_adj &lt;- function(x,y) {\n  ifelse(node_angle(x,y) &gt; 90 & node_angle(x,y) &lt; 270 , 1,0)\n}\n\nFinally, we can make the hierarchical edge bundling visualization type circular dendrogram:\n\nggraph(mygraph, layout = 'dendrogram', circular = TRUE) + \n  geom_edge_diagonal(aes(color=factor(x)),\n                     alpha=0.9,\n                     show.legend = F) +\n  geom_node_point(aes(color=factor(x)),\n                  size=10,\n                  show.legend = F)+\n  geom_node_point(aes(color=factor(x)),\n                  size=10,\n                  shape=8,\n                  show.legend = F)+\n  geom_node_label(aes(filter=!leaf,label=name,color=factor(x)),\n                  label.padding = unit(0.1, \"lines\"),\n                  label.r = unit(0.1, \"lines\"),\n                  label.size = 0.1,\n                  family = \"pangolin\",\n                  fontface=\"bold\",\n                  show.legend = F,\n                  size=4, \n                  alpha=1)+\n  geom_node_text(aes(x = x*1.1, \n                     y=y*1.1, \n                     hjust = node_hjust_adj(x,y),\n                     angle=node_ang_adj(x,y),\n                     filter = leaf, \n                     label=name,\n                     color=factor(x)),\n                 family = \"pangolin\",\n                 fontface=\"bold\",\n                 show.legend = F,\n                 size=4, \n                 alpha=1)+\n  scale_color_manual(values = rep(RColorBrewer::brewer.pal(10,\"Paired\"),10))+\n  scale_x_discrete(expand = c(0,0.3))+\n  scale_y_discrete(expand = c(0,0.3))+\n  coord_fixed()+\n  labs(caption=\"What's inside your YARN?\\ntextures for each type\\n\\nDataSource: #TidyTuesday 2022 week41 Ravelry data\\nDataViz: Federica Gazzelloni (FG) Twitter: @fgazzelloni\\n\",\n       alt=\"Infographics\") +\n  theme_graph()+\n  theme(plot.margin = margin(5,5,5,5,unit = \"pt\"),\n        plot.caption = element_text(face=\"bold\",family=\"pangolin\"))\n\nSave it with setting dpi:\n\nggsave(\"featured.png\",\n      dpi=280,\n      bg=\"white\",\n      width = 9,height = 9)\n\n\n\nHierarchical edge bundling\nggplot extensions\nggraph\ntidygraph\nigraph"
  },
  {
    "objectID": "content/dataviz/data_visualization/dubois2024/challenge01/index.html",
    "href": "content/dataviz/data_visualization/dubois2024/challenge01/index.html",
    "title": "#DuboisChallenge2024 Challenge 01",
    "section": "",
    "text": "My contributions to the #DuboisChallenge2024.\n\n\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(patchwork)"
  },
  {
    "objectID": "content/dataviz/data_visualization/dubois2024/challenge01/index.html#overview",
    "href": "content/dataviz/data_visualization/dubois2024/challenge01/index.html#overview",
    "title": "#DuboisChallenge2024 Challenge 01",
    "section": "",
    "text": "My contributions to the #DuboisChallenge2024.\n\n\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(patchwork)"
  },
  {
    "objectID": "content/dataviz/data_visualization/dubois2024/challenge01/index.html#data-ready-from-the-github-repo",
    "href": "content/dataviz/data_visualization/dubois2024/challenge01/index.html#data-ready-from-the-github-repo",
    "title": "#DuboisChallenge2024 Challenge 01",
    "section": "Data ready from the GitHub Repo",
    "text": "Data ready from the GitHub Repo\nDownload the georgia-1880-county-shapefile.zip file from: https://github.com/ajstarks/dubois-data-portraits/tree/master/challenge/2024/challenge01\n\ngeorgia_shp &lt;- sf::read_sf(\"data/georgia-1880-county-shapefile\")\n\n# georgia_shp%&gt;%head\n\n\ngeorgia_shp%&gt;%\n  ggplot()+\n  geom_sf()\n\n\ndat_sf &lt;- georgia_shp%&gt;%\n  janitor::clean_names()%&gt;%\n  separate(data1870,into=c(\"up70\",\"down70\"))%&gt;%\n  separate(data1880_p,into=c(\"up80\",\"down80\"))%&gt;%\n  mutate(# pop 1870\n         up70=ifelse(up70==\"\",0,up70),\n         down70=ifelse(is.na(down70),0,down70),\n         up70=as.numeric(up70),\n         down70=as.numeric(down70),\n         # pop 1880\n         up80=ifelse(up80==\"\",0,up80),\n         down80=ifelse(is.na(down80),0,down80),\n         up80=as.numeric(up80),\n         down80=as.numeric(down80))%&gt;%\n  rowwise()%&gt;%\n  mutate(pop70=mean(up70,down70),\n         pop80=mean(up80,down80))%&gt;%\n  arrange(pop70,pop80)\n\n\ndata &lt;- dat_sf%&gt;%select(county=icpsrnam,\n                pop70,pop80)%&gt;%\n  mutate(id=case_when(pop70 == 0 ~ 7,\n                      pop70 == 1000 ~ 6,\n                      pop70 == 2500 ~ 5,\n                      pop70 == 5000 ~ 4,\n                      pop70 == 10000 ~ 3,\n                      pop70 == 15000 ~ 2,\n                      pop70 == 20000 ~ 1),\n         pop70=case_when(pop70 == 0 ~ \"UNDER 1,000\",\n                         pop70 == 1000 ~ \"1000 TO 2,500\",\n                         pop70 == 2500 ~ \"2,500 TO 5,000\",\n                         pop70 == 5000 ~ \"5,000 TO 10,000\",\n                         pop70 == 10000 ~ \"10,000 TO 15,000\",\n                         pop70 == 15000 ~ \"15,000 TO 20,000\",\n                         pop70 == 20000 ~ \"20,000 TO 30,000\"))%&gt;%\n  # pop80\n    mutate(id=case_when(pop80 == 0 ~ 7,\n                      pop80 == 1000 ~ 6,\n                      pop80 == 2500 ~ 5,\n                      pop80 == 5000 ~ 4,\n                      pop80 == 10000 ~ 3,\n                      pop80 == 15000 ~ 2,\n                      pop80 == 20000 ~ 1),\n         pop80=case_when(pop80 == 0 ~ \"UNDER 1,000\",\n                         pop80 == 1000 ~ \"1000 TO 2,500\",\n                         pop80 == 2500 ~ \"2,500 TO 5,000\",\n                         pop80 == 5000 ~ \"5,000 TO 10,000\",\n                         pop80 == 10000 ~ \"10,000 TO 15,000\",\n                         pop80 == 15000 ~ \"15,000 TO 20,000\",\n                         pop80 == 20000 ~ \"20,000 TO 30,000\"))\ndata%&gt;%count(id,pop80)\n\nDybois Style\nFonts:\n\nlibrary(sysfonts)\nlibrary(showtext)\nsysfonts::font_add_google(\"Public Sans\",\"Public Sans\")\n# font_add_google(\"Carter One\", \"Carter One\")\nshowtext::showtext_auto()\nshowtext::showtext_opts(dpi=320)\n\nColors:\nBackground:\n\n\"#e7d6c5\"\n\nText:\n\nc(\"#483c32\",\"#bbaa98\")\n\n\nlegend_colors &lt;- c(\"#372c59\",\"#7a5039\",\"#c29e84\",\"#d63352\",\n  \"#e79d96\",\"#edb456\",\"#4b5c4f\")\n\nBounding box: xmin: 939223.1 ymin: -701249.8 xmax: 1425004 ymax: -200888.5\n\npop70_map &lt;- data%&gt;%\n  ggplot()+\n  geom_sf(aes(fill=pop70),\n          show.legend = F,\n          color=\"#483c32\",alpha=0.9,\n          linewidth=0.1)+\n  scale_fill_manual(values=c(\"UNDER 1,000\"=\"#4b5c4f\",\n                             \"1000 TO 2,500\"=\"#edb456\",\n                             \"2,500 TO 5,000\"=\"#e79d96\",\n                             \"5,000 TO 10,000\"=\"#d63352\",\n                             \"10,000 TO 15,000\"=\"#c29e84\",\n                             \"15,000 TO 20,000\"=\"#7a5039\",\n                             \"20,000 TO 30,000\"=\"#372c59\"),na.value = \"#e0cebb\")+\n    annotate(\"text\", x = -84.45, y = 35.1,\n           label = \"1870\",\n           size = 3.5,color=\"#483c32\",\n           fontface = \"bold\",\n           family =  \"Public Sans\" ) +\n   coord_sf(crs=4326,clip = \"off\")+\n  ggthemes::theme_map()+\n  theme(plot.background = element_rect(color=\"#e7d6c5\",fill=\"#e7d6c5\"),\n        panel.background = element_rect(color=\"#e7d6c5\",fill=\"#e7d6c5\"))\n  \npop70_map\n\n\npop80_map &lt;- data%&gt;%\n  ggplot()+\n  geom_sf(aes(fill=pop80),\n          show.legend = F,\n          color=\"#483c32\",alpha=0.9,\n          linewidth=0.1)+\n  scale_fill_manual(values=c(\"UNDER 1,000\"=\"#4b5c4f\",\n                             \"1000 TO 2,500\"=\"#edb456\",\n                             \"2,500 TO 5,000\"=\"#e79d96\",\n                             \"5,000 TO 10,000\"=\"#d63352\",\n                             \"10,000 TO 15,000\"=\"#c29e84\",\n                             \"15,000 TO 20,000\"=\"#7a5039\",\n                             \"20,000 TO 30,000\"=\"#372c59\"),na.value = \"#e0cebb\")+\n      annotate(\"text\", x = -84.45, y = 35.1,\n           label = \"1880\",\n           size = 3.5,color=\"#483c32\",\n           fontface = \"bold\",\n           family =  \"Public Sans\" ) +\n  coord_sf(crs=4326,clip = \"off\")+\n  ggthemes::theme_map()+\n  theme(plot.background = element_rect(color=\"#e7d6c5\",fill=\"#e7d6c5\"),\n        panel.background = element_rect(color=\"#e7d6c5\",fill=\"#e7d6c5\"))\n  \npop80_map\n\nPlot layout\nsource: https://ggplot2-book.org/arranging-plots\n\npop70_map+ ggplot() + ggplot()+ pop80_map + plot_layout(ncol = 2,nrow = 2)\n\n\nlegend1 &lt;- tibble(x=0,y=c(4,3,2,1),\n       label=c(\"5,000 TO 10,000\",\n                   \"2,500 TO 5,000\",\n                    \"1000 TO 2,500\",\n                   \"UNDER 1,000\"),\n       pal=c(\"#d63352\",\"#e79d96\",\"#edb456\",\"#4b5c4f\"))%&gt;%\n  mutate(pal=as.factor(pal))\nlegend1\n\n\nlegend1_plot &lt;- legend1%&gt;%\n  ggplot(aes(x,y))+\n  geom_point(aes(fill=label),\n             shape=21,stroke=0.1,\n             size=8.5,\n             show.legend = F)+\n  scale_fill_manual(values=c(\"5,000 TO 10,000\"=\"#d63352\",\n                                \"2,500 TO 5,000\"=\"#e79d96\",\n                                \"1000 TO 2,500\"=\"#edb456\",\n                                \"UNDER 1,000\"=\"#4b5c4f\"))+\n  geom_text(aes(label=label),\n            family=\"Public Sans\",\n            size=3.5,color=\"#7a5039\",\n            nudge_x = 0,hjust=-0.2)+\n  coord_cartesian(xlim=c(-0.2,1),ylim =c(-0,5) )+\n  ggthemes::theme_map()+\n  theme(plot.background = element_rect(color=\"#e7d6c5\",fill=\"#e7d6c5\"),\n        panel.background = element_rect(color=\"#e7d6c5\",fill=\"#e7d6c5\"))\nlegend1_plot\n\n\nlegend2 &lt;- tibble(x=0,y=c(1,2,3),\n       label=c(\"10,000 TO 15,000\",\n               \"15,000 TO 20,000\",\n               \"BETWEEN 20,000 AND 30,000\"),\n       color=c(\"#c29e84\",\"#7a5039\",\"#372c59\"))                             \n\n\nlegend2_plot &lt;- legend2%&gt;%\n  ggplot(aes(x,y))+\n  geom_point(aes(fill=label),\n             shape=21,stroke=0.1,\n             size=8.5,\n             show.legend = F)+\n  scale_fill_manual(values=c(\"10,000 TO 15,000\"=\"#c29e84\",\n                             \"15,000 TO 20,000\"=\"#7a5039\",\n                             \"BETWEEN 20,000 AND 30,000\"=\"#372c59\"))+\n  geom_text(aes(label=label),\n            size=3.5,color=\"#7a5039\",\n            family=\"Public Sans\",\n            nudge_x = 0.5,hjust=0)+\n  coord_cartesian(xlim=c(-0.2,7),ylim =c(-1,4) )+\n  ggthemes::theme_map()+\n  theme(plot.background = element_rect(color=\"#e7d6c5\",fill=\"#e7d6c5\"),\n        panel.background = element_rect(color=\"#e7d6c5\",fill=\"#e7d6c5\"))\n\n\nlegend2_plot\n\n\npop70_map+ legend2_plot + legend1_plot+ pop80_map + plot_layout(ncol = 2,nrow = 2)+plot_annotation(\n  title = \"NEGRO POPULATION OF GEORGIA BY COUNTIES.\",\n  caption=\"#DuboisChallenge24| Week1 | by Federica Gazzelloni\",\n  theme = theme_void(base_family = \"Public Sans\"))&\n  theme(text=element_text(color=\"#483c32\",face=\"bold\"),\n        plot.title = element_text(hjust=0.5),\n        plot.caption = element_text(size=9),\n        plot.background = element_rect(color=\"#e7d6c5\",fill=\"#e7d6c5\"),\n        panel.background = element_rect(color=\"#e7d6c5\",fill=\"#e7d6c5\"))\n\n\nggsave(\"challenge01.png\",bg=\"#e7d6c5\",height = 8.8)"
  },
  {
    "objectID": "content/dataviz/data_visualization/network/index.html",
    "href": "content/dataviz/data_visualization/network/index.html",
    "title": "Network of game mechanics",
    "section": "",
    "text": "I‚Äôve chosen this graphic for my blog because it turned out to be very interesting. As you can see reading through the article, the shape of the network changes along with the change of the parameters.\nThe dataset I‚Äôve used for making this network comes from #TidyTuesday 2022 week 4 Board games.\nThe picture below is the result of the network visualization.\n\nThe first step is to load the library needed for making the manupulations. I usually load {tidyverse} package because it contains a series of sub packages and functions that are all that is neede for thsi first part of the data wrangling. Also, it provides the pipe %&gt;% operator, which is useful for linking different functions through subsetting the dataset.\n\nlibrary(tidyverse)\n\nThe data sets provided can be loaded from the source like this:\n\nratings &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-25/ratings.csv')\ndetails &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-25/details.csv')\n\nI‚Äôve also added few line of code for backing the original datasets by saving them as .RDS files, a light file format to store information in.\n\nsaveRDS(ratings,\"ratings.rds\")\nsaveRDS(details,\"details.rds\")\n\nAnd assigned them to new variables:\n\nrat &lt;- readRDS(\"ratings.rds\")\ndet &lt;- readRDS(\"details.rds\")\n\n\n\nLet‚Äôs see the variable‚Äôs names inside the sets.\n\nnames(rat)\n\n\nnames(det)\n\nBased on the variables in the data sets, I‚Äôve started googling for some information nad/or visualizations about Board games, to see if I could find any inspiration from past submissions, and in fact found this source of inspiration: https://www.thewayir.com/blog/boardgames/. Looking through the article found the code and the type of visualization I had in mind, so started replicating the code from the article. My surprise was that data updating and my manipulation slightly changed the output of the plot.\nLet‚Äôs go a bit more in deep about that. I‚Äôll go through the steps for replicatiing the network but then sligtly change the output to what you can see in the picture.\nAmong the required libraries found {widyr} package which was very new to me.\n\nEncapsulates the pattern of untidying data into a wide matrix, performing some processing, then turning it back into a tidy form. This is useful for several operations such as co-occurrence counts, correlations, or clustering that are mathematically convenient.\n\nAnd then the other packages such as {igraph}, {ggraph}, and {ggforce}, all packages for making networks of data, and for making extra features.\n\nrequire(widyr)\nrequire(igraph)\nrequire(ggraph)\nrequire(ggforce)\n\n\n\nWhat‚Äôs the best manipulation for making a graph?\nHere is the first part of the data-wrangling\n\nboard_games &lt;- rat %&gt;%\n  select(id,name) %&gt;%\n  left_join(select(det,id,boardgamemechanic),by=\"id\") %&gt;%\n  rename(mechanic=boardgamemechanic) %&gt;%\n  tidyr::separate_rows(mechanic, sep = \",\") %&gt;% \n  mutate(mechanic = str_remove_all(mechanic, \"[[:punct:]]\"),\n         mechanic = str_trim(mechanic),\n         mechanic = gsub(\"^and \",\"\",mechanic)) %&gt;% \n  filter(!is.na(mechanic))\n\n\nkableExtra::kable(head(board_games))\n\nHere is the second part of the wrangling\n\nmechanic &lt;- board_games %&gt;% \n  count(mechanic,sort=T) %&gt;%\n  mutate(mechanic_pct=round(n/sum(n)*100,2))%&gt;%\n  left_join(select(board_games,name,mechanic),by=\"mechanic\") %&gt;%\n  mutate(name=as.factor(name),mechanic=as.factor(mechanic)) %&gt;% \n   distinct() \n\nThis part is for setting the fonts\n\nlibrary(extrafont)\nlibrary(showtext)\nshowtext::showtext_auto()\nshowtext::showtext_opts(dpi=320)\nlibrary(sysfonts)\n#font_families_google()\nfont_add_google(name=\"Piedra\",family=\"games\")\n\nfamily = \"games\"\n\nSelect the first 50 games\n\nboard_games50 &lt;-board_games%&gt;%\n  select(name,mechanic)%&gt;%\n  count(name,sort=T) %&gt;%\n  slice(1:50)\n\n\nThe interesting part is here: if we change the filtering level of the mechanic_pct and/or the widyr::pairwise_cor() from the widyr package we can see the graph changing along with it. More changes if the level of correlation changes to a lower value more than if set to a higher value.\n\ndf &lt;- board_games50%&gt;%\n  left_join(mechanic,by=\"name\") %&gt;%\n  filter(mechanic_pct &gt; 1) %&gt;%\n  pairwise_cor(mechanic, name, sort = T) %&gt;% \n  filter(correlation &gt; .1)\n\nThe function igraph::graph_from_data_frame() transform data frames into igraph graphs. In addition the funtion igraph::tkplot() can be useful for looking at the graph under different perspectives.\nThis is the final version of the plot:\n\nplot &lt;- df %&gt;% \n  igraph::graph_from_data_frame() %&gt;% \n  ggraph() +\n  geom_edge_link(linejoin = \"round\",\n                 color=\"grey5\",\n                 edge_colour=\"red\",\n                 edge_width=0.5,\n                 edge_linetype=\"solid\") +\n  geom_node_point(color=\"midnightblue\",size=40,alpha=0.4) +\n  geom_node_text(aes(label = name), \n                 repel = T,\n                 size=5,\n                 nudge_y = 0,\n                 color=\"orange\",\n                 family=family) + \n  theme_void() +\n   theme(text = element_text(family=family),\n         plot.background = element_rect(color=\"beige\",fill=\"beige\"))\n\n\n\n\nAdding some features and save\n\nlibrary(cowplot)\n\nfinal &lt;-ggdraw()+\n  draw_plot(plot) +\n  draw_label(\"Network of game \\nmechanics\",x=0.5,y=0.85,size=55,fontfamily=family)+\n  draw_label(\"Sliced by the first 50 games by frequency, \n             filtered mechanics greater than 2% proportion of total,\n             then finally taken just the most highly correlated ones\",\n             x=0.8,y=0.12,size=11,fontfamily=family) +\n  draw_label(\"DataSource: Kaggle & Board Games Geek | Viz: Federica Gazzelloni\",\n             x=0.8,y=0.03,angle=0,size=11,alpha=0.5,fontfamily=family) +\n   draw_image(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/static/plot_logo.png\",x=0.09,y=-0.47,scale=0.05)\n\n\nggsave(\"w4_board_games.png\",\n        plot =final,\n        bg=\"white\",\n        dpi = 320,\n        width = 11,\n        height = 6\n       )\n\n\n\ntidygraph\nigraph\nggnet"
  },
  {
    "objectID": "content/dataviz/data_visualization/network/index.html#overview",
    "href": "content/dataviz/data_visualization/network/index.html#overview",
    "title": "Network of game mechanics",
    "section": "",
    "text": "I‚Äôve chosen this graphic for my blog because it turned out to be very interesting. As you can see reading through the article, the shape of the network changes along with the change of the parameters.\nThe dataset I‚Äôve used for making this network comes from #TidyTuesday 2022 week 4 Board games.\nThe picture below is the result of the network visualization.\n\nThe first step is to load the library needed for making the manupulations. I usually load {tidyverse} package because it contains a series of sub packages and functions that are all that is neede for thsi first part of the data wrangling. Also, it provides the pipe %&gt;% operator, which is useful for linking different functions through subsetting the dataset.\n\nlibrary(tidyverse)\n\nThe data sets provided can be loaded from the source like this:\n\nratings &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-25/ratings.csv')\ndetails &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-25/details.csv')\n\nI‚Äôve also added few line of code for backing the original datasets by saving them as .RDS files, a light file format to store information in.\n\nsaveRDS(ratings,\"ratings.rds\")\nsaveRDS(details,\"details.rds\")\n\nAnd assigned them to new variables:\n\nrat &lt;- readRDS(\"ratings.rds\")\ndet &lt;- readRDS(\"details.rds\")\n\n\n\nLet‚Äôs see the variable‚Äôs names inside the sets.\n\nnames(rat)\n\n\nnames(det)\n\nBased on the variables in the data sets, I‚Äôve started googling for some information nad/or visualizations about Board games, to see if I could find any inspiration from past submissions, and in fact found this source of inspiration: https://www.thewayir.com/blog/boardgames/. Looking through the article found the code and the type of visualization I had in mind, so started replicating the code from the article. My surprise was that data updating and my manipulation slightly changed the output of the plot.\nLet‚Äôs go a bit more in deep about that. I‚Äôll go through the steps for replicatiing the network but then sligtly change the output to what you can see in the picture.\nAmong the required libraries found {widyr} package which was very new to me.\n\nEncapsulates the pattern of untidying data into a wide matrix, performing some processing, then turning it back into a tidy form. This is useful for several operations such as co-occurrence counts, correlations, or clustering that are mathematically convenient.\n\nAnd then the other packages such as {igraph}, {ggraph}, and {ggforce}, all packages for making networks of data, and for making extra features.\n\nrequire(widyr)\nrequire(igraph)\nrequire(ggraph)\nrequire(ggforce)\n\n\n\nWhat‚Äôs the best manipulation for making a graph?\nHere is the first part of the data-wrangling\n\nboard_games &lt;- rat %&gt;%\n  select(id,name) %&gt;%\n  left_join(select(det,id,boardgamemechanic),by=\"id\") %&gt;%\n  rename(mechanic=boardgamemechanic) %&gt;%\n  tidyr::separate_rows(mechanic, sep = \",\") %&gt;% \n  mutate(mechanic = str_remove_all(mechanic, \"[[:punct:]]\"),\n         mechanic = str_trim(mechanic),\n         mechanic = gsub(\"^and \",\"\",mechanic)) %&gt;% \n  filter(!is.na(mechanic))\n\n\nkableExtra::kable(head(board_games))\n\nHere is the second part of the wrangling\n\nmechanic &lt;- board_games %&gt;% \n  count(mechanic,sort=T) %&gt;%\n  mutate(mechanic_pct=round(n/sum(n)*100,2))%&gt;%\n  left_join(select(board_games,name,mechanic),by=\"mechanic\") %&gt;%\n  mutate(name=as.factor(name),mechanic=as.factor(mechanic)) %&gt;% \n   distinct() \n\nThis part is for setting the fonts\n\nlibrary(extrafont)\nlibrary(showtext)\nshowtext::showtext_auto()\nshowtext::showtext_opts(dpi=320)\nlibrary(sysfonts)\n#font_families_google()\nfont_add_google(name=\"Piedra\",family=\"games\")\n\nfamily = \"games\"\n\nSelect the first 50 games\n\nboard_games50 &lt;-board_games%&gt;%\n  select(name,mechanic)%&gt;%\n  count(name,sort=T) %&gt;%\n  slice(1:50)\n\n\nThe interesting part is here: if we change the filtering level of the mechanic_pct and/or the widyr::pairwise_cor() from the widyr package we can see the graph changing along with it. More changes if the level of correlation changes to a lower value more than if set to a higher value.\n\ndf &lt;- board_games50%&gt;%\n  left_join(mechanic,by=\"name\") %&gt;%\n  filter(mechanic_pct &gt; 1) %&gt;%\n  pairwise_cor(mechanic, name, sort = T) %&gt;% \n  filter(correlation &gt; .1)\n\nThe function igraph::graph_from_data_frame() transform data frames into igraph graphs. In addition the funtion igraph::tkplot() can be useful for looking at the graph under different perspectives.\nThis is the final version of the plot:\n\nplot &lt;- df %&gt;% \n  igraph::graph_from_data_frame() %&gt;% \n  ggraph() +\n  geom_edge_link(linejoin = \"round\",\n                 color=\"grey5\",\n                 edge_colour=\"red\",\n                 edge_width=0.5,\n                 edge_linetype=\"solid\") +\n  geom_node_point(color=\"midnightblue\",size=40,alpha=0.4) +\n  geom_node_text(aes(label = name), \n                 repel = T,\n                 size=5,\n                 nudge_y = 0,\n                 color=\"orange\",\n                 family=family) + \n  theme_void() +\n   theme(text = element_text(family=family),\n         plot.background = element_rect(color=\"beige\",fill=\"beige\"))\n\n\n\n\nAdding some features and save\n\nlibrary(cowplot)\n\nfinal &lt;-ggdraw()+\n  draw_plot(plot) +\n  draw_label(\"Network of game \\nmechanics\",x=0.5,y=0.85,size=55,fontfamily=family)+\n  draw_label(\"Sliced by the first 50 games by frequency, \n             filtered mechanics greater than 2% proportion of total,\n             then finally taken just the most highly correlated ones\",\n             x=0.8,y=0.12,size=11,fontfamily=family) +\n  draw_label(\"DataSource: Kaggle & Board Games Geek | Viz: Federica Gazzelloni\",\n             x=0.8,y=0.03,angle=0,size=11,alpha=0.5,fontfamily=family) +\n   draw_image(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/static/plot_logo.png\",x=0.09,y=-0.47,scale=0.05)\n\n\nggsave(\"w4_board_games.png\",\n        plot =final,\n        bg=\"white\",\n        dpi = 320,\n        width = 11,\n        height = 6\n       )\n\n\n\ntidygraph\nigraph\nggnet"
  },
  {
    "objectID": "content/dataviz/data_visualization/Statistics/index.html",
    "href": "content/dataviz/data_visualization/Statistics/index.html",
    "title": "Statistics",
    "section": "",
    "text": "This is one of my favourite visualizations. It looks like very simple, and straight forward with the use of the ggdist::stat_dots function to make dotted ditributions of the wages by highest educational status reached.\n\nThe Tidyverse libraries needed for the data manipulation:\n\nlibrary(tidyverse)\n\nThe data set is the Wage dataset from the {ISLR2} package. This package contains a variety of datasets used for statistical analysis in An Introduction to Statistical Learning book.\n\nlibrary(ISLR2)\ndata(Wage)\nwage_h &lt;- Wage%&gt;%group_by(education)%&gt;%summarize(avg_wage=mean(wage))\nkableExtra::kable(wage_h,row.names = F)\n\n\nA bit of data wrangling to group by education and calculate the mean value and the standard deviation of the wage.\n\nWage1 &lt;- Wage %&gt;%\n  mutate(education=gsub(\"\\\\d. \",\"\",education)) %&gt;% #count(year)\n  group_by(education)%&gt;%\n  mutate(mean=mean(wage),\n         sd=sd(wage)) %&gt;%\n  ungroup() %&gt;% # pull(mean)%&gt;%summary\n  select(education,mean,sd) %&gt;%\n  distinct()\n\n\n\nlibrary(extrafont)\n# loadfonts()\n\nFor this visualization I used: family = ‚ÄúChelsea Market‚Äù\n\n\nggdist::stat_dots to make the dots ditribution\ndistributional::dist_normal to normalize the data\n\n\nlibrary(ggdist)\nlibrary(distributional)\n\n\nWage1 %&gt;%\nggplot(aes(y=fct_reorder(education,mean),\n             xdist = dist_normal(mean, sd),\n             layout = \"weave\",\n             fill = stat(x &lt; 111.70))) + \n  stat_dots(position = \"dodge\", color = \"grey70\")+\n  geom_vline(xintercept = 111.70, alpha = 0.25) +\n  scale_x_continuous(breaks = c(20,60,90,112,140,180,220)) +\n  tvthemes::scale_fill_hilda()+\n  # add a title / subtitle and a caption ------\n  labs(x=\"Wage values from 2003 to 2009\",\n       y=\"\",color=\"Race\",fill=\"wage &lt; avg\",\n       title=\"Wage distribution vs education 2003-2009\",\n       subtitle=\"Normalized values\",\n       caption=\"#30DayChartChallenge 2022 #day9 - Distribution/Statistics - v2\\nDataSource: {ISLR2} Wage dataset | DataViz: Federica Gazzelloni\") +\n  # set a customized theme -------\n  tvthemes::theme_avatar() +\n  theme(text = element_text(family=\"Chelsea Market\"),\n        legend.background = element_blank(),\n        legend.box.background = element_blank(),\n        legend.key = element_blank(),\n        legend.key.width = unit(0.5,units=\"cm\"),\n        legend.direction = \"horizontal\",\n        legend.position = c(0.8,0.1))\n\nIf you‚Äôd like to save it as .png you can do it with ggsave()\n\nggsave(\"day9_statistics_v2.png\",\n       dpi=320,\n       width = 9,\n       height = 6)"
  },
  {
    "objectID": "content/dataviz/data_visualization/Statistics/index.html#overview",
    "href": "content/dataviz/data_visualization/Statistics/index.html#overview",
    "title": "Statistics",
    "section": "",
    "text": "This is one of my favourite visualizations. It looks like very simple, and straight forward with the use of the ggdist::stat_dots function to make dotted ditributions of the wages by highest educational status reached.\n\nThe Tidyverse libraries needed for the data manipulation:\n\nlibrary(tidyverse)\n\nThe data set is the Wage dataset from the {ISLR2} package. This package contains a variety of datasets used for statistical analysis in An Introduction to Statistical Learning book.\n\nlibrary(ISLR2)\ndata(Wage)\nwage_h &lt;- Wage%&gt;%group_by(education)%&gt;%summarize(avg_wage=mean(wage))\nkableExtra::kable(wage_h,row.names = F)\n\n\nA bit of data wrangling to group by education and calculate the mean value and the standard deviation of the wage.\n\nWage1 &lt;- Wage %&gt;%\n  mutate(education=gsub(\"\\\\d. \",\"\",education)) %&gt;% #count(year)\n  group_by(education)%&gt;%\n  mutate(mean=mean(wage),\n         sd=sd(wage)) %&gt;%\n  ungroup() %&gt;% # pull(mean)%&gt;%summary\n  select(education,mean,sd) %&gt;%\n  distinct()\n\n\n\nlibrary(extrafont)\n# loadfonts()\n\nFor this visualization I used: family = ‚ÄúChelsea Market‚Äù\n\n\nggdist::stat_dots to make the dots ditribution\ndistributional::dist_normal to normalize the data\n\n\nlibrary(ggdist)\nlibrary(distributional)\n\n\nWage1 %&gt;%\nggplot(aes(y=fct_reorder(education,mean),\n             xdist = dist_normal(mean, sd),\n             layout = \"weave\",\n             fill = stat(x &lt; 111.70))) + \n  stat_dots(position = \"dodge\", color = \"grey70\")+\n  geom_vline(xintercept = 111.70, alpha = 0.25) +\n  scale_x_continuous(breaks = c(20,60,90,112,140,180,220)) +\n  tvthemes::scale_fill_hilda()+\n  # add a title / subtitle and a caption ------\n  labs(x=\"Wage values from 2003 to 2009\",\n       y=\"\",color=\"Race\",fill=\"wage &lt; avg\",\n       title=\"Wage distribution vs education 2003-2009\",\n       subtitle=\"Normalized values\",\n       caption=\"#30DayChartChallenge 2022 #day9 - Distribution/Statistics - v2\\nDataSource: {ISLR2} Wage dataset | DataViz: Federica Gazzelloni\") +\n  # set a customized theme -------\n  tvthemes::theme_avatar() +\n  theme(text = element_text(family=\"Chelsea Market\"),\n        legend.background = element_blank(),\n        legend.box.background = element_blank(),\n        legend.key = element_blank(),\n        legend.key.width = unit(0.5,units=\"cm\"),\n        legend.direction = \"horizontal\",\n        legend.position = c(0.8,0.1))\n\nIf you‚Äôd like to save it as .png you can do it with ggsave()\n\nggsave(\"day9_statistics_v2.png\",\n       dpi=320,\n       width = 9,\n       height = 6)"
  },
  {
    "objectID": "content/dataviz/data_visualization/Statistics/index.html#resources",
    "href": "content/dataviz/data_visualization/Statistics/index.html#resources",
    "title": "Statistics",
    "section": "Resources:",
    "text": "Resources:\n\nAn Introduction to Statistical Learning"
  },
  {
    "objectID": "content/proj/index.html",
    "href": "content/proj/index.html",
    "title": "Projects",
    "section": "",
    "text": "hmsidwR R-package\n\n\nHealth Metrics and the Spread of Infectious Diseases: Machine Learning Application and Spatial Modelling Analysis with R\n\n\n\nHealth Metrics\n\nData Science\n\nMachine Learning\n\n\n\n\n\n\n\n\n\nApr 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nMeasuring Health Performance\n\n\n\n\n\n\n\n\n\n\n\nMar 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nOregon spotted a frog: Rana Pretiosa\n\n\n\n\n\n\nMachine Learning\n\nmlr3\n\n\n\n\n\n\n\n\n\nOct 30, 2022\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "content/proj/projects/performance/index.html",
    "href": "content/proj/projects/performance/index.html",
    "title": "Measuring Health Performance",
    "section": "",
    "text": "These days measuring performance is very appropriate for many different topics. Thinking about health and the fast changing environments, including climate changes, require a ready tool for identifying possible future outcomes. On health, interesting simple metrics are used to classify the state of health of a population, so to be comparable with other near and far.\nHere is a spec of my latest project where I am collecting all that I learned since the start of the Covid19 pandemic on a summary of the techniques used for measuring the health status of a population when in conjunction with an extreme event. Many tools are available and ready to use for the most exceptional purpose someone might had in mind, and I had difficulties choosing one on top of the other. But, why choosing if you can combine them?\n\n\n\n\nThere are three metrics that are used for the purpose of classification in the public health, the DALYs, YLLs, and the YLDs. Respectively are the Disability Adjusted Life Years, Years of Life Lost, and Years Lived with Disabilities.\nBefore going into the calculation detail, the definition of good health and well being is required.\nThe WHO constitution states:\n\n‚ÄúHealth is a state of complete physical, mental and social well-being and not merely the absence of disease or infirmity.‚Äù\n\nAn important implication of this definition is that mental health is more than just the absence of mental disorders or disabilities.\nLet‚Äôs load the {hmsidwR} package, still at its very early stages; a development version can be installed from GitHub:\n\ndevtools::install_github(\"Fgazzelloni/hmsidwR\")\n\n\nlibrary(tidyverse)\nlibrary(hmsidwR)\n\nIt contains some interesting datasets:\nThe Global life tables: Glifetables. A dataset provided by the World Health Organization (WHO). Global Health Observatory data repository\n\n?hmsidwR\n\nAnd the Germany lung cancer: Germany_lungc. A dataset provided by the Institute for Health Metrics and Evaluation (IHME). GBD Results\n\nhmsidwR::germany_lungc %&gt;% head\n\n# A tibble: 6 √ó 8\n  age   sex    prevalence prev_upper prev_lower    dx dx_uppe dx_lower\n  &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 10-14 male         0.08       0.13       0.05 0.322   0.461    0.217\n2 10-14 female       0.18       0.32       0.09 0.457   0.761    0.248\n3 10-14 both         0.13       0.22       0.07 0.779   1.21     0.468\n4 15-19 male         0.48       0.77       0.29 1.27    1.75     0.916\n5 15-19 female       0.9        1.52       0.5  1.56    2.46     0.941\n6 15-19 both         0.68       1.02       0.44 2.83    3.88     2.07 \n\n\n\ngermany_lungc %&gt;%\n  ggplot(aes(age, dx, fill = sex)) +\n  geom_col() +\n  facet_wrap( ~ sex) +\n  scale_x_discrete(breaks = c(\"35-39\", \"65-69\", \"85+\")) +\n  ggthemes::scale_fill_fivethirtyeight() +\n  ggthemes::theme_fivethirtyeight() +\n  labs(title = \"Germany lung cancer -2019\",\n       caption = \"Vis: fgazzelloni|DataSource: IHME\")\n\n\n\n\n\n\n\nThe combination of this to piece of information, the life expectancy and the expected value of lung cancer cases, in proportion of the Germany population, are combined by age class and divided by sex to obtain the YLLs, the numbers of years of life lost.\nIn this case for Germany data is available from the age class 10-14, if we would like to improve this analysis it required some missing value imputation, through data feature engineering.\n\nyll &lt;- germany_lungc %&gt;%\n  full_join(\n    gho_lifetables %&gt;%\n      filter(year == 2019,\n             indicator == \"ex\") %&gt;%\n      rename(life_expectancy = value),\n    by = c(\"age\", \"sex\")\n  ) %&gt;%\n  group_by(age, sex) %&gt;%\n  reframe(yll = dx * life_expectancy) %&gt;%\n  filter(!is.na(yll))\n\nyll %&gt;%\n  head()\n\n# A tibble: 6 √ó 3\n  age   sex      yll\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 10-14 both    51.7\n2 10-14 female  31.5\n3 10-14 male    20.6\n4 15-19 both   174. \n5 15-19 female  99.7\n6 15-19 male    75.4\n\n\n\nyll %&gt;%\n  ggplot(aes(age, yll, fill = sex)) +\n  geom_col() +\n  facet_wrap( ~ sex) +\n  scale_x_discrete(breaks = c(\"35-39\", \"65-69\", \"85+\")) +\n  ggthemes::scale_fill_fivethirtyeight() +\n  ggthemes::theme_fivethirtyeight() +\n  labs(title = \"YLL - Germany lung cancer -2019\",\n       caption = \"Vis: fgazzelloni|DataSource: IHME & WHO\")\n\n\n\n\n\n\n\nTo build the YLDs, the numbers of years lived with a disability due to a disease or injury, we need more data: the prevalence, and the disability weights.\nsource:\n\ndisability weights\n\nThen, the sum of the YLL and the YLD provides the overall value of the DALY which is the key metric used to state the health of a population, and it is used to make comparisons among population of different countries, as well as begin used to provide a comprehensive assessment of the impact of disease and injury on a population, and help prioritize public health interventions and evaluate the effectiveness of public health programs.\nLet‚Äôs now have a look at how infectious diseases can affect the DALYs. The COVID-19 pandemic has had a significant impact on DALYs metrics worldwide.\nStill results are not fully available, but several of the risk factors and non-communicable diseases (NCDs) highlighted by the GBD study, including obesity, diabetes, and cardiovascular disease, are associated with increased risk of serious illness and death from COVID-19, and so, as a consequence linked with an increase of the overall level of DALYs. See The Lancet: Latest global disease estimates reveal perfect storm of rising chronic diseases and public health failures fuelling COVID-19 pandemic\nCOVID-19 is expected to show clearly that it has been the leading cause of global DALYs in 2020.\nMore information on the level of findings are in this interesting article: https://doi.org/10.1016/S0140-6736(20)30925-9 on the Lancet by the GBD collaborator team.\n\nLooking at the Global impact of some infectious diseases, such as: Ebola.\nThe impact of Ebola on DALYs in 2019 can be assessed by comparing the number of DALYs due to Ebola in 2019 to the DALYs caused by other diseases or conditions during the same period.\nAccording to the Global Health Data Exchange (GHDx), the estimated global DALY rate for Ebola virus disease in 2019 was 0.0005, which is relatively low compared to other leading causes of DALYs, such as cardiovascular diseases, lower respiratory infections, and neonatal disorders. See IHME Ebola ‚Äî Level 3 cause\nImport data on global burden of disease (GBD) for a given year, here I already downloaded the cvs file and save it as RData.\n\n# Subset data to only include \ndf_dalys_2019 &lt;- df_gbd_2019 %&gt;%\n  filter(location_name == \"Global\",\n         sex_name == \"Both\") %&gt;%\n  select(!contains(\"_id\"))\n\n\ndf_dalys_2019 %&gt;% count(age_name, val)\n\n# A tibble: 19 √ó 3\n   age_name        val     n\n   &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;\n 1 1-4 years    24933.     1\n 2 10-14 years   8994.     1\n 3 15-19 years  12520.     1\n 4 20-24 years  15651.     1\n 5 25-29 years  17288.     1\n 6 30-34 years  19457.     1\n 7 35-39 years  22626.     1\n 8 40-44 years  26218.     1\n 9 45-49 years  30105.     1\n10 5-9 years     9136.     1\n11 50-54 years  36694.     1\n12 55-59 years  45169.     1\n13 60-64 years  55901.     1\n14 65-69 years  67373.     1\n15 70-74 years  83145.     1\n16 75-79 years  99766.     1\n17 80-84       123203.     1\n18 85+ years   151604.     1\n19 &lt;1 year     256548.     1\n\n\nLet‚Äôs have a look at the DALYs rates and consider the average value by 5 years range, then calculate the total DALYs for all ages in 2019.\n\navg_dalys_2019 &lt;- mean(df_dalys_2019$val)\navg_dalys_2019\n\n[1] 58227.96\n\n\n\ntotal_dalys_2019 &lt;- sum(df_dalys_2019$val)\ntotal_dalys_2019\n\n[1] 1106331\n\n\nImport data on infectious diseases, and select Global, Ebola, both sex.\n\nebola_global_2019 &lt;- infectious_diseases %&gt;%\n  filter(location_name == \"Global\",\n         sex_name == \"Both\",\n         cause_name == \"Ebola\") %&gt;%\n  select(!contains(\"_id\"))\n\n# Calculate total COVID-19 DALYs for 2019\ntotal_ebola_global_2019 &lt;-\n  sum(ebola_global_2019$val)\n\n# Calculate the percentage change in DALYs due to COVID-19\npercent_change_dalys &lt;- round((total_ebola_global_2019 / total_dalys_2019) *100,4)\n\n# Print the percentage change in DALYs due to COVID-19\ncat(\"Total impact of Ebola virus Globally on DALYs rates in 2019:\", percent_change_dalys, \"%\")\n\nTotal impact of Ebola virus Globally on DALYs rates in 2019: 0.0037 %\n\n\nHowever, the impact of Ebola on DALYs is more significant in certain African regions, it accounts for the whole population. For example, during the 2014-2016, the Ebola outbreak in West Africa caused an estimated 11,000 deaths and 261,000 DALYs lost.\nOverall, while the global impact of Ebola on DALYs in 2019 was relatively low, it is still an important health concern in areas where outbreaks occur, and efforts to prevent and control the disease are crucial to reducing its impact on affected populations."
  },
  {
    "objectID": "content/proj/projects/performance/index.html#overview",
    "href": "content/proj/projects/performance/index.html#overview",
    "title": "Measuring Health Performance",
    "section": "",
    "text": "These days measuring performance is very appropriate for many different topics. Thinking about health and the fast changing environments, including climate changes, require a ready tool for identifying possible future outcomes. On health, interesting simple metrics are used to classify the state of health of a population, so to be comparable with other near and far.\nHere is a spec of my latest project where I am collecting all that I learned since the start of the Covid19 pandemic on a summary of the techniques used for measuring the health status of a population when in conjunction with an extreme event. Many tools are available and ready to use for the most exceptional purpose someone might had in mind, and I had difficulties choosing one on top of the other. But, why choosing if you can combine them?\n\n\n\n\nThere are three metrics that are used for the purpose of classification in the public health, the DALYs, YLLs, and the YLDs. Respectively are the Disability Adjusted Life Years, Years of Life Lost, and Years Lived with Disabilities.\nBefore going into the calculation detail, the definition of good health and well being is required.\nThe WHO constitution states:\n\n‚ÄúHealth is a state of complete physical, mental and social well-being and not merely the absence of disease or infirmity.‚Äù\n\nAn important implication of this definition is that mental health is more than just the absence of mental disorders or disabilities.\nLet‚Äôs load the {hmsidwR} package, still at its very early stages; a development version can be installed from GitHub:\n\ndevtools::install_github(\"Fgazzelloni/hmsidwR\")\n\n\nlibrary(tidyverse)\nlibrary(hmsidwR)\n\nIt contains some interesting datasets:\nThe Global life tables: Glifetables. A dataset provided by the World Health Organization (WHO). Global Health Observatory data repository\n\n?hmsidwR\n\nAnd the Germany lung cancer: Germany_lungc. A dataset provided by the Institute for Health Metrics and Evaluation (IHME). GBD Results\n\nhmsidwR::germany_lungc %&gt;% head\n\n# A tibble: 6 √ó 8\n  age   sex    prevalence prev_upper prev_lower    dx dx_uppe dx_lower\n  &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 10-14 male         0.08       0.13       0.05 0.322   0.461    0.217\n2 10-14 female       0.18       0.32       0.09 0.457   0.761    0.248\n3 10-14 both         0.13       0.22       0.07 0.779   1.21     0.468\n4 15-19 male         0.48       0.77       0.29 1.27    1.75     0.916\n5 15-19 female       0.9        1.52       0.5  1.56    2.46     0.941\n6 15-19 both         0.68       1.02       0.44 2.83    3.88     2.07 \n\n\n\ngermany_lungc %&gt;%\n  ggplot(aes(age, dx, fill = sex)) +\n  geom_col() +\n  facet_wrap( ~ sex) +\n  scale_x_discrete(breaks = c(\"35-39\", \"65-69\", \"85+\")) +\n  ggthemes::scale_fill_fivethirtyeight() +\n  ggthemes::theme_fivethirtyeight() +\n  labs(title = \"Germany lung cancer -2019\",\n       caption = \"Vis: fgazzelloni|DataSource: IHME\")\n\n\n\n\n\n\n\nThe combination of this to piece of information, the life expectancy and the expected value of lung cancer cases, in proportion of the Germany population, are combined by age class and divided by sex to obtain the YLLs, the numbers of years of life lost.\nIn this case for Germany data is available from the age class 10-14, if we would like to improve this analysis it required some missing value imputation, through data feature engineering.\n\nyll &lt;- germany_lungc %&gt;%\n  full_join(\n    gho_lifetables %&gt;%\n      filter(year == 2019,\n             indicator == \"ex\") %&gt;%\n      rename(life_expectancy = value),\n    by = c(\"age\", \"sex\")\n  ) %&gt;%\n  group_by(age, sex) %&gt;%\n  reframe(yll = dx * life_expectancy) %&gt;%\n  filter(!is.na(yll))\n\nyll %&gt;%\n  head()\n\n# A tibble: 6 √ó 3\n  age   sex      yll\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 10-14 both    51.7\n2 10-14 female  31.5\n3 10-14 male    20.6\n4 15-19 both   174. \n5 15-19 female  99.7\n6 15-19 male    75.4\n\n\n\nyll %&gt;%\n  ggplot(aes(age, yll, fill = sex)) +\n  geom_col() +\n  facet_wrap( ~ sex) +\n  scale_x_discrete(breaks = c(\"35-39\", \"65-69\", \"85+\")) +\n  ggthemes::scale_fill_fivethirtyeight() +\n  ggthemes::theme_fivethirtyeight() +\n  labs(title = \"YLL - Germany lung cancer -2019\",\n       caption = \"Vis: fgazzelloni|DataSource: IHME & WHO\")\n\n\n\n\n\n\n\nTo build the YLDs, the numbers of years lived with a disability due to a disease or injury, we need more data: the prevalence, and the disability weights.\nsource:\n\ndisability weights\n\nThen, the sum of the YLL and the YLD provides the overall value of the DALY which is the key metric used to state the health of a population, and it is used to make comparisons among population of different countries, as well as begin used to provide a comprehensive assessment of the impact of disease and injury on a population, and help prioritize public health interventions and evaluate the effectiveness of public health programs.\nLet‚Äôs now have a look at how infectious diseases can affect the DALYs. The COVID-19 pandemic has had a significant impact on DALYs metrics worldwide.\nStill results are not fully available, but several of the risk factors and non-communicable diseases (NCDs) highlighted by the GBD study, including obesity, diabetes, and cardiovascular disease, are associated with increased risk of serious illness and death from COVID-19, and so, as a consequence linked with an increase of the overall level of DALYs. See The Lancet: Latest global disease estimates reveal perfect storm of rising chronic diseases and public health failures fuelling COVID-19 pandemic\nCOVID-19 is expected to show clearly that it has been the leading cause of global DALYs in 2020.\nMore information on the level of findings are in this interesting article: https://doi.org/10.1016/S0140-6736(20)30925-9 on the Lancet by the GBD collaborator team.\n\nLooking at the Global impact of some infectious diseases, such as: Ebola.\nThe impact of Ebola on DALYs in 2019 can be assessed by comparing the number of DALYs due to Ebola in 2019 to the DALYs caused by other diseases or conditions during the same period.\nAccording to the Global Health Data Exchange (GHDx), the estimated global DALY rate for Ebola virus disease in 2019 was 0.0005, which is relatively low compared to other leading causes of DALYs, such as cardiovascular diseases, lower respiratory infections, and neonatal disorders. See IHME Ebola ‚Äî Level 3 cause\nImport data on global burden of disease (GBD) for a given year, here I already downloaded the cvs file and save it as RData.\n\n# Subset data to only include \ndf_dalys_2019 &lt;- df_gbd_2019 %&gt;%\n  filter(location_name == \"Global\",\n         sex_name == \"Both\") %&gt;%\n  select(!contains(\"_id\"))\n\n\ndf_dalys_2019 %&gt;% count(age_name, val)\n\n# A tibble: 19 √ó 3\n   age_name        val     n\n   &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;\n 1 1-4 years    24933.     1\n 2 10-14 years   8994.     1\n 3 15-19 years  12520.     1\n 4 20-24 years  15651.     1\n 5 25-29 years  17288.     1\n 6 30-34 years  19457.     1\n 7 35-39 years  22626.     1\n 8 40-44 years  26218.     1\n 9 45-49 years  30105.     1\n10 5-9 years     9136.     1\n11 50-54 years  36694.     1\n12 55-59 years  45169.     1\n13 60-64 years  55901.     1\n14 65-69 years  67373.     1\n15 70-74 years  83145.     1\n16 75-79 years  99766.     1\n17 80-84       123203.     1\n18 85+ years   151604.     1\n19 &lt;1 year     256548.     1\n\n\nLet‚Äôs have a look at the DALYs rates and consider the average value by 5 years range, then calculate the total DALYs for all ages in 2019.\n\navg_dalys_2019 &lt;- mean(df_dalys_2019$val)\navg_dalys_2019\n\n[1] 58227.96\n\n\n\ntotal_dalys_2019 &lt;- sum(df_dalys_2019$val)\ntotal_dalys_2019\n\n[1] 1106331\n\n\nImport data on infectious diseases, and select Global, Ebola, both sex.\n\nebola_global_2019 &lt;- infectious_diseases %&gt;%\n  filter(location_name == \"Global\",\n         sex_name == \"Both\",\n         cause_name == \"Ebola\") %&gt;%\n  select(!contains(\"_id\"))\n\n# Calculate total COVID-19 DALYs for 2019\ntotal_ebola_global_2019 &lt;-\n  sum(ebola_global_2019$val)\n\n# Calculate the percentage change in DALYs due to COVID-19\npercent_change_dalys &lt;- round((total_ebola_global_2019 / total_dalys_2019) *100,4)\n\n# Print the percentage change in DALYs due to COVID-19\ncat(\"Total impact of Ebola virus Globally on DALYs rates in 2019:\", percent_change_dalys, \"%\")\n\nTotal impact of Ebola virus Globally on DALYs rates in 2019: 0.0037 %\n\n\nHowever, the impact of Ebola on DALYs is more significant in certain African regions, it accounts for the whole population. For example, during the 2014-2016, the Ebola outbreak in West Africa caused an estimated 11,000 deaths and 261,000 DALYs lost.\nOverall, while the global impact of Ebola on DALYs in 2019 was relatively low, it is still an important health concern in areas where outbreaks occur, and efforts to prevent and control the disease are crucial to reducing its impact on affected populations."
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "MIT License",
    "section": "",
    "text": "MIT License\nCopyright (c) 2025 Federica Gazzelloni author\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ‚ÄúSoftware‚Äù), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED ‚ÄúAS IS‚Äù, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "content/proj/projects/hmsidwR/index.html",
    "href": "content/proj/projects/hmsidwR/index.html",
    "title": "hmsidwR R-package",
    "section": "",
    "text": "This is the package that covers all datasets used in the Health Metrics and the Spread of Infectious Diseases: Machine Learning Application and Spatial Modelling Analysis with R book.\nBOOK: https://bookdown.org/fede_gazzelloni/hmsidR/\nGITHUB: https://github.com/Fgazzelloni/hmsidwR\nVIGNETTE: https://fgazzelloni.github.io/hmsidwR/\n\n\n\n\n\n‚ÄúIt‚Äôs been an incredible journey, and while the book is nearing its completion, I feel a mix of emotions‚Äîboth the excitement of sharing it with you and a touch of sadness that this creative process is coming to an end. üòå\nWriting this book has been an immensely rewarding experience. I‚Äôve spent countless hours into selecting the right material, striving to present it in a way that‚Äôs accessible and helpful for early-career researchers and students. One of the biggest challenges was deciding what to include and what to leave out, constantly refining the content to ensure it‚Äôs both comprehensive and easy to follow.\nEven though the book is almost finished, my mind is still buzzing with ideas. There‚Äôs always that lingering thought of what else could be added to make it even better. But at some point, you have to step back and trust that what you‚Äôve created will serve its purpose. And now, it‚Äôs time to see how it will resonate with you, the readers.\nAs I reflect on this journey, I‚Äôm filled with gratitude for all the support and feedback I‚Äôve received along the way. Your encouragement has been invaluable in shaping this book into what it is today.\nSo, what‚Äôs next? While this project may be wrapping up, my passion for writing and sharing knowledge is far from over. Stay tuned for more updates and future projects! In the meantime, I invite you to dive into the book, explore its content, and share your thoughts. Your insights will be crucial as I make final touches and consider potential additions.\nThank you for being a part of this journey. I hope the book serves as a valuable resource for your research and studies. Let‚Äôs see where this new chapter takes us!\nHappy learning! ü§ìüìö‚ú®‚Äù"
  },
  {
    "objectID": "content/proj/projects/hmsidwR/index.html#overview",
    "href": "content/proj/projects/hmsidwR/index.html#overview",
    "title": "hmsidwR R-package",
    "section": "",
    "text": "This is the package that covers all datasets used in the Health Metrics and the Spread of Infectious Diseases: Machine Learning Application and Spatial Modelling Analysis with R book.\nBOOK: https://bookdown.org/fede_gazzelloni/hmsidR/\nGITHUB: https://github.com/Fgazzelloni/hmsidwR\nVIGNETTE: https://fgazzelloni.github.io/hmsidwR/\n\n\n\n\n\n‚ÄúIt‚Äôs been an incredible journey, and while the book is nearing its completion, I feel a mix of emotions‚Äîboth the excitement of sharing it with you and a touch of sadness that this creative process is coming to an end. üòå\nWriting this book has been an immensely rewarding experience. I‚Äôve spent countless hours into selecting the right material, striving to present it in a way that‚Äôs accessible and helpful for early-career researchers and students. One of the biggest challenges was deciding what to include and what to leave out, constantly refining the content to ensure it‚Äôs both comprehensive and easy to follow.\nEven though the book is almost finished, my mind is still buzzing with ideas. There‚Äôs always that lingering thought of what else could be added to make it even better. But at some point, you have to step back and trust that what you‚Äôve created will serve its purpose. And now, it‚Äôs time to see how it will resonate with you, the readers.\nAs I reflect on this journey, I‚Äôm filled with gratitude for all the support and feedback I‚Äôve received along the way. Your encouragement has been invaluable in shaping this book into what it is today.\nSo, what‚Äôs next? While this project may be wrapping up, my passion for writing and sharing knowledge is far from over. Stay tuned for more updates and future projects! In the meantime, I invite you to dive into the book, explore its content, and share your thoughts. Your insights will be crucial as I make final touches and consider potential additions.\nThank you for being a part of this journey. I hope the book serves as a valuable resource for your research and studies. Let‚Äôs see where this new chapter takes us!\nHappy learning! ü§ìüìö‚ú®‚Äù"
  },
  {
    "objectID": "content/proj/projects/oregonfrogs/index.html#machine-learning",
    "href": "content/proj/projects/oregonfrogs/index.html#machine-learning",
    "title": "Oregon spotted a frog: Rana Pretiosa",
    "section": "Machine Learning",
    "text": "Machine Learning\nTuning: Tweaking the hyperparameters of the learner\nIn random forests, the hyperparameters mtry, min.node.size and sample.fraction determine the degree of randomness, and should be tuned. This is when machine learning comes into play.\nHyperparameters:\n\n\nmtry indicates how many predictor variables should be used in each tree\n\nsample.fraction parameter specifies the fraction of observations to be used in each tree\n\nmin.node.size parameter indicates the number of observations a terminal node should at least have\n\nsource: https://geocompr.robinlovelace.net/eco.html\n\n#### tuning\ntune_level = mlr3::rsmp(\"spcv_coords\", folds = 5)\n\nterminator = mlr3tuning::trm(\"evals\", n_evals = 50)\n\ntuner = mlr3tuning::tnr(\"random_search\")\n\nSearch space\nTo specify tuning limits paradox::ps() is used:\n\nsearch_space =\n  paradox::ps(\n    mtry = paradox::p_int(lower = 1,\n                          upper = ncol(task$data()) - 1),\n    sample.fraction = paradox::p_dbl(lower = 0.2,\n                                     upper = 0.9),\n    min.node.size = paradox::p_int(lower = 1,\n                                   upper = 10)\n  )\nsearch_space\n\nAutomation\nAutomated tuning specification via the mlr3tuning::AutoTuner() function:\n\nautotuner_rf =\n  mlr3tuning::AutoTuner$new(\n    learner = learner,\n    store_benchmark_result = TRUE,\n    resampling = mlr3::rsmp(\"spcv_coords\",\n                            folds = 5),\n    # spatial partitioning\n    measure = mlr3::msr(\"classif.acc\"),\n    # performance measure\n    terminator = mlr3tuning::trm(\"evals\",\n                                 n_evals = 50),\n    # specify 50 iterations\n    search_space = search_space,\n    # predefined hyperparameter search space\n    tuner = mlr3tuning::tnr(\"random_search\") # specify random search\n  )\n\n\n# hyperparameter tuning\ntime = Sys.time()\n\nset.seed(0412022)\nautotuner_rf$train(task)\n\nSys.time() - time\n\n\nautotuner_rf$tuning_result\n\n\nautotuner_rf$predict(task)\n# pred = terra::predict(..., model = autotuner_rf, fun = predict)\n\n# save.image(\"data/oregonfrogs_mlr3.RData\")\n\n\nres &lt;- autotuner_rf$predict(task)\nautoplot(res)\n\n\nres %&gt;%\n  fortify() %&gt;%\n  pivot_longer(cols = starts_with(\"prob\"),\n               names_to = \"prob_type\",\n               values_to = \"prob\") %&gt;%\n  mutate(prob_type = gsub(\"prob.\", \"\", prob_type)) %&gt;%\n  ggplot(aes(prob, group = prob_type, fill = prob_type)) +\n  geom_density(alpha = 0.5) +\n  facet_wrap(vars(prob_type), scales = \"free\") +\n  labs(fill = \"Water Type\", x = \"Probability\", y = \"Density\") +\n  scale_fill_viridis_d() +\n  theme_bw()"
  },
  {
    "objectID": "content/dataviz/index.html",
    "href": "content/dataviz/index.html",
    "title": "Unlocking The Power of Data Visualization with R",
    "section": "",
    "text": "Data visualization is a powerful tool that allows the transformation of complex data sets into comprehensible and insightful graphical representations. By presenting data visually, it enables the identification of patterns, trends, and correlations that might otherwise go unnoticed, making it easier to understand and communicate complex findings.\nOne of the fundamental concepts in modern data visualization is the Grammar of Graphics, which provides a structured approach to creating visualizations layer by layer. This concept is clearly interpreted in tools like the {ggplot2} R package, part of the {tidyverse} ecosystem, developed by Hadley Wickham in 2005. The Grammar of Graphics helps guide the creation of clear and concise visualizations, offering users a framework for combining data, aesthetics, and geometries.\nHere you‚Äôll find a collection of my favorite Data Visualizations, showcasing a variety of themes and data sets. Each visualization represents my approach to conveying meaningful insights through graphs and plots. Whether you are interested in exploring health metrics, economic trends, or the spread of infectious diseases, my work emphasizes the power of visual storytelling.\nFor a more comprehensive experience, feel free to explore my full collection of plots and data visualizations at my UPDVwR website: Explore all Data Visualizations"
  },
  {
    "objectID": "content/dataviz/index.html#overview",
    "href": "content/dataviz/index.html#overview",
    "title": "Unlocking The Power of Data Visualization with R",
    "section": "",
    "text": "Data visualization is a powerful tool that allows the transformation of complex data sets into comprehensible and insightful graphical representations. By presenting data visually, it enables the identification of patterns, trends, and correlations that might otherwise go unnoticed, making it easier to understand and communicate complex findings.\nOne of the fundamental concepts in modern data visualization is the Grammar of Graphics, which provides a structured approach to creating visualizations layer by layer. This concept is clearly interpreted in tools like the {ggplot2} R package, part of the {tidyverse} ecosystem, developed by Hadley Wickham in 2005. The Grammar of Graphics helps guide the creation of clear and concise visualizations, offering users a framework for combining data, aesthetics, and geometries.\nHere you‚Äôll find a collection of my favorite Data Visualizations, showcasing a variety of themes and data sets. Each visualization represents my approach to conveying meaningful insights through graphs and plots. Whether you are interested in exploring health metrics, economic trends, or the spread of infectious diseases, my work emphasizes the power of visual storytelling.\nFor a more comprehensive experience, feel free to explore my full collection of plots and data visualizations at my UPDVwR website: Explore all Data Visualizations"
  },
  {
    "objectID": "content/dataviz/data_visualization/ggstream/index.html",
    "href": "content/dataviz/data_visualization/ggstream/index.html",
    "title": "Retail Sales with ggstream",
    "section": "",
    "text": "This post is all about Retail Sales with ggstream, the dataset comes from #TidyTuesday 2022 week 50 Monthly State Retail Sales.\nThe picture below is the result of the ggstream visualization.\n\n\n\nLoad libraries\n\nlibrary(tidyverse)\nlibrary(fuzzyjoin)\nlibrary(ggstream)\nlibrary(colorspace)\n\nSet the theme\n\ntheme_set(theme_minimal(base_family = \"Roboto Condensed\",\n                        base_size = 12))\n\n\ntheme_update(\n  plot.title = element_text(\n    size = 20,\n    face = \"bold\",\n    hjust = .5,\n    margin = margin(10, 0, 30, 0)\n  ),\n  plot.caption = element_text(\n    size = 9,\n    color = \"grey40\",\n    hjust = .5,\n    margin = margin(20, 0, 5, 0)\n  ),\n  axis.text.y = element_blank(),\n  axis.title = element_blank(),\n  plot.background = element_rect(fill = \"grey88\", color = NA),\n  panel.background = element_rect(fill = NA, color = NA),\n  panel.grid = element_blank(),\n  panel.spacing.y = unit(0, \"lines\"),\n  strip.text.y = element_text(angle = 0),\n  legend.position = \"bottom\",\n  legend.text = element_text(size = 9, color = \"grey40\"),\n  legend.box.margin = margin(t = 30),\n  legend.background = element_rect(\n    color = \"grey40\",\n    linewidth = .3,\n    fill = \"grey95\"\n  ),\n  legend.key.height = unit(.25, \"lines\"),\n  legend.key.width = unit(2.5, \"lines\"),\n  plot.margin = margin(rep(20, 4))\n)\n\nAnd the color palette\n\npal &lt;- c(\"#FFB400\",\n         \"#C20008\",\n         \"#13AFEF\",\n         \"#8E038E\")\n\nLoad the data\n\ntuesdata &lt;- tidytuesdayR::tt_load(2022, week = 50)\ncoverage_codes &lt;- tuesdata$coverage_codes\nstate_retail &lt;- tuesdata$state_retail\n\nAdd the states‚Äô names\n\nfipcodes &lt;- tigris::fips_codes %&gt;%\n  select(state, state_name)\n\nJoin all sets\n\nmy_df &lt;- state_retail %&gt;%\n  left_join(fipcodes, by = c(\"state_abbr\" = \"state\")) %&gt;%\n  mutate(state_name = ifelse(state_abbr == \"USA\", \"USA\", state_name)) %&gt;%\n  distinct() %&gt;%\n  merge(coverage_codes, by = \"coverage_code\") %&gt;%\n  arrange()\n\nmy_df %&gt;% head\n\nData wrangling\n\nmy_df1 &lt;- my_df %&gt;%\n  select(-naics) %&gt;%\n  mutate(\n    coverage = case_when(\n      coverage == \"non-imputed coverage is greater than or equal to 10% and less than 25% of the state/NAICS total\" ~\n        \"greater than or equal 10% and less than 25% of the state/NAICS total\",\n      coverage == \"non-imputed coverage is greater than or equal to 25% and less than 50% of the state/NAICS total\" ~\n        \"greater than or equal to 25% and less than 50% of the state/NAICS total\",\n      coverage == \"non-imputed coverage is greater than or equal to 50% of the state/NAICS total.\" ~\n        \"greater than or equal to 50% of the state/NAICS total\",\n      coverage == \"non-imputed coverage is less than 10% of the state/NAICS total.\" ~\n        \"less than 10% of the state/NAICS total\",\n      TRUE ~ coverage\n    ),\n    month = as.character(month),\n    year = zoo::as.yearmon(paste0(year, \"-\", month)),\n    change_yoy = ifelse(change_yoy == \"S\", 0, change_yoy),\n    change_yoy_se = ifelse(change_yoy_se == \"S\", 0, change_yoy_se),\n    change_yoy = as.numeric(change_yoy),\n    change_yoy_se = as.numeric(change_yoy_se),\n    coverage = as.factor(coverage),\n    coverage = paste(coverage_code, \"-\", coverage)\n  ) %&gt;%\n  filter(state_abbr %in% c(\"USA\", \"PA\", \"MD\", \"MT\")) %&gt;%\n  filter(!coverage_code == \"S\") %&gt;%\n  group_by(state_name, coverage, year) %&gt;%\n  summarise_if(is.numeric, sum, na.rm = TRUE) %&gt;%\n  mutate(change_yoy = scale(change_yoy, center = FALSE)) %&gt;%\n  ungroup() %&gt;%\n  mutate(year = as.POSIXct(year),\n         year = as.Date(year))\n\nMake the plot\n\nmy_df1 %&gt;%\n  ggplot(aes(\n    x = year,\n    y = change_yoy,\n    color = coverage,\n    fill = coverage\n  )) +\n  geom_stream(\n    geom = \"contour\",\n    color = \"white\",\n    linewidth = 1.25,\n    bw = .45 # Controls smoothness\n  ) +\n  geom_stream(geom = \"polygon\",\n              bw = .45,\n              linewidth = 0.2) +\n  facet_grid(state_name ~ .,\n             scales = \"free_y\",\n             space = \"free\") +\n  scale_y_continuous(trans = scales::modulus_trans(0.1, 1)) +\n  scale_x_date(date_breaks = \"6 months\",\n               date_labels = \"%b-%Y\",\n               expand = c(0, 0)) +\n  scale_color_manual(expand = c(0, 0),\n                     values = pal,\n                     guide = \"none\") +\n  scale_fill_manual(values = pal,\n                    name = NULL) +\n  labs(title = \"Total Year-Over-Year percent change\\nin monthly retail sales value\",\n       subtitle = \"North American Industry Classification System (NAICS) top YoY states\",\n       caption = \"DataSource: #TidyTuesday 2022 Week50 | Monthly State Retail Sales | DataViz: Fgazzelloni\") +\n  theme(legend.direction = \"vertical\")\n\n\nggsave(\"w50_retail_sales.png\")\n\n\n\nggstream\nGraph Gallery"
  },
  {
    "objectID": "content/dataviz/data_visualization/ggstream/index.html#overview",
    "href": "content/dataviz/data_visualization/ggstream/index.html#overview",
    "title": "Retail Sales with ggstream",
    "section": "",
    "text": "This post is all about Retail Sales with ggstream, the dataset comes from #TidyTuesday 2022 week 50 Monthly State Retail Sales.\nThe picture below is the result of the ggstream visualization.\n\n\n\nLoad libraries\n\nlibrary(tidyverse)\nlibrary(fuzzyjoin)\nlibrary(ggstream)\nlibrary(colorspace)\n\nSet the theme\n\ntheme_set(theme_minimal(base_family = \"Roboto Condensed\",\n                        base_size = 12))\n\n\ntheme_update(\n  plot.title = element_text(\n    size = 20,\n    face = \"bold\",\n    hjust = .5,\n    margin = margin(10, 0, 30, 0)\n  ),\n  plot.caption = element_text(\n    size = 9,\n    color = \"grey40\",\n    hjust = .5,\n    margin = margin(20, 0, 5, 0)\n  ),\n  axis.text.y = element_blank(),\n  axis.title = element_blank(),\n  plot.background = element_rect(fill = \"grey88\", color = NA),\n  panel.background = element_rect(fill = NA, color = NA),\n  panel.grid = element_blank(),\n  panel.spacing.y = unit(0, \"lines\"),\n  strip.text.y = element_text(angle = 0),\n  legend.position = \"bottom\",\n  legend.text = element_text(size = 9, color = \"grey40\"),\n  legend.box.margin = margin(t = 30),\n  legend.background = element_rect(\n    color = \"grey40\",\n    linewidth = .3,\n    fill = \"grey95\"\n  ),\n  legend.key.height = unit(.25, \"lines\"),\n  legend.key.width = unit(2.5, \"lines\"),\n  plot.margin = margin(rep(20, 4))\n)\n\nAnd the color palette\n\npal &lt;- c(\"#FFB400\",\n         \"#C20008\",\n         \"#13AFEF\",\n         \"#8E038E\")\n\nLoad the data\n\ntuesdata &lt;- tidytuesdayR::tt_load(2022, week = 50)\ncoverage_codes &lt;- tuesdata$coverage_codes\nstate_retail &lt;- tuesdata$state_retail\n\nAdd the states‚Äô names\n\nfipcodes &lt;- tigris::fips_codes %&gt;%\n  select(state, state_name)\n\nJoin all sets\n\nmy_df &lt;- state_retail %&gt;%\n  left_join(fipcodes, by = c(\"state_abbr\" = \"state\")) %&gt;%\n  mutate(state_name = ifelse(state_abbr == \"USA\", \"USA\", state_name)) %&gt;%\n  distinct() %&gt;%\n  merge(coverage_codes, by = \"coverage_code\") %&gt;%\n  arrange()\n\nmy_df %&gt;% head\n\nData wrangling\n\nmy_df1 &lt;- my_df %&gt;%\n  select(-naics) %&gt;%\n  mutate(\n    coverage = case_when(\n      coverage == \"non-imputed coverage is greater than or equal to 10% and less than 25% of the state/NAICS total\" ~\n        \"greater than or equal 10% and less than 25% of the state/NAICS total\",\n      coverage == \"non-imputed coverage is greater than or equal to 25% and less than 50% of the state/NAICS total\" ~\n        \"greater than or equal to 25% and less than 50% of the state/NAICS total\",\n      coverage == \"non-imputed coverage is greater than or equal to 50% of the state/NAICS total.\" ~\n        \"greater than or equal to 50% of the state/NAICS total\",\n      coverage == \"non-imputed coverage is less than 10% of the state/NAICS total.\" ~\n        \"less than 10% of the state/NAICS total\",\n      TRUE ~ coverage\n    ),\n    month = as.character(month),\n    year = zoo::as.yearmon(paste0(year, \"-\", month)),\n    change_yoy = ifelse(change_yoy == \"S\", 0, change_yoy),\n    change_yoy_se = ifelse(change_yoy_se == \"S\", 0, change_yoy_se),\n    change_yoy = as.numeric(change_yoy),\n    change_yoy_se = as.numeric(change_yoy_se),\n    coverage = as.factor(coverage),\n    coverage = paste(coverage_code, \"-\", coverage)\n  ) %&gt;%\n  filter(state_abbr %in% c(\"USA\", \"PA\", \"MD\", \"MT\")) %&gt;%\n  filter(!coverage_code == \"S\") %&gt;%\n  group_by(state_name, coverage, year) %&gt;%\n  summarise_if(is.numeric, sum, na.rm = TRUE) %&gt;%\n  mutate(change_yoy = scale(change_yoy, center = FALSE)) %&gt;%\n  ungroup() %&gt;%\n  mutate(year = as.POSIXct(year),\n         year = as.Date(year))\n\nMake the plot\n\nmy_df1 %&gt;%\n  ggplot(aes(\n    x = year,\n    y = change_yoy,\n    color = coverage,\n    fill = coverage\n  )) +\n  geom_stream(\n    geom = \"contour\",\n    color = \"white\",\n    linewidth = 1.25,\n    bw = .45 # Controls smoothness\n  ) +\n  geom_stream(geom = \"polygon\",\n              bw = .45,\n              linewidth = 0.2) +\n  facet_grid(state_name ~ .,\n             scales = \"free_y\",\n             space = \"free\") +\n  scale_y_continuous(trans = scales::modulus_trans(0.1, 1)) +\n  scale_x_date(date_breaks = \"6 months\",\n               date_labels = \"%b-%Y\",\n               expand = c(0, 0)) +\n  scale_color_manual(expand = c(0, 0),\n                     values = pal,\n                     guide = \"none\") +\n  scale_fill_manual(values = pal,\n                    name = NULL) +\n  labs(title = \"Total Year-Over-Year percent change\\nin monthly retail sales value\",\n       subtitle = \"North American Industry Classification System (NAICS) top YoY states\",\n       caption = \"DataSource: #TidyTuesday 2022 Week50 | Monthly State Retail Sales | DataViz: Fgazzelloni\") +\n  theme(legend.direction = \"vertical\")\n\n\nggsave(\"w50_retail_sales.png\")\n\n\n\nggstream\nGraph Gallery"
  },
  {
    "objectID": "content/dataviz/data_visualization/dubois2023/index.html",
    "href": "content/dataviz/data_visualization/dubois2023/index.html",
    "title": "DuBois Challenge 2022",
    "section": "",
    "text": "Overview\nThis week is all about #DuBoisChallenge2022, I choose plate number 6.\n\nlibrary(tidyverse)\n\n\nLoad #TidyTuesday 2022/07 data\n\nplate6 &lt;- read_csv('https://raw.githubusercontent.com/ajstarks/dubois-data-portraits/master/challenge/2022/challenge06/data.csv')\n\nAdd a column with distances\n\ndf &lt;- plate6 %&gt;%\n  janitor::clean_names() %&gt;%\n  mutate(x_axis=c(1860,1860-cumsum(diff(iliteracy_rate))))\ndf\n\nCreate axis lables vectors\n\niliteracy_rate&lt;-df$iliteracy_rate\nx_axis &lt;-df$x_axis\n\nFonts\n\nlibrary(showtext)\nfont_add(family = \"Public Sans Thin\",\n         regular = \"PublicSans-Thin.ttf\")\n\nfont_add(family = \"PublicSans-Medium\",\n         regular = \"PublicSans-Medium.ttf\")\n \nshowtext_auto()\n\nMake the plot\n\nlibrary(ggstar) # for making the triangles\n  \nnumber6 &lt;- df %&gt;%\n  ggplot(aes(x = x_axis, y = iliteracy_rate))+\n  \n  # add the columns\n  geom_col(width = 1.5,fill=\"black\") +\n\n  # this is one way to add the horizontal lines\n  geom_segment(aes(x = 1858, xend = x_axis+0.1,\n                   y = iliteracy_rate, yend = iliteracy_rate),\n               size = 2.5, color = \"black\") +\n  geom_segment(aes(x = 1858, xend = x_axis+0.1,\n                   y = iliteracy_rate, yend = iliteracy_rate),\n               size = 2.4, color = \"#d9ccbf\")+\n  \n  # with ggstar add the triangular shape\n  geom_star(starshape = 20, size = 0.8,angle = 90, fill = \"black\",\n            position = position_nudge(x = -0.1, y = -0.15)) +\n  \n  # add the little round corners\n  geom_curve(aes(x = x_axis - 0.1, xend = x_axis + 0.65,\n                 y = iliteracy_rate + 1.2, yend = iliteracy_rate),\n             curvature = -0.6, size = 0.1) +\n  \n  # customize the axis values\n  scale_x_continuous(breaks = x_axis,\n                     labels = c(\"99%\",\"92%\",\"81.6%\",\"67.27%\",\"(50%?)\"),\n                     expand = expansion(0.01)) +\n  scale_y_continuous(breaks = iliteracy_rate,\n                     labels = c(1860,1870,1880,1890,\"(1900?)\"),\n                     expand = expansion(0.01)) +\n  \n  # add a title and a theme\n  labs(title = \"ILLITERACY.\\n\", subtitle = \" \", caption = \"fg\")+\n  theme_void()+\n  theme(text = element_text(size = 18, family = \"Public Sans Thin\", color = \"grey25\"),\n        plot.title = element_text(size = 24, family = \"PublicSans-Medium\",face = \"bold\", hjust = 0.5),\n        plot.background = element_rect(fill = \"#d2c2b3\", color = \"#d2c2b3\"),\n        panel.background =  element_rect(fill = \"#d2c2b3\",color = \"#d2c2b3\"),\n        axis.text.x = element_text(),\n        axis.text.y = element_text(),\n        plot.margin = margin(0,25,0,25))\n\nAssemble background, plot and annotation\nWith {cowplot} add a background image as the same as the original one and the text on the left side of the x-axis\nSave the plot\n\nggsave(\"w7_Number6.png\", width = 1000, height = 1350, \n       units = \"px\", dpi = 320)\n\n\n\n Back to top"
  },
  {
    "objectID": "content/dataviz/data_visualization/erasmus/index.html",
    "href": "content/dataviz/data_visualization/erasmus/index.html",
    "title": "Erasmus students exchange",
    "section": "",
    "text": "This week 10 of #TidyTuesday 2022 theme is #Erasmus students exchange in the European countries.\nThe data set is from Erasmus student mobility, Data.Europa.eu and Wimdu.co to discover the most popular Erasmus destinations.\nThe idea is to make a network of sending and receiving countries, let‚Äôs have a look at the data.\n\nif(!require(pacman)) install.packages(\"pacman\")\npacman::p_load(tidyverse, ggbump, cowplot, wesanderson)\n\n\nerasmus &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-03-08/erasmus.csv')\n\nThe set is made of information about students, such as the age, the nationality, the lenght of stay, the gender, academic year, and others. I selected some of them, to extract the information I needed to make the network.\n\ndf &lt;- erasmus%&gt;%\n  select(sending_country_code,receiving_country_code,\n         participant_nationality,participants,\n         participant_age,\n         academic_year,mobility_duration,\n         participant_gender)\n\nkableExtra::kable(head(df)) \n\nLooking at the participant_age we see that we have some misleading data:\n\ndf %&gt;% pull(participant_age) %&gt;% summary()\n\nFor this reason the best way is to filter students between 17 and 28 years old. Also, mobility_duration is quite surprising:\n\ndf %&gt;% pull(mobility_duration) %&gt;% summary()\n\nThe median value of the students‚Äô stay is ONE day, while the mean is just a little above TWO days. Very few students stay more than 10 days, but someone reaches a max of 273 days (39 weeks).\n\ndf %&gt;% count(mobility_duration) %&gt;% mutate(perc = round(n/sum(n)*100,2)) %&gt;% head()\ndf %&gt;% count(mobility_duration) %&gt;% mutate(perc = round(n/sum(n)*100,3)) %&gt;% tail()\n\n\nStudent participants are almost all solo participants as the median shows to be ONE student per observation, TWO students on average, with a max value of 279. So that, to have a picture of the phenomenon select the average value of the student participants as representative.\n\ndf %&gt;% pull(participants) %&gt;% summary(participants)\n\nFinally, gender, Females are slightly more than males, just a little above 50%.\n\ntbl &lt;-df %&gt;% pull(participant_gender) %&gt;% table()\ncbind(n=tbl,pct=round(prop.table(tbl)*100,2))\n\nThis is our new dataset on which we will build our network.\n\ndf &lt;- df %&gt;%\n  group_by(academic_year) %&gt;%\n  filter(between(x = participant_age,17,28),\n         mobility_duration&gt;3) %&gt;%\n  summarise(m_participants=mean(participants),\n            sending_country_code,receiving_country_code,\n            participant_gender,.groups=\"drop\") %&gt;%\n  ungroup() %&gt;%\n  select(-m_participants) %&gt;%\n  distinct()\nkableExtra::kable(head(df))%&gt;%\n  kableExtra::kable_styling(latex_options = \"scale_down\")\n\nAt this point I‚Äôd like to have the full country‚Äôs name, and use {ISOcodes} package. I do that because I‚Äôd like to make a spatial visualization as well. The package contains the values for the countries‚Äô abbreviations coded as ‚ÄúAlpha_2‚Äù. I needed to adjust UK and Greece. To verify this you might need to use the count() function and the str_detect() a couple of times before identifying all the values that needs an adjustment.\n\nlibrary(ISOcodes)\nisocodes&lt;-ISOcodes::ISO_3166_1\nisocodes2 &lt;- isocodes%&gt;%\n  mutate(Alpha_2=case_when(Alpha_2==\"GB\"~\"UK\",\n                           Alpha_2==\"GR\"~\"EL\",\n                           TRUE ~ Alpha_2))\n\nSome more manipulations for selecting just the countries in the dataset.\n\nsending_country_code &lt;- df %&gt;% count(sending_country_code) %&gt;% select(-n) %&gt;% unlist()\nreceiving_country_code &lt;- df %&gt;% count(receiving_country_code) %&gt;% select(-n) %&gt;% unlist()\n\nsending &lt;- isocodes2 %&gt;% filter(Alpha_2 %in% sending_country_code)\nreceiving &lt;- isocodes2 %&gt;% filter(Alpha_2 %in% receiving_country_code)\n\nsending_unlst &lt;- sending %&gt;% count(Name) %&gt;% select(-n) %&gt;% unlist()\nreceiving_unlst &lt;- receiving %&gt;% count(Name) %&gt;% select(-n) %&gt;% unlist()\n\n{ggplot2} package provides spatial data, as you might notice, students of the Erasmus programs come from all over the World. Still, some countries‚Äô name adjustments are needed.\n\nworld &lt;- map_data(\"world\")%&gt;%\n  filter(!region==\"Antarctica\")%&gt;%\n  mutate(region=case_when(region==\"UK\"~ \"United Kingdom\",\n                          region==\"Czech Republic\"~\"Czechia\",\n                          region==\"Moldova\"~\"Moldova, Republic of\",\n                          region==\"Palestine\"~\"Palestine, State of\",\n                          region==\"Russia\" ~ \"Russian Federation\",\n                          TRUE ~ region))\n\n\nsending_geo &lt;- world %&gt;% filter(region %in% sending_unlst)\n\nsending_geo_full &lt;- sending %&gt;%\n  select(Alpha_2,Name) %&gt;%\n  left_join(sending_geo,by = c(\"Name\"=\"region\"))\n\n\nreceiving_geo &lt;- world %&gt;% filter(region %in% receiving_unlst)\n\nreceiving_geo_full &lt;- receiving %&gt;%\n  select(Alpha_2,Name) %&gt;%\n  left_join(receiving_geo,by = c(\"Name\"=\"region\"))\n\nThen finally, make the centroids. We will use this in the visualization further below.\n\nsending_geo_centroids &lt;- sending_geo_full %&gt;%\n  group_by(Name) %&gt;%\n  mutate(avg_long = mean(range(long)),avg_lat = mean(range(lat))) %&gt;%\n  count(Alpha_2,Name,avg_long,avg_lat)\n\nreceiving_geo_centroids &lt;- receiving_geo_full %&gt;%\n  group_by(Name) %&gt;%\n  mutate(avg_long = mean(range(long)),avg_lat = mean(range(lat))) %&gt;%\n  count(Alpha_2,Name,avg_long,avg_lat)\n\ndf2 is our new data set, made of countries‚Äô names and spatials:\n\ndf2 &lt;- df %&gt;%\n  left_join(sending_geo_centroids,by=c(\"sending_country_code\"=\"Alpha_2\")) %&gt;%\n  left_join(receiving_geo_centroids,by=c(\"receiving_country_code\"=\"Alpha_2\")) %&gt;%\n  rename(sending_country_name=Name.x,receiving_country_name=Name.y,\n         avg_long_s=avg_long.x,avg_lat_s=avg_lat.x,\n         avg_long_r=avg_long.y,avg_lat_r=avg_lat.y)%&gt;%\n  select(-n.x,-n.y)\ndf2 %&gt;% head\n\norder_sending and order_receiving will be very useful for a third visualization in which all the sending countries will be matched with students‚Äô destinations. We will see this further below.\n\norder_sending &lt;- df2%&gt;%\n  count(sending_country_name,sort=TRUE)%&gt;%\n  mutate(index_sending=seq(1,length(sending_country_name),1),\n         index_sending=rev(index_sending))%&gt;%\n  drop_na()\n\norder_receiving &lt;- df2%&gt;%\n  count(receiving_country_name,sort=TRUE)%&gt;%\n  mutate(index_receiving=seq(1,length(receiving_country_name),1),\n         index_receiving=rev(index_receiving))%&gt;%\n  drop_na()\n\nAnd here is the data set that we will use to make the rank network:\n\nerasmus_network &lt;- df2 %&gt;%\n  select(academic_year,\n         sending_country_name,receiving_country_name) %&gt;%\n  distinct() %&gt;%\n  left_join(order_sending,by=\"sending_country_name\") %&gt;%\n  left_join(order_receiving,by=\"receiving_country_name\") %&gt;%\n  mutate(group = glue::glue(\"{sending_country_name}-{receiving_country_name}\")) %&gt;%\n  distinct() %&gt;%\n  drop_na() %&gt;%\n  arrange(index_sending) %&gt;%\n  mutate(year_id=case_when(academic_year==\"2014-2015\"~1,\n                           academic_year==\"2015-2016\"~2,\n                           academic_year==\"2016-2017\"~3,\n                           academic_year==\"2017-2018\"~4,\n                           academic_year==\"2018-2019\"~5,\n                           academic_year==\"2019-2020\"~6)) %&gt;%\n  relocate(year_id) %&gt;%\n  arrange(year_id)\n\nerasmus_network %&gt;% head\n\nLoad the packages for setting a nice font.\n\nlibrary(showtext)\nlibrary(sysfonts)\nlibrary(extrafont)\n\nshowtext::showtext_auto()\nshowtext::showtext_opts(dpi=320)\n\nfont_add_google(name=\"Noto Sans\",family=\"notosans\")\n\n\nerasmus_network%&gt;%\n  arrange(index_sending)%&gt;%\n  ggplot()+\n  \n  geom_text(aes(x = -2, y = index_sending+5, label = sending_country_name), \n            vjust=0, \n            hjust=\"left\", color = \"darkred\", size = 3) +\n  \n  ggbump::geom_sigmoid(aes(x = -2, xend = 16.1, \n                           y = index_sending+5, yend =index_receiving+18, \n                           group=factor(group),color=receiving_country_name), \n                       alpha = .6, smooth = 10, size = 0.1,show.legend = F) +\n  \n  geom_text(aes(x = 16, y = index_receiving+17.5, label = receiving_country_name), \n            vjust=-1.5, hjust=\"right\", color = \"darkred\", size = 3) +\n  coord_cartesian()+\n  theme_void()\n\nOr a simplified version:\n\n\nnetwork\n\nLet‚Äôs select Top 5 sending countries:\n\nerasmus_network2 &lt;- erasmus_network %&gt;%\n  filter(sending_country_name%in%c(\"Italy\",\n                                   \"Germany\",\n                                   \"United Kingdom\",\n                                   \"Romania\",\"Spain\")) %&gt;%\n  mutate(sending_country_name=case_when(sending_country_name==\"United Kingdom\"~\"UK\",\n                                        TRUE~sending_country_name))%&gt;%\n  count(year_id,academic_year,sending_country_name) %&gt;%\n  group_by(academic_year)%&gt;%\n  mutate(rank=rank(x=n))%&gt;%\n  ungroup()\n\nerasmus_network2 %&gt;% head()\n\n\nlibrary(ggthemes)\n\nggplot(erasmus_network2,\n       mapping=aes(academic_year,rank,\n                   group=factor(sending_country_name),\n                   color=factor(sending_country_name)))+ \n  geom_point(size = 7) +\n  geom_text(data = erasmus_network2 %&gt;% filter(year_id == min(year_id)),\n            aes(x = year_id - .1, \n                label = sending_country_name), size = 4, hjust = 1) +\n  geom_text(data = erasmus_network2 %&gt;% \n              filter(year_id == max(year_id)),\n            aes(x = year_id + .1, label = sending_country_name), \n            size = 4, hjust = 0,check_overlap = T) +\n  geom_bump(size = 2, smooth = 8) +\n  labs(y = \"RANK\",\n       x = \"Academic Year\",\n       title=\"Erasmus Top 5 student exchange countries\",\n       subtitle=\"Ranks of the highest sending frequency\",\n       caption=\"DataSource: Erasmus student mobility | Data.Europa.eu | Wimdu.co\\nDataViz: Federica Gazzelloni | #TidyTuesday Week 10 Erasmus\") +\n  scale_y_reverse() +\n  scale_color_manual(values = wesanderson::wes_palette(5, name = \"Royal2\"))+\n  cowplot::theme_minimal_grid(font_size = 14, line_size = 0) +\n  theme(legend.position = \"none\",\n        panel.grid.major = element_blank(),\n        plot.title = element_text(color=\"#ffc7ba\"),\n        plot.subtitle = element_text(color=\"#ffc7ba\"),\n        plot.caption = element_text(color=\"#ffc7ba\",size=8),\n        axis.text = element_text(color=\"#ffc7ba\"),\n        axis.title = element_text(color=\"#ffc7ba\"),\n        plot.background = element_rect(color=\"black\",fill=\"black\"),\n        panel.background = element_rect(color=\"black\",fill=\"black\"))\n\nHere is the final part of this post, I set the spatials for making a map visualizaton of the sending to receiving countries.\n\nsending_geo_full2&lt;-sending_geo_full%&gt;%mutate(direction=\"sending\")\nreceiving_geo_full2&lt;-receiving_geo_full%&gt;%mutate(direction=\"receiving\")\n\ngeo_full &lt;-rbind(sending_geo_full2,receiving_geo_full2)\n\n\ncentr_s&lt;- sending_geo_centroids%&gt;%mutate(direction=\"Sending\")\ncentr_r&lt;- receiving_geo_centroids%&gt;%mutate(direction=\"Receiving\")\ncentroids&lt;-rbind(centr_s,centr_r)%&gt;%\n  mutate(direction=as.factor(direction))\n  \nlevels(centroids$direction)&lt;-c(\"Sending\",\"Receiving\")\n\n\ngeo_full2&lt;- geo_full%&gt;%\n  mutate(direction=as.factor(direction))\n\nlevels(geo_full2$direction)&lt;-c(\"Receiving\",\"Sending\")\n\ngeo_full2$direction&lt;-relevel(geo_full2$direction,ref=\"Sending\")\n\n\nggplot(geo_full2)+\n  geom_polygon(data = world,\n               aes(x=long,y=lat,group=group),fill=\"grey78\",color=\"grey5\")+\n  \n  geom_polygon(aes(x=long,y=lat,group=group,fill=direction),alpha=0.3)+\n  \n  geom_point(data=centroids,\n             aes(x=avg_long, y=avg_lat,color=direction,shape=direction))+\n \n  coord_map(\"ortho\", orientation = c(33.366449, 24.022840, 0))+\n  facet_wrap(vars(direction))+\n  scale_x_continuous(\"Latitude\", expand=c(0,0)) +\n  scale_y_continuous(\"Longitude\", expand=c(0,0)) +\n  theme_void()+\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "content/dataviz/data_visualization/erasmus/index.html#overview",
    "href": "content/dataviz/data_visualization/erasmus/index.html#overview",
    "title": "Erasmus students exchange",
    "section": "",
    "text": "This week 10 of #TidyTuesday 2022 theme is #Erasmus students exchange in the European countries.\nThe data set is from Erasmus student mobility, Data.Europa.eu and Wimdu.co to discover the most popular Erasmus destinations.\nThe idea is to make a network of sending and receiving countries, let‚Äôs have a look at the data.\n\nif(!require(pacman)) install.packages(\"pacman\")\npacman::p_load(tidyverse, ggbump, cowplot, wesanderson)\n\n\nerasmus &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-03-08/erasmus.csv')\n\nThe set is made of information about students, such as the age, the nationality, the lenght of stay, the gender, academic year, and others. I selected some of them, to extract the information I needed to make the network.\n\ndf &lt;- erasmus%&gt;%\n  select(sending_country_code,receiving_country_code,\n         participant_nationality,participants,\n         participant_age,\n         academic_year,mobility_duration,\n         participant_gender)\n\nkableExtra::kable(head(df)) \n\nLooking at the participant_age we see that we have some misleading data:\n\ndf %&gt;% pull(participant_age) %&gt;% summary()\n\nFor this reason the best way is to filter students between 17 and 28 years old. Also, mobility_duration is quite surprising:\n\ndf %&gt;% pull(mobility_duration) %&gt;% summary()\n\nThe median value of the students‚Äô stay is ONE day, while the mean is just a little above TWO days. Very few students stay more than 10 days, but someone reaches a max of 273 days (39 weeks).\n\ndf %&gt;% count(mobility_duration) %&gt;% mutate(perc = round(n/sum(n)*100,2)) %&gt;% head()\ndf %&gt;% count(mobility_duration) %&gt;% mutate(perc = round(n/sum(n)*100,3)) %&gt;% tail()\n\n\nStudent participants are almost all solo participants as the median shows to be ONE student per observation, TWO students on average, with a max value of 279. So that, to have a picture of the phenomenon select the average value of the student participants as representative.\n\ndf %&gt;% pull(participants) %&gt;% summary(participants)\n\nFinally, gender, Females are slightly more than males, just a little above 50%.\n\ntbl &lt;-df %&gt;% pull(participant_gender) %&gt;% table()\ncbind(n=tbl,pct=round(prop.table(tbl)*100,2))\n\nThis is our new dataset on which we will build our network.\n\ndf &lt;- df %&gt;%\n  group_by(academic_year) %&gt;%\n  filter(between(x = participant_age,17,28),\n         mobility_duration&gt;3) %&gt;%\n  summarise(m_participants=mean(participants),\n            sending_country_code,receiving_country_code,\n            participant_gender,.groups=\"drop\") %&gt;%\n  ungroup() %&gt;%\n  select(-m_participants) %&gt;%\n  distinct()\nkableExtra::kable(head(df))%&gt;%\n  kableExtra::kable_styling(latex_options = \"scale_down\")\n\nAt this point I‚Äôd like to have the full country‚Äôs name, and use {ISOcodes} package. I do that because I‚Äôd like to make a spatial visualization as well. The package contains the values for the countries‚Äô abbreviations coded as ‚ÄúAlpha_2‚Äù. I needed to adjust UK and Greece. To verify this you might need to use the count() function and the str_detect() a couple of times before identifying all the values that needs an adjustment.\n\nlibrary(ISOcodes)\nisocodes&lt;-ISOcodes::ISO_3166_1\nisocodes2 &lt;- isocodes%&gt;%\n  mutate(Alpha_2=case_when(Alpha_2==\"GB\"~\"UK\",\n                           Alpha_2==\"GR\"~\"EL\",\n                           TRUE ~ Alpha_2))\n\nSome more manipulations for selecting just the countries in the dataset.\n\nsending_country_code &lt;- df %&gt;% count(sending_country_code) %&gt;% select(-n) %&gt;% unlist()\nreceiving_country_code &lt;- df %&gt;% count(receiving_country_code) %&gt;% select(-n) %&gt;% unlist()\n\nsending &lt;- isocodes2 %&gt;% filter(Alpha_2 %in% sending_country_code)\nreceiving &lt;- isocodes2 %&gt;% filter(Alpha_2 %in% receiving_country_code)\n\nsending_unlst &lt;- sending %&gt;% count(Name) %&gt;% select(-n) %&gt;% unlist()\nreceiving_unlst &lt;- receiving %&gt;% count(Name) %&gt;% select(-n) %&gt;% unlist()\n\n{ggplot2} package provides spatial data, as you might notice, students of the Erasmus programs come from all over the World. Still, some countries‚Äô name adjustments are needed.\n\nworld &lt;- map_data(\"world\")%&gt;%\n  filter(!region==\"Antarctica\")%&gt;%\n  mutate(region=case_when(region==\"UK\"~ \"United Kingdom\",\n                          region==\"Czech Republic\"~\"Czechia\",\n                          region==\"Moldova\"~\"Moldova, Republic of\",\n                          region==\"Palestine\"~\"Palestine, State of\",\n                          region==\"Russia\" ~ \"Russian Federation\",\n                          TRUE ~ region))\n\n\nsending_geo &lt;- world %&gt;% filter(region %in% sending_unlst)\n\nsending_geo_full &lt;- sending %&gt;%\n  select(Alpha_2,Name) %&gt;%\n  left_join(sending_geo,by = c(\"Name\"=\"region\"))\n\n\nreceiving_geo &lt;- world %&gt;% filter(region %in% receiving_unlst)\n\nreceiving_geo_full &lt;- receiving %&gt;%\n  select(Alpha_2,Name) %&gt;%\n  left_join(receiving_geo,by = c(\"Name\"=\"region\"))\n\nThen finally, make the centroids. We will use this in the visualization further below.\n\nsending_geo_centroids &lt;- sending_geo_full %&gt;%\n  group_by(Name) %&gt;%\n  mutate(avg_long = mean(range(long)),avg_lat = mean(range(lat))) %&gt;%\n  count(Alpha_2,Name,avg_long,avg_lat)\n\nreceiving_geo_centroids &lt;- receiving_geo_full %&gt;%\n  group_by(Name) %&gt;%\n  mutate(avg_long = mean(range(long)),avg_lat = mean(range(lat))) %&gt;%\n  count(Alpha_2,Name,avg_long,avg_lat)\n\ndf2 is our new data set, made of countries‚Äô names and spatials:\n\ndf2 &lt;- df %&gt;%\n  left_join(sending_geo_centroids,by=c(\"sending_country_code\"=\"Alpha_2\")) %&gt;%\n  left_join(receiving_geo_centroids,by=c(\"receiving_country_code\"=\"Alpha_2\")) %&gt;%\n  rename(sending_country_name=Name.x,receiving_country_name=Name.y,\n         avg_long_s=avg_long.x,avg_lat_s=avg_lat.x,\n         avg_long_r=avg_long.y,avg_lat_r=avg_lat.y)%&gt;%\n  select(-n.x,-n.y)\ndf2 %&gt;% head\n\norder_sending and order_receiving will be very useful for a third visualization in which all the sending countries will be matched with students‚Äô destinations. We will see this further below.\n\norder_sending &lt;- df2%&gt;%\n  count(sending_country_name,sort=TRUE)%&gt;%\n  mutate(index_sending=seq(1,length(sending_country_name),1),\n         index_sending=rev(index_sending))%&gt;%\n  drop_na()\n\norder_receiving &lt;- df2%&gt;%\n  count(receiving_country_name,sort=TRUE)%&gt;%\n  mutate(index_receiving=seq(1,length(receiving_country_name),1),\n         index_receiving=rev(index_receiving))%&gt;%\n  drop_na()\n\nAnd here is the data set that we will use to make the rank network:\n\nerasmus_network &lt;- df2 %&gt;%\n  select(academic_year,\n         sending_country_name,receiving_country_name) %&gt;%\n  distinct() %&gt;%\n  left_join(order_sending,by=\"sending_country_name\") %&gt;%\n  left_join(order_receiving,by=\"receiving_country_name\") %&gt;%\n  mutate(group = glue::glue(\"{sending_country_name}-{receiving_country_name}\")) %&gt;%\n  distinct() %&gt;%\n  drop_na() %&gt;%\n  arrange(index_sending) %&gt;%\n  mutate(year_id=case_when(academic_year==\"2014-2015\"~1,\n                           academic_year==\"2015-2016\"~2,\n                           academic_year==\"2016-2017\"~3,\n                           academic_year==\"2017-2018\"~4,\n                           academic_year==\"2018-2019\"~5,\n                           academic_year==\"2019-2020\"~6)) %&gt;%\n  relocate(year_id) %&gt;%\n  arrange(year_id)\n\nerasmus_network %&gt;% head\n\nLoad the packages for setting a nice font.\n\nlibrary(showtext)\nlibrary(sysfonts)\nlibrary(extrafont)\n\nshowtext::showtext_auto()\nshowtext::showtext_opts(dpi=320)\n\nfont_add_google(name=\"Noto Sans\",family=\"notosans\")\n\n\nerasmus_network%&gt;%\n  arrange(index_sending)%&gt;%\n  ggplot()+\n  \n  geom_text(aes(x = -2, y = index_sending+5, label = sending_country_name), \n            vjust=0, \n            hjust=\"left\", color = \"darkred\", size = 3) +\n  \n  ggbump::geom_sigmoid(aes(x = -2, xend = 16.1, \n                           y = index_sending+5, yend =index_receiving+18, \n                           group=factor(group),color=receiving_country_name), \n                       alpha = .6, smooth = 10, size = 0.1,show.legend = F) +\n  \n  geom_text(aes(x = 16, y = index_receiving+17.5, label = receiving_country_name), \n            vjust=-1.5, hjust=\"right\", color = \"darkred\", size = 3) +\n  coord_cartesian()+\n  theme_void()\n\nOr a simplified version:\n\n\nnetwork\n\nLet‚Äôs select Top 5 sending countries:\n\nerasmus_network2 &lt;- erasmus_network %&gt;%\n  filter(sending_country_name%in%c(\"Italy\",\n                                   \"Germany\",\n                                   \"United Kingdom\",\n                                   \"Romania\",\"Spain\")) %&gt;%\n  mutate(sending_country_name=case_when(sending_country_name==\"United Kingdom\"~\"UK\",\n                                        TRUE~sending_country_name))%&gt;%\n  count(year_id,academic_year,sending_country_name) %&gt;%\n  group_by(academic_year)%&gt;%\n  mutate(rank=rank(x=n))%&gt;%\n  ungroup()\n\nerasmus_network2 %&gt;% head()\n\n\nlibrary(ggthemes)\n\nggplot(erasmus_network2,\n       mapping=aes(academic_year,rank,\n                   group=factor(sending_country_name),\n                   color=factor(sending_country_name)))+ \n  geom_point(size = 7) +\n  geom_text(data = erasmus_network2 %&gt;% filter(year_id == min(year_id)),\n            aes(x = year_id - .1, \n                label = sending_country_name), size = 4, hjust = 1) +\n  geom_text(data = erasmus_network2 %&gt;% \n              filter(year_id == max(year_id)),\n            aes(x = year_id + .1, label = sending_country_name), \n            size = 4, hjust = 0,check_overlap = T) +\n  geom_bump(size = 2, smooth = 8) +\n  labs(y = \"RANK\",\n       x = \"Academic Year\",\n       title=\"Erasmus Top 5 student exchange countries\",\n       subtitle=\"Ranks of the highest sending frequency\",\n       caption=\"DataSource: Erasmus student mobility | Data.Europa.eu | Wimdu.co\\nDataViz: Federica Gazzelloni | #TidyTuesday Week 10 Erasmus\") +\n  scale_y_reverse() +\n  scale_color_manual(values = wesanderson::wes_palette(5, name = \"Royal2\"))+\n  cowplot::theme_minimal_grid(font_size = 14, line_size = 0) +\n  theme(legend.position = \"none\",\n        panel.grid.major = element_blank(),\n        plot.title = element_text(color=\"#ffc7ba\"),\n        plot.subtitle = element_text(color=\"#ffc7ba\"),\n        plot.caption = element_text(color=\"#ffc7ba\",size=8),\n        axis.text = element_text(color=\"#ffc7ba\"),\n        axis.title = element_text(color=\"#ffc7ba\"),\n        plot.background = element_rect(color=\"black\",fill=\"black\"),\n        panel.background = element_rect(color=\"black\",fill=\"black\"))\n\nHere is the final part of this post, I set the spatials for making a map visualizaton of the sending to receiving countries.\n\nsending_geo_full2&lt;-sending_geo_full%&gt;%mutate(direction=\"sending\")\nreceiving_geo_full2&lt;-receiving_geo_full%&gt;%mutate(direction=\"receiving\")\n\ngeo_full &lt;-rbind(sending_geo_full2,receiving_geo_full2)\n\n\ncentr_s&lt;- sending_geo_centroids%&gt;%mutate(direction=\"Sending\")\ncentr_r&lt;- receiving_geo_centroids%&gt;%mutate(direction=\"Receiving\")\ncentroids&lt;-rbind(centr_s,centr_r)%&gt;%\n  mutate(direction=as.factor(direction))\n  \nlevels(centroids$direction)&lt;-c(\"Sending\",\"Receiving\")\n\n\ngeo_full2&lt;- geo_full%&gt;%\n  mutate(direction=as.factor(direction))\n\nlevels(geo_full2$direction)&lt;-c(\"Receiving\",\"Sending\")\n\ngeo_full2$direction&lt;-relevel(geo_full2$direction,ref=\"Sending\")\n\n\nggplot(geo_full2)+\n  geom_polygon(data = world,\n               aes(x=long,y=lat,group=group),fill=\"grey78\",color=\"grey5\")+\n  \n  geom_polygon(aes(x=long,y=lat,group=group,fill=direction),alpha=0.3)+\n  \n  geom_point(data=centroids,\n             aes(x=avg_long, y=avg_lat,color=direction,shape=direction))+\n \n  coord_map(\"ortho\", orientation = c(33.366449, 24.022840, 0))+\n  facet_wrap(vars(direction))+\n  scale_x_continuous(\"Latitude\", expand=c(0,0)) +\n  scale_y_continuous(\"Longitude\", expand=c(0,0)) +\n  theme_void()+\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "content/dataviz/data_visualization/european_flights/index.html",
    "href": "content/dataviz/data_visualization/european_flights/index.html",
    "title": "European flights",
    "section": "",
    "text": "This #TidyTuesday week 28 is all about European flights. I was looking for a visualization that would represent the differences among countries in terms of the number of airports versus number of flights. I looked on the internet for getting some inspiration and then landed on Pintarest, where I found exactly what I was hoping for: a Sankey! First of all, What is a Sankey? The answer is clearer after you make one of your own. In short, it is a network graph connecting diffent groups with a ribbon. A few things that made me think about a lot were the requirenment for the type of data to combine, the consequent label results, and the grouping.\n\nSo, let‚Äôs have a go in making a Sankey.\nHave a quick look at the data for European flights:\nLoad the {tidyverse} and the data from: #TidyTuesday GitHub repo\n\nflights &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-07-12/flights.csv')\n\nJus a little cleaning for this type of data with janitor::clean_names() function, and it‚Äôs ready to use.\n\nflights &lt;- flights%&gt;%\n  janitor::clean_names()\n\nThe best way is to select a subgroup among the list of the European countries in the data set, and represent the diversity in aerial traffic.\nAs an example I choose Ukraine airports, and found that there is only one airoprt for Ukraine in the dataset. But, the interesting thing is that it covers on average almost the same aerial traffic of other European countries such as Italy which is provided with a far larger number of airports, in this dataset.\n\nflights %&gt;%\n  filter(state_name == \"Ukraine\") %&gt;%\n  count(state_name,apt_name)\n\nCompare Italy median of the total number of flights by airports with the only available airport aerial traffic in Ukraine:\n\nflights %&gt;%\n  filter(state_name %in% c(\"Ukraine\",\"Italy\")) %&gt;%\n  count(state_name,pivot_label,flt_tot_1) %&gt;%\n  group_by(state_name) %&gt;%\n  summarize(total_median = median(flt_tot_1)) \n\nThe results of this preliminary data exploration arise a question:"
  },
  {
    "objectID": "content/dataviz/data_visualization/european_flights/index.html#overview",
    "href": "content/dataviz/data_visualization/european_flights/index.html#overview",
    "title": "European flights",
    "section": "",
    "text": "This #TidyTuesday week 28 is all about European flights. I was looking for a visualization that would represent the differences among countries in terms of the number of airports versus number of flights. I looked on the internet for getting some inspiration and then landed on Pintarest, where I found exactly what I was hoping for: a Sankey! First of all, What is a Sankey? The answer is clearer after you make one of your own. In short, it is a network graph connecting diffent groups with a ribbon. A few things that made me think about a lot were the requirenment for the type of data to combine, the consequent label results, and the grouping.\n\nSo, let‚Äôs have a go in making a Sankey.\nHave a quick look at the data for European flights:\nLoad the {tidyverse} and the data from: #TidyTuesday GitHub repo\n\nflights &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-07-12/flights.csv')\n\nJus a little cleaning for this type of data with janitor::clean_names() function, and it‚Äôs ready to use.\n\nflights &lt;- flights%&gt;%\n  janitor::clean_names()\n\nThe best way is to select a subgroup among the list of the European countries in the data set, and represent the diversity in aerial traffic.\nAs an example I choose Ukraine airports, and found that there is only one airoprt for Ukraine in the dataset. But, the interesting thing is that it covers on average almost the same aerial traffic of other European countries such as Italy which is provided with a far larger number of airports, in this dataset.\n\nflights %&gt;%\n  filter(state_name == \"Ukraine\") %&gt;%\n  count(state_name,apt_name)\n\nCompare Italy median of the total number of flights by airports with the only available airport aerial traffic in Ukraine:\n\nflights %&gt;%\n  filter(state_name %in% c(\"Ukraine\",\"Italy\")) %&gt;%\n  count(state_name,pivot_label,flt_tot_1) %&gt;%\n  group_by(state_name) %&gt;%\n  summarize(total_median = median(flt_tot_1)) \n\nThe results of this preliminary data exploration arise a question:"
  },
  {
    "objectID": "content/dataviz/data_visualization/european_flights/index.html#does-the-number-of-airports-influence-countries-aerial-traffic",
    "href": "content/dataviz/data_visualization/european_flights/index.html#does-the-number-of-airports-influence-countries-aerial-traffic",
    "title": "European flights",
    "section": "Does the number of airports influence countries‚Äô aerial traffic?",
    "text": "Does the number of airports influence countries‚Äô aerial traffic?\n\nWaffle\nMake the first part of the visualization as a series of waffle, one for each selected country to show the diferences in number of airports.\nSelected are 6 countries with a varied number of airports and flights, this is done to give the idea of the differences.\n\ndf &lt;- flights%&gt;%\n  filter(state_name %in% c(\"Ukraine\",\"Belgium\",\"France\",\"Italy\",\"Spain\",\"United Kingdom\")) %&gt;%\n  group_by(state_name) %&gt;%\n  count(apt_name,sort=T) %&gt;%\n  mutate(apt_id = seq(1,length(state_name),1)) %&gt;%\n  summarise(tot = max(apt_id)) %&gt;%\n  arrange(-tot)\n\ndf\n\nLoad both libraries {waffle} and {ggsankey} to use a feature in the waffle which is provided by the ggsankey package.\n\nlibrary(waffle)\nlibrary(ggsankey)\n\nThe waffles shows the number of airports for the selected countries on a total of 100. To make the waffle we can safetly use the geom_waffle() layer\n\nwaffle &lt;- df %&gt;%\n  mutate(rr = 100 - tot) %&gt;% # this is the remaining part of the 100 set of cubes in the waffle\n  pivot_longer(cols = c(tot,rr),names_to = \"values_name\",values_to = \"values\") %&gt;% \n  arrange(state_name) %&gt;%\n  ggplot(aes(fill = values_name, values = values)) +\n  geom_waffle(n_rows = 10, \n              size = 0.33, \n              make_proportional = F,\n              colour = \"white\", \n              flip = TRUE,\n              show.legend = F) +\n  facet_wrap(~ state_name,nrow = 1)+\n  coord_equal() +\n  scale_fill_manual(values=c(\"grey60\",\"#8E0152\"))+\n  theme_sankey(base_size = 16) +\n  theme_enhance_waffle() +\n  theme(strip.text = element_blank(),\n        plot.background = element_blank(),\n        panel.background = element_blank())\n\nwaffle\n\nSankey\nThe purpose of this sankey is to show the differences among selected countries on number of airports and number of flights, from 2016 to 2022 for 6 selected countries.\nThe {ggsankey} package uses an interesting function make_long() which transform provided object into a longer data frame, with vectors named as:\n\nx, next_x, node, and next_node\n\nready to be used inside the geom_sankey.\n\nsankey &lt;- df %&gt;%\n  left_join(flights %&gt;% select(state_name,flt_tot_1), by = \"state_name\") %&gt;%\n  group_by(state_name,tot) %&gt;%\n  summarize(total_med = median(flt_tot_1),.groups = \"drop\") %&gt;%\n  ungroup() %&gt;%\n  mutate(class = cut(tot,\n                     breaks = c(0,1,5,50,65)), #) %&gt;% pull(tot) %&gt;% summary()\n         tot_class = cut(total_med,\n                         breaks = c(0,10,30,65,120,700))) %&gt;% #count(tot_class)\n  mutate(class_id = case_when(class == \"(0,1]\" ~ \"one\",\n                              class == \"(1,5]\" ~ \"up to 5\",\n                              class == \"(5,50]\" ~ \"up to 50\",\n                              TRUE ~ \"up to 65\"),\n         tot_class_id = case_when(tot_class == \"(0,10]\" ~ \"one\",\n                              tot_class == \"(10,30]\" ~ \"up to 30\",\n                              tot_class == \"(30,65]\" ~ \"up to 65\",\n                              tot_class == \"(65,120]\" ~ \"up to 120\",\n                              TRUE ~ \"up to 700\")) %&gt;%\n  mutate(state_name= ifelse(state_name==\"United Kingdom\",\"UK\",state_name)) %&gt;%\n  # function to make the object ready to be used in the geom_sankey\n  make_long(tot_class_id,class_id,state_name) %&gt;% \n  ggplot(aes(x = x, \n             label= node,\n             next_x = next_x, \n             node = node, \n             next_node = (next_node),\n             fill = factor(node))) +\n  geom_sankey(flow.alpha = 0.8, \n              node.color = 1,\n              show.legend = FALSE) +\n  geom_sankey_text(angle=0,family = \"Roboto Condensed\", size = 3)+\n  scale_fill_manual(values = colorRampPalette(RColorBrewer::brewer.pal(11, \"PiYG\"))(13))+\n  theme_sankey(base_size = 16) +\n  coord_flip(expand = F) +\n  theme(axis.text = element_blank(),\n        axis.title = element_blank(),\n        plot.background = element_blank(),\n        panel.background = element_blank())\n  \nsankey\n\nEuropean Map\nThe map has been saved as map.png and sourced in the main visualization. The code can be found in a separate file named ‚Äúeu_coords.R‚Äù.\n\nUse {cowplot} for assembling the plots, adding notes, the map and the logo as images.\n\nlibrary(cowplot)\n\n\n combo &lt;- ggdraw() +\n  draw_image(\"map.png\",\n             scale=0.5,\n             x=0.3,y=0.4)+\n  draw_plot(waffle,\n            scale=1,\n            x=0,y=0.2) +\n  draw_plot(sankey, \n            scale=0.7,\n            width = 1.4,\n            height = 0.85,\n            x=-0.2, y=-0.093) +\n  draw_label(\"Countries such as France and Spain have the highest number of airports while this\\nseems reasonable, other countries such as Ukraine with just one airport record\\namong the countries with the highest total number of flights. Data are released\\nwithin a range of 7 years from 2016 to 2022.\",\n             x=0.02,y=0.90,size=9,hjust=0, \n             fontfamily=\"Roboto Condensed\") +\n  draw_label(\"N. of airports\", x=0.1,y=0.4,\n             fontfamily=\"Roboto Condensed\") +\n  draw_label(\"N. of flights\\n(median values)\", x=0.1,y=0.16,\n             fontfamily=\"Roboto Condensed\") +\n  draw_image(\"eurocontrol_logo.png\",\n             scale=0.1,\n             x=-0.45,y=-0.52) +\n  draw_label(\"Eurocontrol aviation intelligence\\n(ansperformance.eu)\",\n             x=0.22,y=-0.02,size=9,fontfamily=\"Roboto Condensed\")\n\n\ncombo\n\nUse {ggpubr} for arranging the grid of the main visualization, so it can be annotate with annotate_figure() to making it a bit more standing out with spaces around the plot and top and bottmo annotations already at the right distance/position.\n\nlibrary(ggpubr)\n\nggpubr::annotate_figure() provides a framework for annotating the plot on the four sides top, bottom, left and right. It comes a handy function when you‚Äôd like to position notes or even rich text at specified positions. To use it, it requires an object from ggpubr::ggarrange().\n\nplot &lt;- ggpubr::ggarrange(combo) \n\n  ggpubr::annotate_figure(plot,\n               top = text_grob(\"Does the number of airports influence countries' aerial traffic?\", \n                               color = c(\"#8E0152\"), face = \"bold\", family = \"Roboto Condensed\", \n                               size = 18, vjust = 1.4),\n               bottom = text_grob(\"DataSource:TidyTuesday 2022 week28 European flights\\nDataViz: Federica Gazzelloni (@fgazzelloni)\",\n                                  color = \"#8E0152\",\n                                  hjust = 1, x = 1, face = \"italic\",  family = \"Roboto Condensed\", \n                                  size = 10),\n               left = text_grob(\"\", color = c(\"#7FBC41\"), rot = 90, size=10),\n               right = text_grob(bquote(\"\"), color=c(\"#DE77AE\"), rot = 90, size=10),\n                fig.lab = \"European flights\", fig.lab.face = \"bold\")\n\nThen finally, save it as .png file with ggsave() function. I specified a height a little bit more than the default values provided as I needed more space.\nThe other arguments, dpi and bg are to set the pixels and the background color.\n\nggsave(\"waffle_sankey.png\",\n       dpi=320,\n       bg = \"grey95\",\n       height = 7.2)"
  },
  {
    "objectID": "content/dataviz/data_visualization/oregonfrogs/index.html",
    "href": "content/dataviz/data_visualization/oregonfrogs/index.html",
    "title": "Oregon Spotted a frog: Rana Pretiosa",
    "section": "",
    "text": "This #TidyTuesday week 31 is all about Oregon Spotted a frog: Rana Pretiosa.\nLoad the {tidyverse} and the data from: #TidyTuesday GitHub repo\n\n\nfrogs &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-08-02/frogs.csv')\n\nOr, as I made a package of these data, the dataset can be installed via:\n\n# install.packages(\"remotes\")\nremotes::install_github(\"fgazzelloni/oregonfrogs\")\n\nAnd then:\n\nlibrary(oregonfrogs)\ndata(package = 'oregonfrogs')\n\nMore about Oregon Frogs and spatial modeling is in this R-Ladies DC talk:\n\nThis is the final code for assembling all plots saved in the container images folder using {cowplot} package. In addition, some annotations and grobs are included.\nAll separate scripts are selfcontainers:\n\nGlobe\nNetwork plot\nRoc plot\nVIP plot\n\n\ng &lt;- grid::circleGrob(gp = grid::gpar(fill = NA,color=\"gray\"))\n\n\nlibrary(cowplot)\nggdraw() +\n  draw_label(\"Oregon Spotted a Frog!\", \n             x=0.227,y=0.95,size=34,\n             fontface = \"bold\",\n             fontfamily = \"Roboto Condensed\") +\n  draw_label(label=\"Captured\",x=0.7,y=0.9,fontfamily = \"Roboto Condensed\") +\n  draw_label(label=\"Visual\",x=0.6,y=0.9,fontfamily = \"Roboto Condensed\") +\n  draw_label(label=\"Frequency\",x=0.5,y=0.9,fontfamily = \"Roboto Condensed\") +\n  draw_label(label=\"Radio-telemetry is used to study frogs (Rana pretiosa)\\nat Crane Prairie Reservoir in Oregon.\\nIndividual frog location tracking occurred roughly\\nweekly between September and late November of 2018.\",\n             fontfamily = \"Roboto Condensed\",\n             x=0.015,y=0.85,hjust=0) +\n  draw_label(label=\"On average more males are caught on radio-telemetry\\nfrequencies than females. In the map the grey circles\\nindicate the tracking location ranges based on mean\\nrange difference among frequencies in same subsite. \\n\\nDataSource: #TidyTuesday 2022 week31\\n@USGS data & @fgazzelloni | DataViz: Federica Gazzelloni\",\n             fontfamily = \"Roboto Condensed\",\n             x=0.015,y=0.08,hjust=0,size=11) +\n    draw_label(label=\"On the left is the network\\nof subsite and water type,\\nit shows more frogs are\\ncaptured in specific locations.\\n\\nOn the right is the models\\nranking among many models.\\nRandom Forest is the best\\nperforming. Results shows on\\naverage male are twice more\\nlikely to get caught than\\nfemales. More info:\\nfedericagazzelloni.netlify.app\",\n             fontfamily = \"Roboto Condensed\",\n             x=0.668,y=0.13,hjust=0,size=8) +\n  draw_image(\"container/images/globe.png\",\n             scale=0.18,\n             x=0.4,y=0.38)+\n  draw_image(\"container/images/network_plot.png\",\n             scale=0.29,\n             x=0.022,y=-0.38)+\n  draw_image(\"container/images/roc_plot.png\",\n             scale=0.245,\n             x=0.4,y=-0.38)+\n  draw_image(\"container/images/lake_map.png\",\n             scale=0.7,\n             x=0.14,y=0.01) +\n  draw_image(\"container/images/vip_plot.png\",\n             scale=0.62,\n             x=-0.3,y=-0.03)+\n  draw_image(\"container/images/frog_logo_visual.png\", \n              scale=0.2,\n             x=0.1, y=0.32) +\n  draw_image(\"container/images/frog_logo_captured.png\",\n              scale=0.2,\n             x=0.2, y=0.32) +\n  draw_grob(g, scale = 0.05,x = 0,y = 0.33)+\n  draw_grob(g, scale = 0.025,x = 0,y = 0.33)+\n  draw_grob(g, scale = 0.01,x = 0,y = 0.33)\n\n\n# ggsave(\"w31_frogs.png\",\n#        width=10,\n#        height = 8,\n#        dpi=320,\n#        bg = \"white\")"
  },
  {
    "objectID": "content/dataviz/data_visualization/oregonfrogs/index.html#overview",
    "href": "content/dataviz/data_visualization/oregonfrogs/index.html#overview",
    "title": "Oregon Spotted a frog: Rana Pretiosa",
    "section": "",
    "text": "This #TidyTuesday week 31 is all about Oregon Spotted a frog: Rana Pretiosa.\nLoad the {tidyverse} and the data from: #TidyTuesday GitHub repo\n\n\nfrogs &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-08-02/frogs.csv')\n\nOr, as I made a package of these data, the dataset can be installed via:\n\n# install.packages(\"remotes\")\nremotes::install_github(\"fgazzelloni/oregonfrogs\")\n\nAnd then:\n\nlibrary(oregonfrogs)\ndata(package = 'oregonfrogs')\n\nMore about Oregon Frogs and spatial modeling is in this R-Ladies DC talk:\n\nThis is the final code for assembling all plots saved in the container images folder using {cowplot} package. In addition, some annotations and grobs are included.\nAll separate scripts are selfcontainers:\n\nGlobe\nNetwork plot\nRoc plot\nVIP plot\n\n\ng &lt;- grid::circleGrob(gp = grid::gpar(fill = NA,color=\"gray\"))\n\n\nlibrary(cowplot)\nggdraw() +\n  draw_label(\"Oregon Spotted a Frog!\", \n             x=0.227,y=0.95,size=34,\n             fontface = \"bold\",\n             fontfamily = \"Roboto Condensed\") +\n  draw_label(label=\"Captured\",x=0.7,y=0.9,fontfamily = \"Roboto Condensed\") +\n  draw_label(label=\"Visual\",x=0.6,y=0.9,fontfamily = \"Roboto Condensed\") +\n  draw_label(label=\"Frequency\",x=0.5,y=0.9,fontfamily = \"Roboto Condensed\") +\n  draw_label(label=\"Radio-telemetry is used to study frogs (Rana pretiosa)\\nat Crane Prairie Reservoir in Oregon.\\nIndividual frog location tracking occurred roughly\\nweekly between September and late November of 2018.\",\n             fontfamily = \"Roboto Condensed\",\n             x=0.015,y=0.85,hjust=0) +\n  draw_label(label=\"On average more males are caught on radio-telemetry\\nfrequencies than females. In the map the grey circles\\nindicate the tracking location ranges based on mean\\nrange difference among frequencies in same subsite. \\n\\nDataSource: #TidyTuesday 2022 week31\\n@USGS data & @fgazzelloni | DataViz: Federica Gazzelloni\",\n             fontfamily = \"Roboto Condensed\",\n             x=0.015,y=0.08,hjust=0,size=11) +\n    draw_label(label=\"On the left is the network\\nof subsite and water type,\\nit shows more frogs are\\ncaptured in specific locations.\\n\\nOn the right is the models\\nranking among many models.\\nRandom Forest is the best\\nperforming. Results shows on\\naverage male are twice more\\nlikely to get caught than\\nfemales. More info:\\nfedericagazzelloni.netlify.app\",\n             fontfamily = \"Roboto Condensed\",\n             x=0.668,y=0.13,hjust=0,size=8) +\n  draw_image(\"container/images/globe.png\",\n             scale=0.18,\n             x=0.4,y=0.38)+\n  draw_image(\"container/images/network_plot.png\",\n             scale=0.29,\n             x=0.022,y=-0.38)+\n  draw_image(\"container/images/roc_plot.png\",\n             scale=0.245,\n             x=0.4,y=-0.38)+\n  draw_image(\"container/images/lake_map.png\",\n             scale=0.7,\n             x=0.14,y=0.01) +\n  draw_image(\"container/images/vip_plot.png\",\n             scale=0.62,\n             x=-0.3,y=-0.03)+\n  draw_image(\"container/images/frog_logo_visual.png\", \n              scale=0.2,\n             x=0.1, y=0.32) +\n  draw_image(\"container/images/frog_logo_captured.png\",\n              scale=0.2,\n             x=0.2, y=0.32) +\n  draw_grob(g, scale = 0.05,x = 0,y = 0.33)+\n  draw_grob(g, scale = 0.025,x = 0,y = 0.33)+\n  draw_grob(g, scale = 0.01,x = 0,y = 0.33)\n\n\n# ggsave(\"w31_frogs.png\",\n#        width=10,\n#        height = 8,\n#        dpi=320,\n#        bg = \"white\")"
  },
  {
    "objectID": "content/dataviz/data_visualization/usecases/malaria/index.html",
    "href": "content/dataviz/data_visualization/usecases/malaria/index.html",
    "title": "The case of Malaria",
    "section": "",
    "text": "August 18, 2023 ‚ÄúA case of locally acquired #malaria has been confirmed in Maryland, Washington, D.C., area. Nine cases have been reported this summer in Florida and Texas, the first in the US in 20 years, according to the US Centers for Disease Control and Prevention. #epidemics‚Äù 1\n\n\nThe Malaria case is somehow a concerning case, eradicated all over the World except for some areas in the Africa‚Äôs continent, the highlight of cases of domestic origins are considered epidemics.\nLet‚Äôs dig on some data about Malaria."
  },
  {
    "objectID": "content/dataviz/data_visualization/usecases/malaria/index.html#overview",
    "href": "content/dataviz/data_visualization/usecases/malaria/index.html#overview",
    "title": "The case of Malaria",
    "section": "",
    "text": "August 18, 2023 ‚ÄúA case of locally acquired #malaria has been confirmed in Maryland, Washington, D.C., area. Nine cases have been reported this summer in Florida and Texas, the first in the US in 20 years, according to the US Centers for Disease Control and Prevention. #epidemics‚Äù 1\n\n\nThe Malaria case is somehow a concerning case, eradicated all over the World except for some areas in the Africa‚Äôs continent, the highlight of cases of domestic origins are considered epidemics.\nLet‚Äôs dig on some data about Malaria."
  },
  {
    "objectID": "content/dataviz/data_visualization/usecases/malaria/index.html#who-malaria-map",
    "href": "content/dataviz/data_visualization/usecases/malaria/index.html#who-malaria-map",
    "title": "The case of Malaria",
    "section": "WHO Malaria Map",
    "text": "WHO Malaria Map\nThe first source of data is the WHO Malaria Map, a collection of information about location of cases, the vector species, their invasive status, and other variables of interest such as temporal of the study, the sampling methods, and so on.\nLoad necessary libraries\n\nlibrary(tidyverse)\nlibrary(readxl)\n\n\nmalaria_who &lt;- read_excel(\"data/who_data.xlsx\", \n    sheet = \"Data\")\nmalaria_who %&gt;% head()\n\n\ndim(malaria_who)\n\n\nmalaria_who%&gt;%glimpse\n\n\nmalaria_who &lt;- malaria_who %&gt;%\n  janitor::clean_names() %&gt;%\n  mutate(longitude=as.double(longitude),latitude=as.double(latitude))\n\nHistorical collection of data about mosquito number shows data is collected from 1985 and updated to 2022.\nLet‚Äôs omit missing values for the year_start variable and create a new variable midyear which is the middle year between the year_start and year_end of each study.\n\nmalaria_who_mid_year &lt;- malaria_who %&gt;%\n  filter(!is.na(year_start),\n         !is.na(year_end),\n         !is.na(mosquito_number),\n         !mosquito_number==\"NR\") %&gt;%\n  mutate(mosquito_number=as.numeric(mosquito_number),\n         year_end=as.double(year_end),\n         midyear=round((year_end+year_start)/2,0))\n\nAlong the time, the trend of the number of mosquito varied with ups and downs.\n\nmalaria_who_mid_year %&gt;%\ngroup_by(midyear)%&gt;%# count(midyear)\n  mutate(avg_n_mosquito=mean(mosquito_number))%&gt;%\n  ungroup()%&gt;%\n  count(country_name,midyear,avg_n_mosquito,invasive_status)%&gt;%\n  ggplot(aes(x=midyear,y=log10(avg_n_mosquito),\n             group=invasive_status,color=invasive_status))+\n  geom_line(linewidth=2)+\n  scale_color_viridis_d(labels = c(\"Invasive\", \"Native\"),\n                       guide = guide_legend(reverse=TRUE,\n                                            override.aes = list(size = 10)))+\n  coord_cartesian(clip = 'off') +\n  labs(title=\"Time series Malaria 1985 - 2022\",\n       caption=\"Graphics: FG\",color=\"Status\")+ \n  ggthemes::theme_fivethirtyeight()\n\nMosquito have been categorized as invasive species, after 2016.\n\nmalaria_who_mid_year%&gt;%\n  filter(midyear&gt;= 2010)%&gt;%\n  ggplot(aes(x=factor(midyear),y=log10(mosquito_number),\n             group=midyear,fill=invasive_status))+\n  geom_violin()+\n  scale_fill_viridis_d(labels = c(\"Invasive\", \"Native\"),\n                       guide = guide_legend(reverse=TRUE,\n                                            override.aes = list(size = 10)))+\n  ggthemes::theme_fivethirtyeight()+\n  theme(legend.position = \"top\")+\n  labs(title=\"When moqsquito became tag invasive\",\n       caption=\"Graphics: FG\",fill=\"Status\")\n\nThe location of cases revealed by consistent sentinel surveillance procedure, identify the area in the south est Africa/Asia to be the most affected by the danger of malaria virus to spread across the rest of the World.\nHere is a map of the invasive vector species in this area:\n\nworld &lt;- map_data(\"world\")%&gt;%filter(!region==\"Antarctica\")\n\nmalaria_who %&gt;%\n  ggplot(mapping=aes(x=longitude,y=latitude))+\n  geom_polygon(data=world,\n               mapping=aes(x=long,y=lat,group=group),\n               linewidth=0.2,\n               color=\"grey80\",\n               fill=\"white\")+\n  geom_point(aes(fill=invasive_status),\n             color=\"grey80\",\n             shape=21,\n             stroke=0.2,\n             size=0.7,alpha=0.5)+\n  coord_sf(xlim = c(-50, 110), expand = TRUE) +\n  scale_fill_viridis_d()+\n  labs(title=\"Invasive vector species from 1985 to 2022\",fill=\"Status\")+\n  ggthemes::theme_map()+\n  theme(plot.background = element_rect(color=\"steelblue\",fill=\"steelblue\"))\n\nLet‚Äôs zoom in to the center of the mass points. Setting the mean range of the latitude and the longitude, to identify the central point, within the mass of points where mosquito were located, and setting a zoom level, a closer focus at the locations is possible, even with a specification of the new range of to be assigned to the map boundaries.2\n\nlon_avg &lt;- mean(range(malaria_who$longitude))\nlat_avg &lt;- mean(range(malaria_who$latitude))\nlon_avg;lat_avg\n\n\nzoom_to &lt;- c(lon_avg,lat_avg)  \n\nzoom_level &lt;- 1.5\n\nlon_span &lt;- 360 / 2^zoom_level\nlat_span &lt;- 180 / 2^zoom_level\n\n\nlon_bounds &lt;- c(zoom_to[1] - lon_span / 2, zoom_to[1] + lon_span / 2)\nlat_bounds &lt;- c(zoom_to[2] - lat_span / 2, zoom_to[2] + lat_span / 2)\n\n\nlibrary(sf)\n\n\nggplot()+\n  geom_polygon(data=world,\n               mapping=aes(x=long,y=lat,group=group),\n               linewidth=0.2,\n               color=\"grey80\",\n               fill=\"white\")+\n  geom_point(data= malaria_who,\n             mapping=aes(x=longitude,y=latitude,\n                         fill=invasive_status),\n             color=\"black\",\n             shape=21,\n             stroke=0.2,\n             size=3,alpha=0.5)+\n  geom_sf_text(data = st_sfc(st_point(zoom_to), crs = 4326),\n            label = '.') +\n  scale_fill_viridis_d()+\n  coord_sf(xlim = lon_bounds, ylim = lat_bounds,expand = TRUE) + \n  labs(title=\"A closer look at invasive vector species\",\n      subtitle=\"from 1985 to 2022\",\n       caption=\"DataSource: WHO Malaria Data | Graphics: FG\",\n       fill=\"Status\")+\n  ggthemes::theme_map()+\n  theme(text=element_text(family=\"Roboto Condensed\"),\n        plot.title = element_text(size = 18),\n        plot.background = element_rect(color=\"steelblue\",fill=\"steelblue\"),\n        legend.position = c(0,0.001))"
  },
  {
    "objectID": "content/dataviz/data_visualization/usecases/malaria/index.html#incidence-of-malaria",
    "href": "content/dataviz/data_visualization/usecases/malaria/index.html#incidence-of-malaria",
    "title": "The case of Malaria",
    "section": "Incidence of malaria",
    "text": "Incidence of malaria\nLooking at a different data source, the Worldbank data provides a reports with the incidence of malaria from 2000 and 2010. 3\n\nmalaria_wb &lt;- read_csv(\"data/worldbank_data.csv\")\n\n\nmalaria_wb_long &lt;- malaria_wb%&gt;%\n  janitor::clean_names()%&gt;%\n  select(-series_name,-series_code) %&gt;%\n  pivot_longer(cols = c(3:14),names_to=\"year\")%&gt;%\n  mutate(value=trimws(value),\n         value=as.numeric(value),\n         value=round(value,3))%&gt;%\n  filter(!is.na(value))%&gt;%\n  mutate(year=gsub(\"_yr[0-9]+$\",\"\",year),\n         year=gsub(\"^x\",\"\",year),\n         year=as.integer(year))%&gt;%\n  arrange(year)\nmalaria_wb_long%&gt;%glimpse\n\n\nmalaria_wb_long%&gt;%\n  group_by(year)%&gt;%\n  reframe(avg=mean(value))%&gt;%\n  mutate(year=as.factor(year))%&gt;%\n  ggplot(aes(year,avg,group=1))+\n  #geom_col()+ \n  geom_line()+\n  labs(title=\"Incidence of Malaria\",\n  subtitle=\"per 1,000 population at risk\",\n  caption = \"Graphics: FG\")+\n  ggthemes::theme_fivethirtyeight()"
  },
  {
    "objectID": "content/dataviz/data_visualization/usecases/malaria/index.html#case-study---antimalarial-drug-resistance",
    "href": "content/dataviz/data_visualization/usecases/malaria/index.html#case-study---antimalarial-drug-resistance",
    "title": "The case of Malaria",
    "section": "Case Study - antimalarial drug resistance",
    "text": "Case Study - antimalarial drug resistance\nTracking antimalarial drug resistance using mosquito blood meals: a cross-sectional study\n\nArticle: https://www.thelancet.com/journals/lanmic/article/PIIS2666-5247(23)00063-0/fulltext\nGitHub repository: https://github.com/hannaehrlich/maldrugres_SSA\n\n\nurl &lt;- \"https://raw.githubusercontent.com/hannaehrlich/maldrugres_SSA/main/Survey_MolecMarker_Data.csv\"\n\n\nmaldrugres &lt;- read.csv(url)\nnames(maldrugres)\n\n\nmaldrugres &lt;- maldrugres %&gt;%\n  filter(!is.na(Lat),!is.na(Drug),!Drug==\"\")\n\nmaldrugres%&gt;%count(Drug)\n\n\nworld &lt;- map_data(\"world\")%&gt;%filter(!region==\"Antarctica\")\nafrica &lt;- world%&gt;%filter(long &gt;= -50,long&lt; 60)\n\nggplot(data= maldrugres) +\n  geom_polygon(data= africa, \n               mapping=aes(x=long,y=lat,group=group),\n               linewidth=0.1,\n               color=\"grey70\",fill=\"grey90\")+\n  geom_point(mapping=aes(x=Lon,y=Lat,\n                         color=Drug,\n                         fill=Present),\n             shape=21,\n             stroke=0.2,\n             size=3)+\n  scale_color_manual(values = c(\"steelblue\",\"darkred\"))+\n  scale_fill_gradient(low=NA,high = \"darkred\")+\n  coord_sf(xlim = c(-20,50),ylim = c(-40,60),clip = \"off\")+\n    labs(title=\"antiMalarial drug resistance\",\n       caption=\"DataSource: GitHub hannaehrlich/maldrugres_SSA | Graphics: FG\")+\n  ggthemes::theme_map()+\n  theme(text = element_text(family=\"Roboto Condensed\"),\n        plot.title = element_text(size=30,hjust = 0.5,family=\"Roboto Condensed\"),\n        plot.title.position = \"plot\",\n        legend.position = c(-0.6,0.1))\n\n\nmaldrugres_new &lt;- maldrugres%&gt;%\n  mutate(MidYear = round((StartYr+EndYr)/2,0),\n         MidYear= as.factor(MidYear))%&gt;%\n  select(Country,Site,Lon,Lat,MidYear,Tested,Present,MixedPres,Drug)%&gt;%\n  janitor::clean_names()\n\nmaldrugres_new%&gt;%head\n\nTidymodels\nEDA\n\nmaldrugres_new%&gt;%\n  count(drug)%&gt;%\n  ggplot(aes(x=drug,y=n,fill=drug))+\n  geom_col()+\n  labs(title=\"Drug class imbalance\",\n       caption=\"Graphics: FG\")+\n  scale_fill_viridis_d()+\n  ggthemes::theme_fivethirtyeight()+\n  theme(legend.position = \"none\")\n\n\nmaldrugres_new%&gt;%\n  group_by(country,drug)%&gt;%\n  reframe(avg_drug=mean(present))%&gt;%\n  ggplot(aes(x=avg_drug,y=fct_reorder(country,avg_drug)))+\n  geom_col(aes(fill=drug))+\n  scale_fill_viridis_d()+\n  labs(title=\"AntiMalarial Drug Resistance Present\",\n       caption=\"Graphics: FG\",\n       x=\"Average value by Country\",y=\"\")+\n   ggthemes::theme_fivethirtyeight()+\n  theme(axis.text.x = element_text(angle=0,hjust = 1))\n\n\nmaldrugres_new %&gt;%\n  ggplot(aes(present))+\n  geom_density()+\n  labs(title=\"Density distribution of antimalarial drug resistance\",\n       caption = \"Graphics: FG\")+\n  ggthemes::theme_fivethirtyeight()\n\n\nmaldrugres_new%&gt;%\n  group_by(country,drug)%&gt;%\n  reframe(avg_drug=mean(present))%&gt;%\n  ggplot(aes(x=avg_drug,y=fct_reorder(country,avg_drug)))+\n  geom_boxplot()+\n  labs(title=\"AntiMalaria Drug Resistance Present\",\n       caption=\"Graphics: FG\",\n       x=\"Average value by Country\",y=\"\")+\n  theme(axis.text.x = element_text(angle=0,hjust = 1))+\n  ggthemes::theme_fivethirtyeight()\n\nSpending data\n\nlibrary(tidymodels)\n\n\nset.seed(123)\nsplit &lt;- initial_split(maldrugres_new)\ntraining &lt;- training(split)\ntesting &lt;- testing(split)\ncv_folds &lt;- vfold_cv(training,v = 10)\n\nFeaturing Engineering\n\nrec_pca &lt;- recipe(present ~., training) %&gt;%\n  step_dummy(all_nominal_predictors(),keep_original_cols = F)%&gt;%\n  step_corr(all_numeric_predictors())%&gt;%\n  step_normalize(all_predictors())%&gt;%\n  step_pca(all_predictors())\n  \nrec_pca_df &lt;- rec_pca %&gt;%\nprep()%&gt;%\n  juice()%&gt;%\n  cbind(drug=training$drug,country=training$country)\nrec_pca_df%&gt;%head\n\n\nrec_pca_df %&gt;%\nggplot(aes(x=PC1,PC2,group=drug,color=drug))+\n  geom_point()+\n  geom_smooth(se=F)+\n  scale_color_viridis_d()+\n  labs(title=\"Principal Components Analysis\",\n       caption = \"Graphics: FG\")+\n  ggthemes::theme_fivethirtyeight()+\n  theme(axis.title = element_text())\n\n\nrec_pca_df %&gt;%\nggplot(aes(x=PC1,y=fct_reorder(country,PC1),group=country))+\n  geom_boxplot()+\n  labs(title=\"Principal Components Analysis - boxplot\",\n       caption = \"Graphics: FG\",y=\"\")+\n  ggthemes::theme_fivethirtyeight()+\n    theme(axis.title = element_text(),\n          plot.title = element_text(hjust = 1))\n\n\nrec_pca_df %&gt;%\nggplot(aes(x=PC1,y=present))+\n  geom_point()+\n  scale_y_log10()+\n  geom_smooth(method = 'gam', formula = y ~ s(x, bs = \"cs\"))+\n  labs(title=\"Principal Components Analysis - scatterplot\",\n       caption = \"Graphics: FG\",y=\"Present\")+\n  ggthemes::theme_fivethirtyeight()+\n    theme(axis.title = element_text(),\n          plot.title = element_text(hjust = 0))\n\n\nrec_ica &lt;- recipe(present ~., training) %&gt;%\n  step_dummy(all_nominal_predictors(),keep_original_cols = F)%&gt;%\n  step_corr(all_numeric_predictors())%&gt;%\n  step_normalize(all_predictors())%&gt;%\n  step_ica(all_predictors())%&gt;%\n  prep()%&gt;%\n  juice()%&gt;%\n  cbind(drug=training$drug)\n\n\nrec_ica %&gt;%\nggplot(aes(x=IC1,IC2,group=drug,color=drug))+\n  geom_point()+\n  geom_smooth(se=F)+\n  scale_colour_viridis_d()+\n  labs(title=\"Independent Components Analysis\",\n       caption = \"Graphics: FG\")+\n  ggthemes::theme_fivethirtyeight()+\n    theme(axis.title = element_text(),\n          plot.title = element_text(hjust = 0))"
  },
  {
    "objectID": "content/dataviz/data_visualization/usecases/malaria/index.html#footnotes",
    "href": "content/dataviz/data_visualization/usecases/malaria/index.html#footnotes",
    "title": "The case of Malaria",
    "section": "Footnotes",
    "text": "Footnotes\n\nworldbank_data source: https://databank.worldbank.org/reports.aspx?source=2&series=SH.MLR.INCD.P3&country=#‚Ü©Ô∏é\nworldbank_data source: https://databank.worldbank.org/reports.aspx?source=2&series=SH.MLR.INCD.P3&country=#‚Ü©Ô∏é\nworldbank_data source: https://databank.worldbank.org/reports.aspx?source=2&series=SH.MLR.INCD.P3&country=#‚Ü©Ô∏é"
  },
  {
    "objectID": "content/blog/blog_posts/june2025_book/index.html",
    "href": "content/blog/blog_posts/june2025_book/index.html",
    "title": "Writing a book in R",
    "section": "",
    "text": "Overview\n\n\nIn this post, I will share my experience of writing a book in R, and the challenges I faced along the way.\nI will also discuss the tools I used, the process I followed, and the lessons I learned.\n\n\n\n\nIt‚Äôs been an incredible journey, and while the book is nearing its completion, I feel a mix of emotions‚Äîboth the excitement of sharing it with you and a touch of sadness that this creative process is coming to an end. üòå\nWriting this book has been an immensely rewarding experience. I‚Äôve spent countless hours into selecting the right material, striving to present it in a way that‚Äôs accessible and helpful for early-career researchers and students. One of the biggest challenges was deciding what to include and what to leave out, constantly refining the content to ensure it‚Äôs both comprehensive and easy to follow.\nEven though the book is almost finished, my mind is still buzzing with ideas. There‚Äôs always that lingering thought of what else could be added to make it even better. But at some point, you have to step back and trust that what you‚Äôve created will serve its purpose. And now, it‚Äôs time to see how it will resonate with you, the readers.\nAs I reflect on this journey, I‚Äôm filled with gratitude for all the support and feedback I‚Äôve received along the way. Your encouragement has been invaluable in shaping this book into what it is today.\nSo, what‚Äôs next? While this project may be wrapping up, my passion for writing and sharing knowledge is far from over. Stay tuned for more updates and future projects! In the meantime, I invite you to dive into the book, explore its content, and share your thoughts. Your insights will be crucial as I make final touches and consider potential additions.\nThank you for being a part of this journey. I hope the book serves as a valuable resource for your research and studies. Let‚Äôs see where this new chapter takes us!\n\n\n\n\n Back to top"
  },
  {
    "objectID": "content/stats/statistics/linear_prediction/index.html",
    "href": "content/stats/statistics/linear_prediction/index.html",
    "title": "How does geom_smooth() make predictions",
    "section": "",
    "text": "Linear regression is a statistical technique used to represent the linear relationship between a response \\((y)\\) and a predictor \\((x)\\).\n\\[\ny= \\beta_0 + \\beta_1x\n\\]\n\n\n\n\n\n\nBelow we examine some mismatch in output when plotting the prediction results of a linear model made with the lm() and the predict() functions versus the output produced by the geom_smooth() layer in a ggplot() visualization.\nThe first dataset used is from the HistData package HistData::CholeraDeaths1849. We select just the deaths due to Cholera within 12 months in 1849, and visualize the trend in time of the number of deaths with the addition of a further layer made with the geom_smooth().\n?geom_smooth()\n\n?HistData::CholeraDeaths1849\n\nlibrary(tidyverse)\nlibrary(HistData)\ncholera &lt;- HistData::CholeraDeaths1849 %&gt;%\n  filter(cause_of_death==\"Cholera\")%&gt;%\n  select(date,deaths)\n  \ncholera %&gt;% head\n\n# A tibble: 6 √ó 2\n  date       deaths\n  &lt;date&gt;      &lt;dbl&gt;\n1 1849-01-01     13\n2 1849-01-02     19\n3 1849-01-03     28\n4 1849-01-04     24\n5 1849-01-05     23\n6 1849-01-06     39\n\n\n\nsummary(cholera)\n\n      date                deaths    \n Min.   :1849-01-01   Min.   :   0  \n 1st Qu.:1849-04-02   1st Qu.:   8  \n Median :1849-07-02   Median :  23  \n Mean   :1849-07-02   Mean   : 146  \n 3rd Qu.:1849-10-01   3rd Qu.: 192  \n Max.   :1849-12-31   Max.   :1121  \n\n\n\nggplot(cholera,aes(x=date,y=deaths))+\n  geom_point()+\n  geom_smooth(method = \"lm\")+\n  labs(title=\"Deaths due to Cholera in London (1849)\",\n       x=\"Date\",y=\"Cholera death\")+\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nDeaths due to Cholera in London (1849)\n\n\n\nThe purpose of making a model is that to identify the inside pattern of a series of observations. This means that the model would need to be able to interpolate given observations in order to represent the overall pattern. As in the visualization above the geom_smooth() with the specification of the method=\"lm\" helps us visualize what the direction of a linear pattern would be on this data. If it is a growing pattern or not.\nClearly the points are shaping a bell distribution of deaths in time, and this is not the case of a linear relationship between date and cholera deaths, but we would like to dig into the output of the prediction of the application of a linear model on this data and then compare it with the output of the geom_smooth(method=\"lm\") line.\nLet‚Äôs apply a linear model to this data and make some rough predictions.\n\nmod &lt;- lm(deaths ~ date, data = cholera)\nsummary(mod)\n\n\nCall:\nlm(formula = deaths ~ date, data = cholera)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-265.88 -104.28  -59.87   11.19  930.92 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 2.954e+04  4.878e+03   6.056 3.49e-09 ***\ndate        6.678e-01  1.108e-01   6.026 4.13e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 223.1 on 363 degrees of freedom\nMultiple R-squared:  0.09094,   Adjusted R-squared:  0.08843 \nF-statistic: 36.31 on 1 and 363 DF,  p-value: 4.132e-09\n\n\nThe application of a linear model on this data produced an estimation of the intercept \\((\\beta_0)\\) and the slope \\((\\beta_1)\\).\n\\[\n\\beta_0=29540 \\text{ and } \\beta_1=0.6678\n\\] The intercept is the starting point of a linear model line on the y axes, while the slope is the inclination of the line, that can be positive or negative, indicating the growing or decreasing tendency of the relationship between the response and the predictor.\nLet‚Äôs draw this line.\n\nggplot(cholera, aes(x=date, y=deaths)) + \n    geom_point() + \n    geom_abline(slope=0.6678, intercept=29540,\n                col=\"pink\")+\n  theme_minimal()\n\n\n\n\n\n\n\nNow calculate the prediction and the Root Mean Squared Error (RMSE) to evaluate how the model worked.\n\npredictions &lt;- predict(mod, newdata = NULL)\n\nrmse &lt;- sqrt(mean((predictions - cholera$deaths)^2))\ncat(\"Root Mean Squared Error (RMSE):\", rmse, \"\\n\")\n\nRoot Mean Squared Error (RMSE): 222.4774 \n\n\n\n\n\n\n\n\nWhat if we want to visualize the output of our prediction?\n\n\n\n\nggplot(cholera, aes(x=date)) + \n  geom_point(aes(y=deaths)) + \n  geom_smooth(aes(y=deaths),method = \"lm\",linewidth=2)+\n  geom_line(y=predictions, col=\"red\")+\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nNow let‚Äôs use a different dataset. This data set comes from a paper by Brigham et al.¬†(2003) that analyses some tables from Farr‚Äôs report of the Registrar-General on mortality due to cholera in England in the years 1848-1849, during which there was a large epidemic throughout the country. In this case we do not have the time variable but the numbers of deaths are considered by 38 districts in London.\n?HistData::Cholera\n\ncholera2 &lt;- HistData::Cholera %&gt;%\n  rownames_to_column(var=\"id\")%&gt;%\n  select(id,district,cholera_deaths,popn)\n  \ncholera2 %&gt;% head\n\n  id            district cholera_deaths  popn\n1  1           Newington            907 63074\n2  2         Rotherhithe            352 17208\n3  3          Bermondsey            836 50900\n4  4 St George Southwark            734 45500\n5  5            St Olave            349 19278\n6  6          St Saviour            539 35227\n\n\nThe predictor in this case is a character, we are considering the relationship between the deaths rate and the districts, so we are looking at to see whether the deaths rate is different among 38 districts. The order in this case is arbitrary and this influences the pattern. The geom_smooth() is not drowning a line, it doesn‚Äôt know how the x-axis has to be ordered, because there is not a specified order to follow.\n\nggplot(cholera2, aes(x=id,y=cholera_deaths)) + \n  geom_point()+\n  geom_smooth(method = \"lm\")+\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nwhile if we set an order with as.integer(id), the line can be drawn but it hasn‚Äôt got much meaning. The trend is not going up or down because we are considering districts in the x-axis.\n\nggplot(cholera2, aes(x=as.integer(id),y=cholera_deaths)) + \n  geom_point()+\n  geom_smooth(method=\"lm\")+\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nIf we consider the population, in the middle of 1849, a numeric vector, these values are by districts, each popn value corresponds to the level of population in one of the 38 districts. Let‚Äôs see what happens if we plot popn versus cholera_deaths.\n\nggplot(cholera2, aes(x=popn,y=cholera_deaths)) + \n  geom_point()+\n  geom_smooth(method=\"lm\")+\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nNow the values on the x-axis are numeric and have a meaning to be ordered from the lower to the highest but it is not a trend. Each point is one district population value with some deaths due to cholera. The geom_smooth line it is telling us that if the level of the population is higher, the level of deaths due to cholera is higher, than in other location with a lower level of population.\nBut we can evaluate the growing relationship between population level and numbers of deaths due to cholera.\nLet‚Äôs make a linear model and predict the future, roughly.\n\nmod2 &lt;- lm(cholera_deaths ~ popn , data = cholera2)\nmod2\n\n\nCall:\nlm(formula = cholera_deaths ~ popn, data = cholera2)\n\nCoefficients:\n(Intercept)         popn  \n  1.073e+02    4.357e-03  \n\n\nLet‚Äôs draw this line.\n\nggplot(cholera2, aes(x=popn, y=cholera_deaths)) + \n    geom_point() + \n    geom_abline(slope=0.004357, intercept=107.3,\n                col=\"pink\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\npredictions2 &lt;- predict(mod2, newdata = NULL)\n\nrmse &lt;- sqrt(mean((predictions2 - cholera2$cholera_deaths)^2))\ncat(\"Root Mean Squared Error (RMSE):\", rmse, \"\\n\")\n\nRoot Mean Squared Error (RMSE): 281.7808 \n\n\n\n\n\n\n\n\nWhat if we want to visualize the output of our prediction?\n\n\n\n\nplot &lt;- ggplot(cholera2, aes(x=popn)) + \n  geom_point(aes(y=cholera_deaths)) + \n  geom_smooth(aes(y=cholera_deaths),method = \"lm\",linewidth=2)+\n  geom_line(y=predictions2, col=\"red\")+\n  labs(title=\"Cholera Deaths explanined by\\nLondon Districts Population (1849)\",\n       x=\"1849 London Population by 38 Districts\",\n       y=\"Cholera Deaths\",\n       caption=\"William Farr's Data on Cholera in London, 1849\")+\n  theme_minimal()\nplot\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nCholera Deaths explanined by London Districts Population (1849)\n\n\n\n\nThink about that!\n\n\n\n\n\nImagine that we absolutely want to replicate the geom_smooth(method=\"lm\") line, we would need to consider some steps that the function takes in order to plot the prediction line that doesn‚Äôt much with ours this time. First think to consider is that we haven‚Äôt used new data but just produced the prediction from our dataset. But this is exactly as the same as before.\nThere is a function ggplot_build() that let‚Äôs us dig into the ggplot data manipulation used to make the geom_smooth line.\n?ggplot_build()\n\n# source: https://stackoverflow.com/questions/42673665/geom-smooth-gives-different-fit-than-nls-alone\ndat = ggplot_build(plot)$data[[2]]\n\n`geom_smooth()` using formula = 'y ~ x'\n\ndat &lt;- dat%&gt;%rename(cholera_drate=y,popn=x)\n\nThis time we use the newdata = dat insted of NULL.\n\npredictions3 &lt;- predict(mod2, newdata = dat)\n\nrmse &lt;- sqrt(mean((predictions3 - cholera2$cholera_deaths)^2))\n\nWarning in predictions3 - cholera2$cholera_deaths: longer object length is not\na multiple of shorter object length\n\ncat(\"Root Mean Squared Error (RMSE):\", rmse, \"\\n\")\n\nRoot Mean Squared Error (RMSE): 411.1835 \n\n\n\nggplot(cholera2, aes(x=popn)) + \n  geom_point(aes(y=cholera_deaths)) + \n  geom_smooth(aes(y=cholera_deaths),method = \"lm\",linewidth=2)+\n  geom_line(data=dat,y=predictions3, col=\"red\")+\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nWe eventually matched the geom_smooth line, but why did our predictions on the original data result in that squiggly line?"
  },
  {
    "objectID": "content/stats/statistics/linear_prediction/index.html#overview",
    "href": "content/stats/statistics/linear_prediction/index.html#overview",
    "title": "How does geom_smooth() make predictions",
    "section": "",
    "text": "Linear regression is a statistical technique used to represent the linear relationship between a response \\((y)\\) and a predictor \\((x)\\).\n\\[\ny= \\beta_0 + \\beta_1x\n\\]\n\n\n\n\n\n\nBelow we examine some mismatch in output when plotting the prediction results of a linear model made with the lm() and the predict() functions versus the output produced by the geom_smooth() layer in a ggplot() visualization.\nThe first dataset used is from the HistData package HistData::CholeraDeaths1849. We select just the deaths due to Cholera within 12 months in 1849, and visualize the trend in time of the number of deaths with the addition of a further layer made with the geom_smooth().\n?geom_smooth()\n\n?HistData::CholeraDeaths1849\n\nlibrary(tidyverse)\nlibrary(HistData)\ncholera &lt;- HistData::CholeraDeaths1849 %&gt;%\n  filter(cause_of_death==\"Cholera\")%&gt;%\n  select(date,deaths)\n  \ncholera %&gt;% head\n\n# A tibble: 6 √ó 2\n  date       deaths\n  &lt;date&gt;      &lt;dbl&gt;\n1 1849-01-01     13\n2 1849-01-02     19\n3 1849-01-03     28\n4 1849-01-04     24\n5 1849-01-05     23\n6 1849-01-06     39\n\n\n\nsummary(cholera)\n\n      date                deaths    \n Min.   :1849-01-01   Min.   :   0  \n 1st Qu.:1849-04-02   1st Qu.:   8  \n Median :1849-07-02   Median :  23  \n Mean   :1849-07-02   Mean   : 146  \n 3rd Qu.:1849-10-01   3rd Qu.: 192  \n Max.   :1849-12-31   Max.   :1121  \n\n\n\nggplot(cholera,aes(x=date,y=deaths))+\n  geom_point()+\n  geom_smooth(method = \"lm\")+\n  labs(title=\"Deaths due to Cholera in London (1849)\",\n       x=\"Date\",y=\"Cholera death\")+\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nDeaths due to Cholera in London (1849)\n\n\n\nThe purpose of making a model is that to identify the inside pattern of a series of observations. This means that the model would need to be able to interpolate given observations in order to represent the overall pattern. As in the visualization above the geom_smooth() with the specification of the method=\"lm\" helps us visualize what the direction of a linear pattern would be on this data. If it is a growing pattern or not.\nClearly the points are shaping a bell distribution of deaths in time, and this is not the case of a linear relationship between date and cholera deaths, but we would like to dig into the output of the prediction of the application of a linear model on this data and then compare it with the output of the geom_smooth(method=\"lm\") line.\nLet‚Äôs apply a linear model to this data and make some rough predictions.\n\nmod &lt;- lm(deaths ~ date, data = cholera)\nsummary(mod)\n\n\nCall:\nlm(formula = deaths ~ date, data = cholera)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-265.88 -104.28  -59.87   11.19  930.92 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 2.954e+04  4.878e+03   6.056 3.49e-09 ***\ndate        6.678e-01  1.108e-01   6.026 4.13e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 223.1 on 363 degrees of freedom\nMultiple R-squared:  0.09094,   Adjusted R-squared:  0.08843 \nF-statistic: 36.31 on 1 and 363 DF,  p-value: 4.132e-09\n\n\nThe application of a linear model on this data produced an estimation of the intercept \\((\\beta_0)\\) and the slope \\((\\beta_1)\\).\n\\[\n\\beta_0=29540 \\text{ and } \\beta_1=0.6678\n\\] The intercept is the starting point of a linear model line on the y axes, while the slope is the inclination of the line, that can be positive or negative, indicating the growing or decreasing tendency of the relationship between the response and the predictor.\nLet‚Äôs draw this line.\n\nggplot(cholera, aes(x=date, y=deaths)) + \n    geom_point() + \n    geom_abline(slope=0.6678, intercept=29540,\n                col=\"pink\")+\n  theme_minimal()\n\n\n\n\n\n\n\nNow calculate the prediction and the Root Mean Squared Error (RMSE) to evaluate how the model worked.\n\npredictions &lt;- predict(mod, newdata = NULL)\n\nrmse &lt;- sqrt(mean((predictions - cholera$deaths)^2))\ncat(\"Root Mean Squared Error (RMSE):\", rmse, \"\\n\")\n\nRoot Mean Squared Error (RMSE): 222.4774 \n\n\n\n\n\n\n\n\nWhat if we want to visualize the output of our prediction?\n\n\n\n\nggplot(cholera, aes(x=date)) + \n  geom_point(aes(y=deaths)) + \n  geom_smooth(aes(y=deaths),method = \"lm\",linewidth=2)+\n  geom_line(y=predictions, col=\"red\")+\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nNow let‚Äôs use a different dataset. This data set comes from a paper by Brigham et al.¬†(2003) that analyses some tables from Farr‚Äôs report of the Registrar-General on mortality due to cholera in England in the years 1848-1849, during which there was a large epidemic throughout the country. In this case we do not have the time variable but the numbers of deaths are considered by 38 districts in London.\n?HistData::Cholera\n\ncholera2 &lt;- HistData::Cholera %&gt;%\n  rownames_to_column(var=\"id\")%&gt;%\n  select(id,district,cholera_deaths,popn)\n  \ncholera2 %&gt;% head\n\n  id            district cholera_deaths  popn\n1  1           Newington            907 63074\n2  2         Rotherhithe            352 17208\n3  3          Bermondsey            836 50900\n4  4 St George Southwark            734 45500\n5  5            St Olave            349 19278\n6  6          St Saviour            539 35227\n\n\nThe predictor in this case is a character, we are considering the relationship between the deaths rate and the districts, so we are looking at to see whether the deaths rate is different among 38 districts. The order in this case is arbitrary and this influences the pattern. The geom_smooth() is not drowning a line, it doesn‚Äôt know how the x-axis has to be ordered, because there is not a specified order to follow.\n\nggplot(cholera2, aes(x=id,y=cholera_deaths)) + \n  geom_point()+\n  geom_smooth(method = \"lm\")+\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nwhile if we set an order with as.integer(id), the line can be drawn but it hasn‚Äôt got much meaning. The trend is not going up or down because we are considering districts in the x-axis.\n\nggplot(cholera2, aes(x=as.integer(id),y=cholera_deaths)) + \n  geom_point()+\n  geom_smooth(method=\"lm\")+\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nIf we consider the population, in the middle of 1849, a numeric vector, these values are by districts, each popn value corresponds to the level of population in one of the 38 districts. Let‚Äôs see what happens if we plot popn versus cholera_deaths.\n\nggplot(cholera2, aes(x=popn,y=cholera_deaths)) + \n  geom_point()+\n  geom_smooth(method=\"lm\")+\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nNow the values on the x-axis are numeric and have a meaning to be ordered from the lower to the highest but it is not a trend. Each point is one district population value with some deaths due to cholera. The geom_smooth line it is telling us that if the level of the population is higher, the level of deaths due to cholera is higher, than in other location with a lower level of population.\nBut we can evaluate the growing relationship between population level and numbers of deaths due to cholera.\nLet‚Äôs make a linear model and predict the future, roughly.\n\nmod2 &lt;- lm(cholera_deaths ~ popn , data = cholera2)\nmod2\n\n\nCall:\nlm(formula = cholera_deaths ~ popn, data = cholera2)\n\nCoefficients:\n(Intercept)         popn  \n  1.073e+02    4.357e-03  \n\n\nLet‚Äôs draw this line.\n\nggplot(cholera2, aes(x=popn, y=cholera_deaths)) + \n    geom_point() + \n    geom_abline(slope=0.004357, intercept=107.3,\n                col=\"pink\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\npredictions2 &lt;- predict(mod2, newdata = NULL)\n\nrmse &lt;- sqrt(mean((predictions2 - cholera2$cholera_deaths)^2))\ncat(\"Root Mean Squared Error (RMSE):\", rmse, \"\\n\")\n\nRoot Mean Squared Error (RMSE): 281.7808 \n\n\n\n\n\n\n\n\nWhat if we want to visualize the output of our prediction?\n\n\n\n\nplot &lt;- ggplot(cholera2, aes(x=popn)) + \n  geom_point(aes(y=cholera_deaths)) + \n  geom_smooth(aes(y=cholera_deaths),method = \"lm\",linewidth=2)+\n  geom_line(y=predictions2, col=\"red\")+\n  labs(title=\"Cholera Deaths explanined by\\nLondon Districts Population (1849)\",\n       x=\"1849 London Population by 38 Districts\",\n       y=\"Cholera Deaths\",\n       caption=\"William Farr's Data on Cholera in London, 1849\")+\n  theme_minimal()\nplot\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nCholera Deaths explanined by London Districts Population (1849)\n\n\n\n\nThink about that!\n\n\n\n\n\nImagine that we absolutely want to replicate the geom_smooth(method=\"lm\") line, we would need to consider some steps that the function takes in order to plot the prediction line that doesn‚Äôt much with ours this time. First think to consider is that we haven‚Äôt used new data but just produced the prediction from our dataset. But this is exactly as the same as before.\nThere is a function ggplot_build() that let‚Äôs us dig into the ggplot data manipulation used to make the geom_smooth line.\n?ggplot_build()\n\n# source: https://stackoverflow.com/questions/42673665/geom-smooth-gives-different-fit-than-nls-alone\ndat = ggplot_build(plot)$data[[2]]\n\n`geom_smooth()` using formula = 'y ~ x'\n\ndat &lt;- dat%&gt;%rename(cholera_drate=y,popn=x)\n\nThis time we use the newdata = dat insted of NULL.\n\npredictions3 &lt;- predict(mod2, newdata = dat)\n\nrmse &lt;- sqrt(mean((predictions3 - cholera2$cholera_deaths)^2))\n\nWarning in predictions3 - cholera2$cholera_deaths: longer object length is not\na multiple of shorter object length\n\ncat(\"Root Mean Squared Error (RMSE):\", rmse, \"\\n\")\n\nRoot Mean Squared Error (RMSE): 411.1835 \n\n\n\nggplot(cholera2, aes(x=popn)) + \n  geom_point(aes(y=cholera_deaths)) + \n  geom_smooth(aes(y=cholera_deaths),method = \"lm\",linewidth=2)+\n  geom_line(data=dat,y=predictions3, col=\"red\")+\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nWe eventually matched the geom_smooth line, but why did our predictions on the original data result in that squiggly line?"
  },
  {
    "objectID": "content/stats/statistics/rladies/index.html",
    "href": "content/stats/statistics/rladies/index.html",
    "title": "R-Ladies Events Stats",
    "section": "",
    "text": "Let‚Äôs scrap the R-Ladies chapters events from Meetup.com We can use the {meetupr} package.\nurlname &lt;- c(\"rladies-paris\",\"rladies-rome\")\nevents &lt;- purrr::map(urlname,get_events)\ndat &lt;- dplyr::bind_rows(events)\n\n\n\n\nLoad necessary libraries\n\nsuppressPackageStartupMessages({\n  library(meetupr)\n  library(jsonlite)\n  library(tidyverse)\n  library(stringr)\n  library(tidytext)\n  library(wordcloud)\n  library(topicmodels)\n  library(broom)\n  library(scales)\n})\n\n\ntheme_set(theme_bw())\n\n\n\nurlname &lt;- \"rladies-rome\"\nevents &lt;- get_events(urlname)\ndplyr::arrange(events, desc(time))%&gt;%\n  head()\n\n\nurlname &lt;- c(\"rladies-paris\",\"rladies-rome\")\nevents &lt;- purrr::map(urlname,get_events)\ndat &lt;- dplyr::bind_rows(events)\ndat%&gt;%\n  mutate(link=gsub(\"https://www.meetup.com/\",\"\",link),\n         chapter=stringr::str_extract(link, \"^rladies(.+?)/\"),\n         chapter=gsub(\"/\",\"\",chapter))%&gt;%\n  count(chapter)\n\n\nTo do it for all chapters on meetup, we need the list of the chapters from the rladies github archive.\n\ndata &lt;- jsonlite::fromJSON('https://raw.githubusercontent.com/rladies/meetup_archive/main/data/events.json')\n\n\nchapter &lt;- data %&gt;%\n  count(group_urlname)%&gt;%\n  filter(!str_detect(group_urlname,\"@\"))\nchapters &lt;- chapter$group_urlname\n\nevents &lt;- purrr::map(chapters,get_events)\n# saveRDS(events,\"events.rds\")\n# another way\n# x &lt;- lapply(paths, func)\n# res &lt;- dplyr::bind_rows(x)\n\n\nbind_rows(events[1])%&gt;%head()\n\n\ndat &lt;- dplyr::bind_rows(events)\n# saveRDS(dat,\"dat.rds\")\n\n\ndat1 &lt;- dat%&gt;% \n  mutate(link=gsub(\"https://www.meetup.com/\",\"\",link),\n         chapter=stringr::str_extract(link, \"^rladies(.+?)/\"),\n         chapter=gsub(\"/\",\"\",chapter))%&gt;%\n  relocate(chapter)\n\n# saveRDS(dat1,\"dat1.rds\")\n\n\ndat2 &lt;- dat1%&gt;%\n  select(time,chapter,title,going,venue_city,\n         venue_lon,venue_lat,venue_state,venue_country)%&gt;%\n  mutate(time=as.Date(time))%&gt;%\n  arrange(desc(going))\n\n\ndat2%&gt;%\n  mutate(year=year(time),.after = time)%&gt;%\n  pull(year)%&gt;%\n  summary(year)\n\n\ndat3 &lt;- dat2%&gt;%\n  tidytext::unnest_tokens(word, title,drop = F)%&gt;%\n  select(chapter,title,going,word)%&gt;% \n  anti_join(get_stopwords())%&gt;%\n  filter(!str_length(word)&lt;=3)\n\n\ndat3%&gt;%\n  count(word, sort = TRUE) %&gt;%\n  with(wordcloud::wordcloud(word, n, max.words = 100))\n\n\n\nchapters_dtm &lt;- dat3 %&gt;%\n  count(title, word, sort = TRUE)%&gt;%\n  cast_dtm(title, word, n)\n\nchapters_dtm\n\n\nchapters_lda &lt;- topicmodels::LDA(chapters_dtm, \n                    k = 4, \n                    control = list(seed = 1234))\nchapters_lda_td &lt;- tidy(chapters_lda)\nchapters_lda_td\ntop_terms &lt;- chapters_lda_td %&gt;%\n  group_by(topic) %&gt;%\n  slice_max(beta, n = 5) %&gt;%\n  ungroup() %&gt;%\n  arrange(topic, -beta)\n\ntop_terms\n\n\ntop_terms %&gt;%\n  mutate(term = reorder_within(term, beta, topic)) %&gt;%\n  ggplot(aes(term, beta)) +\n  geom_col() +\n  scale_x_reordered() +\n  facet_wrap(vars(topic), scales = \"free_x\")\n\n\nassignments &lt;- augment(chapters_lda, data = chapters_dtm)\n\nassignments%&gt;%\n  filter(!term==\"ladies\")\n\n# how words in titles changed overtime\ninaug_freq &lt;- dat3 %&gt;%\n  inner_join(dat2,by=c(\"chapter\",\"title\",\"going\"))%&gt;%#View\n  count(time, word) %&gt;%\n  complete(time, word, fill = list(n = 0)) %&gt;%\n  group_by(time) %&gt;%\n  mutate(time_total = sum(n), \n         percent = n / time_total) %&gt;%\n  ungroup()\n\ninaug_freq\n\n\n# library(broom)\nmodels &lt;- inaug_freq %&gt;%\n  group_by(word) %&gt;%\n  filter(sum(n) &gt; 50) %&gt;%\n  group_modify(\n    ~ tidy(glm(cbind(n, time_total - n) ~ time, ., \n               family = \"binomial\"))\n  ) %&gt;%\n  ungroup() %&gt;%\n  filter(term == \"time\")\n\nmodels\nmodels %&gt;%\n  filter(term == \"time\") %&gt;%\n  arrange(desc(abs(estimate)))\n\n\nmodels %&gt;%\n  mutate(adjusted.p.value = p.adjust(p.value)) %&gt;%\n  ggplot(aes(estimate, adjusted.p.value)) +\n  geom_point(shape=\".\") +\n  #scale_y_log10() +\n  geom_text(aes(label = word), \n            #vjust = 1, hjust = 1, \n            check_overlap = TRUE) +\n  labs(x = \"Estimated change over time\", y = \"Adjusted p-value\")\n\n\nmodels %&gt;%\n  slice_max(abs(estimate), n = 6) %&gt;%\n  inner_join(inaug_freq) %&gt;%\n  ggplot(aes(time, percent)) +\n  geom_point() +\n  geom_smooth() +\n  facet_wrap(vars(word)) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  labs(y = \"Frequency of word in speech\")"
  },
  {
    "objectID": "content/stats/statistics/rladies/index.html#overview",
    "href": "content/stats/statistics/rladies/index.html#overview",
    "title": "R-Ladies Events Stats",
    "section": "",
    "text": "Let‚Äôs scrap the R-Ladies chapters events from Meetup.com We can use the {meetupr} package.\nurlname &lt;- c(\"rladies-paris\",\"rladies-rome\")\nevents &lt;- purrr::map(urlname,get_events)\ndat &lt;- dplyr::bind_rows(events)\n\n\n\n\nLoad necessary libraries\n\nsuppressPackageStartupMessages({\n  library(meetupr)\n  library(jsonlite)\n  library(tidyverse)\n  library(stringr)\n  library(tidytext)\n  library(wordcloud)\n  library(topicmodels)\n  library(broom)\n  library(scales)\n})\n\n\ntheme_set(theme_bw())\n\n\n\nurlname &lt;- \"rladies-rome\"\nevents &lt;- get_events(urlname)\ndplyr::arrange(events, desc(time))%&gt;%\n  head()\n\n\nurlname &lt;- c(\"rladies-paris\",\"rladies-rome\")\nevents &lt;- purrr::map(urlname,get_events)\ndat &lt;- dplyr::bind_rows(events)\ndat%&gt;%\n  mutate(link=gsub(\"https://www.meetup.com/\",\"\",link),\n         chapter=stringr::str_extract(link, \"^rladies(.+?)/\"),\n         chapter=gsub(\"/\",\"\",chapter))%&gt;%\n  count(chapter)\n\n\nTo do it for all chapters on meetup, we need the list of the chapters from the rladies github archive.\n\ndata &lt;- jsonlite::fromJSON('https://raw.githubusercontent.com/rladies/meetup_archive/main/data/events.json')\n\n\nchapter &lt;- data %&gt;%\n  count(group_urlname)%&gt;%\n  filter(!str_detect(group_urlname,\"@\"))\nchapters &lt;- chapter$group_urlname\n\nevents &lt;- purrr::map(chapters,get_events)\n# saveRDS(events,\"events.rds\")\n# another way\n# x &lt;- lapply(paths, func)\n# res &lt;- dplyr::bind_rows(x)\n\n\nbind_rows(events[1])%&gt;%head()\n\n\ndat &lt;- dplyr::bind_rows(events)\n# saveRDS(dat,\"dat.rds\")\n\n\ndat1 &lt;- dat%&gt;% \n  mutate(link=gsub(\"https://www.meetup.com/\",\"\",link),\n         chapter=stringr::str_extract(link, \"^rladies(.+?)/\"),\n         chapter=gsub(\"/\",\"\",chapter))%&gt;%\n  relocate(chapter)\n\n# saveRDS(dat1,\"dat1.rds\")\n\n\ndat2 &lt;- dat1%&gt;%\n  select(time,chapter,title,going,venue_city,\n         venue_lon,venue_lat,venue_state,venue_country)%&gt;%\n  mutate(time=as.Date(time))%&gt;%\n  arrange(desc(going))\n\n\ndat2%&gt;%\n  mutate(year=year(time),.after = time)%&gt;%\n  pull(year)%&gt;%\n  summary(year)\n\n\ndat3 &lt;- dat2%&gt;%\n  tidytext::unnest_tokens(word, title,drop = F)%&gt;%\n  select(chapter,title,going,word)%&gt;% \n  anti_join(get_stopwords())%&gt;%\n  filter(!str_length(word)&lt;=3)\n\n\ndat3%&gt;%\n  count(word, sort = TRUE) %&gt;%\n  with(wordcloud::wordcloud(word, n, max.words = 100))\n\n\n\nchapters_dtm &lt;- dat3 %&gt;%\n  count(title, word, sort = TRUE)%&gt;%\n  cast_dtm(title, word, n)\n\nchapters_dtm\n\n\nchapters_lda &lt;- topicmodels::LDA(chapters_dtm, \n                    k = 4, \n                    control = list(seed = 1234))\nchapters_lda_td &lt;- tidy(chapters_lda)\nchapters_lda_td\ntop_terms &lt;- chapters_lda_td %&gt;%\n  group_by(topic) %&gt;%\n  slice_max(beta, n = 5) %&gt;%\n  ungroup() %&gt;%\n  arrange(topic, -beta)\n\ntop_terms\n\n\ntop_terms %&gt;%\n  mutate(term = reorder_within(term, beta, topic)) %&gt;%\n  ggplot(aes(term, beta)) +\n  geom_col() +\n  scale_x_reordered() +\n  facet_wrap(vars(topic), scales = \"free_x\")\n\n\nassignments &lt;- augment(chapters_lda, data = chapters_dtm)\n\nassignments%&gt;%\n  filter(!term==\"ladies\")\n\n# how words in titles changed overtime\ninaug_freq &lt;- dat3 %&gt;%\n  inner_join(dat2,by=c(\"chapter\",\"title\",\"going\"))%&gt;%#View\n  count(time, word) %&gt;%\n  complete(time, word, fill = list(n = 0)) %&gt;%\n  group_by(time) %&gt;%\n  mutate(time_total = sum(n), \n         percent = n / time_total) %&gt;%\n  ungroup()\n\ninaug_freq\n\n\n# library(broom)\nmodels &lt;- inaug_freq %&gt;%\n  group_by(word) %&gt;%\n  filter(sum(n) &gt; 50) %&gt;%\n  group_modify(\n    ~ tidy(glm(cbind(n, time_total - n) ~ time, ., \n               family = \"binomial\"))\n  ) %&gt;%\n  ungroup() %&gt;%\n  filter(term == \"time\")\n\nmodels\nmodels %&gt;%\n  filter(term == \"time\") %&gt;%\n  arrange(desc(abs(estimate)))\n\n\nmodels %&gt;%\n  mutate(adjusted.p.value = p.adjust(p.value)) %&gt;%\n  ggplot(aes(estimate, adjusted.p.value)) +\n  geom_point(shape=\".\") +\n  #scale_y_log10() +\n  geom_text(aes(label = word), \n            #vjust = 1, hjust = 1, \n            check_overlap = TRUE) +\n  labs(x = \"Estimated change over time\", y = \"Adjusted p-value\")\n\n\nmodels %&gt;%\n  slice_max(abs(estimate), n = 6) %&gt;%\n  inner_join(inaug_freq) %&gt;%\n  ggplot(aes(time, percent)) +\n  geom_point() +\n  geom_smooth() +\n  facet_wrap(vars(word)) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  labs(y = \"Frequency of word in speech\")"
  },
  {
    "objectID": "content/stats/statistics/bayesian_statistics/index.html",
    "href": "content/stats/statistics/bayesian_statistics/index.html",
    "title": "Bayesian Statistics model comparison",
    "section": "",
    "text": "In this post I‚Äôll go through some differences between Bayesian statistical packages in R. Bayesian statistics involves probabilities. This means that the probability of an event to occur is considered in the modeling procedure, and is mainly used in for making inferences, and can be used for an analysis of the speculation of the root cause of a phenomenon under the term of causal inference.\n\n\n\n\nIn more details, when Bayesian statistics is performed, the response variable is tested against (causal) predictors with the application of suited prior distributions, and the use of the likelihood function, to finally produce a posterior distribution which should be as much as possible close to the real future outcome of the response variable distribution.\nThe prior distribution is the starting point; it is the probability distribution on which the future outcome is linked to, such as the probability to have a Girl given the probability to have had a Boy.\n\\[P( \\text{ Girl } | \\text{ Boy })\\]\nThe probability to have had a Boy is the prior, while the conditional probability to have a Girl is the posterior.\nBriefly, here is a comparison between different R packages that use Bayesian inference for the calculation of the model probability distribution of the posterior.\nThe Stan model engine, for model replication and prediction is used in conjunction with the Montecarlo simulation technique for the best model solution. The Stan model engine is applied in the following packages:\n\nbrms\nrstanarm\nrethinking\nMCMCglmm\n\nAll of these packages adapt and adjust different model options for a modeling procedure which is the result of the best combination of efficiency to increase productivity and effectiveness, to identify and remove unnecessary steps, automate repetitive tasks, and utilize the most suitable software tools.\nThis is the original source code that I have updated: https://www.jstatsoft.org/article/view/v080i01\n\nA wide range of distributions and link functions are supported, allowing users to fit - among others - linear, robust linear, binomial, Poisson, survival, ordinal, zero-inflated, hurdle, and even non-linear models all in a multilevel context. (The Brms package)\n\nLoading required packages\n\nlibrary(tidyverse)\nlibrary(\"brms\") \nlibrary(\"rstanarm\")\nlibrary(\"rethinking\") \nlibrary(\"MCMCglmm\")  \n\nHelper function to better compute the effective sample size\n\neff_size &lt;- function(x) {\n  if (is(x, \"brmsfit\")) {\n    samples &lt;- as.data.frame(x$fit)\n  } else if (is(x, \"stanreg\")) {\n    samples &lt;- as.data.frame(x$stanfit)\n  } else if (is(x, \"ulam\")) {\n    samples &lt;- as.data.frame(x@stanfit)\n  } else if (is(x, \"stanfit\")) {\n    samples &lt;- as.data.frame(x)\n  } else if (is(x, \"MCMCglmm\")) {\n    samples &lt;- cbind(x$Sol, x$VCV)\n  } else {\n    stop(\"invalid input\")\n  }\n  # call an internal function of rstan\n  floor(apply(samples, MARGIN = 2, FUN = rstan:::ess_rfun))\n}\n\n\n\n# only used for Stan packages\niter &lt;- 6000  \nwarmup &lt;- 1000\nchains &lt;- 1\nadapt_delta &lt;- 0.8\n\n# only used for MCMCglmm\nnitt &lt;- 35000  \nburnin &lt;- 10000\nthin &lt;- 5\n# leads to 5000 posterior samples\n\n\n\n\nprior_dye_brms &lt;- c(set_prior(\"normal(0, 2000)\", class = \"Intercept\"),\n                    set_prior(\"cauchy(0, 50)\", class = \"sd\"),\n                    set_prior(\"cauchy(0, 50)\", class = \"sigma\"))\n\ndye_brms &lt;- brm(Yield ~ 1 + (1 | Batch), \n                data = lme4::Dyestuff, \n                prior = prior_dye_brms, \n                chains = 0)\n\ntime_dye_brms &lt;- system.time(capture.output(\n  dye_brms &lt;- update(dye_brms, \n                     iter = iter, \n                     warmup = warmup, \n                     chains = chains,\n                     control = list(adapt_delta = adapt_delta))\n))\n# summary(dye_brms)\neff_dye_brms &lt;- min(eff_size(dye_brms)) / time_dye_brms[[1]]\n\n\n\ntime_dye_rstanarm &lt;- system.time(capture.output(\n  dye_rstanarm &lt;- stan_glmer(Yield ~ 1 + (1 | Batch), data = lme4::Dyestuff,\n                             prior_intercept = normal(0, 2000),\n                             iter = iter, warmup = warmup, chains = chains,\n                             adapt_delta = adapt_delta)\n))\n# summary(dye_rstanarm)\neff_dye_rstanarm &lt;- min(eff_size(dye_rstanarm)) / time_dye_rstanarm[[1]]\n\n\n\nd &lt;-  lme4::Dyestuff\n\ndat &lt;- list(\n  Yield = d$Yield,\n  Batch = d$Batch\n)\n\ndye_flist &lt;- alist(\n  Yield ~ dnorm(eta, sigma),\n  eta &lt;- a + a_Batch[Batch],\n  a ~ dnorm(0,2000),\n  a_Batch[Batch] ~ dnorm(0, sd_Batch),\n  sigma ~ dcauchy(0, 50),\n  sd_Batch ~ dcauchy(0, 50))\n\ndye_rethinking &lt;- ulam(dye_flist, \n                       data = dat, \n                       chains=1,\n                       cores = 4,\n                       sample = TRUE)\n\ntime_dye_rethinking &lt;- system.time(capture.output(\n  dye_rethinking &lt;- update(dye_rethinking,\n       iter = iter, \n       warmup = warmup, \n       chains = chains,\n       control = list(adapt_delta = adapt_delta))\n))\n\n\n# summary(dye_rethinking)\neff_dye_rethinking &lt;- min(eff_size(dye_rethinking)) / time_dye_rethinking[[1]]\n\n\n\ntime_dye_MCMCglmm &lt;- system.time(capture.output(\n  dye_MCMCglmm &lt;- MCMCglmm(Yield ~ 1, \n                           random = ~ Batch, data = lme4::Dyestuff, \n                           thin = thin, nitt = nitt, burnin = burnin)\n))\n# summary(dye_MCMCglmm)\neff_dye_MCMCglmm &lt;- min(eff_size(dye_MCMCglmm)) / time_dye_MCMCglmm[[1]]\n\n\n\nprint(c(brms = eff_dye_brms, \n        rstanarm = eff_dye_rstanarm, \n        rethinking = eff_dye_rethinking, \n        MCMCglmm = eff_dye_MCMCglmm))\n\n\n\nbrms\nrstanarm\nrethinking\nMCMCglmm\n\n\n559.55398\n202.97177\n3660.71429\n34.11514"
  },
  {
    "objectID": "content/stats/statistics/bayesian_statistics/index.html#dyestuff",
    "href": "content/stats/statistics/bayesian_statistics/index.html#dyestuff",
    "title": "Bayesian Statistics model comparison",
    "section": "",
    "text": "prior_dye_brms &lt;- c(set_prior(\"normal(0, 2000)\", class = \"Intercept\"),\n                    set_prior(\"cauchy(0, 50)\", class = \"sd\"),\n                    set_prior(\"cauchy(0, 50)\", class = \"sigma\"))\n\ndye_brms &lt;- brm(Yield ~ 1 + (1 | Batch), \n                data = lme4::Dyestuff, \n                prior = prior_dye_brms, \n                chains = 0)\n\ntime_dye_brms &lt;- system.time(capture.output(\n  dye_brms &lt;- update(dye_brms, \n                     iter = iter, \n                     warmup = warmup, \n                     chains = chains,\n                     control = list(adapt_delta = adapt_delta))\n))\n# summary(dye_brms)\neff_dye_brms &lt;- min(eff_size(dye_brms)) / time_dye_brms[[1]]\n\n\n\ntime_dye_rstanarm &lt;- system.time(capture.output(\n  dye_rstanarm &lt;- stan_glmer(Yield ~ 1 + (1 | Batch), data = lme4::Dyestuff,\n                             prior_intercept = normal(0, 2000),\n                             iter = iter, warmup = warmup, chains = chains,\n                             adapt_delta = adapt_delta)\n))\n# summary(dye_rstanarm)\neff_dye_rstanarm &lt;- min(eff_size(dye_rstanarm)) / time_dye_rstanarm[[1]]\n\n\n\nd &lt;-  lme4::Dyestuff\n\ndat &lt;- list(\n  Yield = d$Yield,\n  Batch = d$Batch\n)\n\ndye_flist &lt;- alist(\n  Yield ~ dnorm(eta, sigma),\n  eta &lt;- a + a_Batch[Batch],\n  a ~ dnorm(0,2000),\n  a_Batch[Batch] ~ dnorm(0, sd_Batch),\n  sigma ~ dcauchy(0, 50),\n  sd_Batch ~ dcauchy(0, 50))\n\ndye_rethinking &lt;- ulam(dye_flist, \n                       data = dat, \n                       chains=1,\n                       cores = 4,\n                       sample = TRUE)\n\ntime_dye_rethinking &lt;- system.time(capture.output(\n  dye_rethinking &lt;- update(dye_rethinking,\n       iter = iter, \n       warmup = warmup, \n       chains = chains,\n       control = list(adapt_delta = adapt_delta))\n))\n\n\n# summary(dye_rethinking)\neff_dye_rethinking &lt;- min(eff_size(dye_rethinking)) / time_dye_rethinking[[1]]\n\n\n\ntime_dye_MCMCglmm &lt;- system.time(capture.output(\n  dye_MCMCglmm &lt;- MCMCglmm(Yield ~ 1, \n                           random = ~ Batch, data = lme4::Dyestuff, \n                           thin = thin, nitt = nitt, burnin = burnin)\n))\n# summary(dye_MCMCglmm)\neff_dye_MCMCglmm &lt;- min(eff_size(dye_MCMCglmm)) / time_dye_MCMCglmm[[1]]\n\n\n\nprint(c(brms = eff_dye_brms, \n        rstanarm = eff_dye_rstanarm, \n        rethinking = eff_dye_rethinking, \n        MCMCglmm = eff_dye_MCMCglmm))\n\n\n\nbrms\nrstanarm\nrethinking\nMCMCglmm\n\n\n559.55398\n202.97177\n3660.71429\n34.11514"
  },
  {
    "objectID": "content/stats/statistics/simpson_paradox/index.html",
    "href": "content/stats/statistics/simpson_paradox/index.html",
    "title": "Understanding Simpson‚Äôs Paradox: A Simple Explanation",
    "section": "",
    "text": "Simpson's Paradox is a statistical phenomenon where a trend appears in different groups of data but disappears or reverses when these groups are combined. This paradox highlights the importance of considering confounding variables and understanding the causal relationship between variables.\nExample Scenario: Work Environment\nLet‚Äôs consider a hypothetical work environment where the number of women (W) is greater than the number of men (M). However, when looking at the distribution of managerial positions (P), it seems that more men occupy higher-level positions compared to women.\nNow, suppose there‚Äôs a characteristic Z, representing gender, and you suspect it might influence the choice of assigning a managerial position (P) because a specific time dedicated to a critical task (T) is primarily marketed toward men (M).\nTo illustrate this paradox, we‚Äôll create synthetic data in R.\nInstall and load necessary library\n\nlibrary(dplyr)\n\nSet seed for reproducibility\n\nset.seed(123)\n\nGenerate synthetic data\n\nn &lt;- 1000  # Number of employees \nW &lt;- round(runif(n, 200, 800))  # Number of women\nM &lt;- n - W  # Number of men\n\nAssign managerial positions based on gender and a confounding variable\n\nP_W &lt;- round(runif(W, 0, 1))  # 0 for no managerial position, 1 for managerial position for women\nP_M &lt;- round(runif(M, 0.2, 1))  # Higher chance of managerial position for men due to confounding variable\n\nCreate a data frame\n\nw &lt;- tibble(gender=\"Women\",count=W,manager=P_W)\n\nm &lt;- tibble(gender=\"Men\",count=M,manager=P_M)\n\ndata &lt;- rbind(w,m)\ndata%&gt;%head\n\n# A tibble: 6 √ó 3\n  gender count manager\n  &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 Women    373       0\n2 Women    673       1\n3 Women    445       0\n4 Women    730       1\n5 Women    764       1\n6 Women    227       0\n\n\nDisplay the initial summary\n\nsummary(data)\n\n    gender              count        manager      \n Length:2000        Min.   :200   Min.   :0.0000  \n Class :character   1st Qu.:352   1st Qu.:0.0000  \n Mode  :character   Median :500   Median :1.0000  \n                    Mean   :500   Mean   :0.5615  \n                    3rd Qu.:648   3rd Qu.:1.0000  \n                    Max.   :800   Max.   :1.0000  \n\n\nIn this example, we have created a dataset with a larger number of women, but the chance of obtaining a managerial position for men is influenced by a confounding variable. Now, let‚Äôs examine the paradox.\nCalculate the proportion of managerial positions for each gender\n\nproportion_table &lt;- data %&gt;%\n  group_by(gender) %&gt;%\n  summarize(proportion = mean(manager))\n\nDisplay the proportions\n\nproportion_table\n\n# A tibble: 2 √ó 2\n  gender proportion\n  &lt;chr&gt;       &lt;dbl&gt;\n1 Men         0.626\n2 Women       0.497\n\n\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.4.1\n\nproportion_table%&gt;%\n  ggplot(aes(gender,proportion,fill=gender))+\n  geom_col(color=\"white\",show.legend = F)+\n  scale_fill_viridis_d()+\n  labs(title = \"Proportion of Managers by Gender\",\n       subtitle = \"Example of the Simpson's Paradox\",\n       x=\"\",\n       caption = \"Data: Syntetic | Graphics: Federica Gazzelloni\") +\n  coord_equal()+\n  ggthemes::theme_pander()+\n  theme(plot.caption = element_text(hjust = 0.5))\n\n\n\n\n\n\n\nIn this scenario, when examining the proportion of managerial positions within each gender group, it might appear that men have a higher chance. However, when we consider the entire dataset, we may find the opposite due to the confounding variable.\nThe key takeaway is that understanding causation is crucial, and Simpson‚Äôs Paradox emphasizes the need to consider confounding factors when interpreting data.\n\n\n Back to top"
  },
  {
    "objectID": "about/workshops/index.html",
    "href": "about/workshops/index.html",
    "title": "Federica Gazzelloni's Website",
    "section": "",
    "text": "CDC Centers for Disease Control\nData Analysis and Visualization with R\nDate: 2024-08-19 to 2024-08-20 | Centers for Disease Control\nWebsite: https://rrlove-cdc.github.io/2024-08-19-cdc-online/\nEtherPad: https://pad.carpentries.org/2024-08-19-cdc-online\n\nData Carpentry Genomics\nDate: 2024-06-10 to 2024-06-13 | Centers for Disease Control\nWebsite: https://fgazzelloni.github.io/2024-06-10-cdc-online/\nEtherPad: https://pad.carpentries.org/2024-06-10-cdc-online\n\n\n\nUniversity of Washington\nNetwork of the National Library of Medicine (NNLM) Region 5 Library Carpentry\nDate: March 19-21 2024 | Network of the National Library of Medicine (NNLM)\nWebsite: https://nnlm-ncds.github.io/2024-03-19-nnlm-uw-online/\nEtherPad: https://pad.carpentries.org/2024-03-19-nnlm-uw-online\n\n\n\nHelmholtz Information & Data Science Academy\nSoftware Carpentry (Shell, Git, and programming with R)\nDate: February 12-13 2024 | Helmholtz Online\nWebsite: https://macrobiotus.github.io/2024-02-12-helmholtz-online/\nEtherPad: https://pad.carpentries.org/2024-02-12-helmholtz-online\n\n\n\n\n Back to top"
  },
  {
    "objectID": "about/rladies/index.html",
    "href": "about/rladies/index.html",
    "title": "Federica Gazzelloni's Website",
    "section": "",
    "text": "Sharing the mission with the open source community\n\n\n\nR-Ladies DC\nDate: September 2022\nAbstract: This tutorial is meant for people new to spatial analysis and modeling with RStudio but comfortable in making simple data visualization with ggplot2. In this video, you will learn how to make a map with RStudio, and how to use data modeling for making spatial model analysis.\nMaterial: It is helpful to have the following R packages installed beforehand: {tidyverse}, {ggthemes}, {maptools}, {ggmap}, {sf}, {spocc}, {dismo}, {SpatialEpi}, and {oregonfrogs} dataset from\nremotes::install_github(\"fgazzelloni/oregonfrogs\")\n\nGitHub Repo: https://github.com/Fgazzelloni/How-to-Spatial-Modeling-with-R\nBook: https://fgazzelloni.github.io/How-to-Spatial-Modeling-with-R\n\n\n\n\n\nR-Ladies NYC & R-Ladies Rome\nDate: April 2023\nAbstract: In this video, you will learn about Modeling infectious diseases with R using both deterministic and Bayesian SIR model methods. We will explore both Deterministic and Bayesian SIR model methods, and learn how to use the well-known SIR model to understand how epidemics unfold and how to prevent their spread.\nThis video is perfect for students of science, healthcare professionals, or anyone who wants to gain a deeper understanding of how to use R for Modeling infectious diseases.\nAgenda:\n\nIntroduction presentation of the Chapters and R-Ladies Global action\nSIR model with R - quick intro assessment (Speaker Federica Gazzelloni)\nBayesian workflow for disease transmission (Speaker Jacqueline Buros)\nQ&A session\n\nMaterial:\n\nGitHub Repo: https://github.com/Fgazzelloni/sir-model-with-R/\n\n\n\n\n\nR-Ladies Cambridge\nDate: March 2024\nAbstract: In this video, you‚Äôll learn how to use {ggplot2} to replicate one of the ongoing #DuboisChallenge2024 plates. The original plates are part of W.E.B. Du Bois‚Äôs legacy from back in 1900, showcased at the Paris Exposition. I‚Äôll be using modern tools such as R. We‚Äôll explore the colors used in the plates and delve into the intricacies of the challenge by understanding the perspective of hand-made graphs in the ‚ÄòDuboisian‚Äô style.\nMaterial:\n\nGitHub Repo: https://github.com/Fgazzelloni/R-Ladies-Cambridge-Dataviz-lunch-Replicating-Du-Bois-with-R\nWebSite Collection: https://fgazzelloni.quarto.pub/unlocking-the-power-of-data-visualization-with-r/duboischallenge/\n\n\n\n\n\nR-Ladies NYC & R-Ladies Rome\nDate: June 2024\nAbstract: In this video, you‚Äôll learn how to make a website in R with Quarto. We‚Äôll explore the basics of Quarto and how to publish a website. The tutorial is designed to be accessible to beginners, with no prior experience required. By the end of the session, you will have a fully functional website that you can use to showcase your projects and achievements.\nMaterial:\n\nGithub repository: https://github.com/Fgazzelloni/building_a_website_in_r\nFinal version of the Website: https://fgazzelloni.quarto.pub/my-website-in-r‚Äìquarto/\n\n\nResources:\n\nQuarto Docs: https://quarto.org/docs/websites/\nCSS customization for your website: https://www.w3schools.com/css/\nBackground image (change - ‚Äúpink‚Äù with your favorite pick): https://www.pexels.com/search/pink/\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "about/profile/index.html",
    "href": "about/profile/index.html",
    "title": "Federica Gazzelloni's Website",
    "section": "",
    "text": "STATISTICIAN | ACTUARY | DATA SCIENTIST\n‚ÄúI‚Äôve always loved maths and science, I meant to be a Veterinary, I wanted to do ballet classes, teach gymnastic, be an economist, a psychologist‚Ä¶ Many years passed since I started the journey of my career as an investigator, it turned out to be an everyday challenge with tasks to solve, and most importantly a continuous learning path. Each day brings with it a fresh set of challenges to solve and opportunities for growth. It‚Äôs a path marked by curiosity, resilience, and a relentless pursuit of excellence‚Äîa journey that I‚Äôm grateful to be on.‚Äù\nFederica Gazzelloni is an Actuary, and a Data Scientist with a focus on health metrics, machine learning, and data visualization. With experience spanning corporate, academic, and research roles, Federica has developed a robust skill set that bridges actuarial science, statistical modeling, and public health.\nFederica began her career as an actuary, working in corporate and academic settings where she focused on quantitative analysis, risk modeling, and compliance in the insurance and pensions industry. As a research-oriented actuary, she not only applied advanced actuarial principles but also developed a deep understanding of the statistical methods. Federica taught mathematics to high school students and instructed university students in computer science, helping to cultivate the next generation of data-driven professionals.\nIn recent years, Federica has expanded her focus to health data modeling, particularly in the context of infectious disease research. As the world faced the Covid-19 pandemic, her expertise in statistical methods and health metrics became even more relevant. Collaborating with the Institute for Health Metrics and Evaluation (IHME), Federica contributed to global health studies that aimed to understand the spread and impact of Covid-19, as well as other pressing health issues. This experience inspired her to create a practical manual for health data analysis, which serves as a resource for professionals and students alike who wish to explore health metrics and epidemiological modeling in-depth. Her book, to be published by CRC Press, combines foundational knowledge with practical applications, including R code for real-world case studies in health metrics.\nFederica‚Äôs role in the pre-publication stages of GBD research enables her to directly contribute to some of the most impactful global health research being conducted today. This work not only requires technical acumen in statistical modeling but also a keen eye for detail and a commitment to accuracy, given the far-reaching implications of public health data.\nBeyond her work in health metrics, Federica is an active member of the open-source community, where she contributes to several organizations dedicated to education and software development. She is a Certified Carpentries Instructor, delivering workshops that empower learners with foundational data science skills in R, Python, and other tools. Federica has taught workshops for prestigious organizations, including the Helmholtz Information & Data Science Academy, the University of Washington, and the Centers for Disease Control and Prevention (CDC), where she introduced participants to key concepts in programming, data wrangling, and statistical analysis. Her teaching style is characterized by clarity, engagement, and inclusivity, making technical content accessible to learners of all backgrounds and skill levels.\nFederica also serves as the Lead Organizer for R-Ladies Rome, a chapter of the global R-Ladies organization that promotes gender diversity in the R programming and data science communities. Since its founding in 2023, R-Ladies Rome has grown significantly under her leadership, reaching a large and engaged audience through online and in-person events. Through R-Ladies, Federica has organized a range of activities, from tutorials on data visualization to workshops on advanced R packages, all aimed at fostering a supportive and inclusive community for women and underrepresented groups in data science. Notable events include a session with Hadley Wickham, Chief Scientist at Posit PBC, and an upcoming workshop on building reproducible data pipelines, which reflects the group‚Äôs commitment to high-quality, practical learning opportunities.\nIn addition to her work with R-Ladies Rome, Federica has collaborated with other R user groups globally, including R-Ladies New York, R-Ladies Paris, and the TunisR User Group. These partnerships have expanded the reach of her initiatives, creating a more interconnected and supportive global R community. Federica is also involved with Bioconductor and the R Consortium, furthering her commitment to open-source development and collaborative learning.\nOne of Federica‚Äôs key strengths is her ability to use data visualization as a tool for communication and insight. Her background in data visualization allows her to create compelling visual representations of complex health data, making it easier for diverse audiences to grasp intricate statistical relationships. Whether through static graphs or interactive dashboards, Federica‚Äôs visualizations aim to tell a story, uncover trends, and empower decision-makers with actionable insights.\nThroughout her career, Federica has consistently demonstrated a commitment to knowledge sharing and community building. Her work is informed by a belief in the power of data to drive positive change, especially in fields like public health where informed decision-making can save lives. Her contributions to education, open-source software, and health metrics research underscore her dedication to making data science an inclusive and impactful field.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\n",
    "section": "",
    "text": "Data Science Hub\n    \nThis is a dedicated platform for sharing ideas, insights, resources, and projects made in R and Python programming languages. With expertise in advanced statistical methods, data modeling, and public health analytics, I focus on topics like health metrics and the dynamics of infectious diseases.\n    \n    Buy my Book"
  },
  {
    "objectID": "index.html#news-hmsid-book-is-available-online-and-for-pre-order",
    "href": "index.html#news-hmsid-book-is-available-online-and-for-pre-order",
    "title": "\n",
    "section": "News: HMSID Book Is Available ONLINE and for PRE-ORDER!",
    "text": "News: HMSID Book Is Available ONLINE and for PRE-ORDER!\n\nGazzelloni, Federica. Health Metrics and the Spread of Infectious Diseases: Machine Learning Applications and Spatial Modelling Analysis with R. First edition. Boca Raton, FL: CRC Press, 2025. Print.\n\n\nüéâ Explore the Online and Printed Versions\nüìñ Read it online for free:\n\nüîó bookdown.org/fede_gazzelloni/hmsidR\nüîó fgazzelloni.quarto.pub/hmsidr\n\nüõí Pre-Order your printed copy:\n\nReserve it today from:üëâ Routledge Website or üëâ Amazon\n\n\n\n\n\n\n\n\nIt‚Äôs been an incredible journey, and now that the book is officially printed and available for pre-order, I feel a mix of emotions‚Äîboth the excitement of sharing it with you and a touch of nostalgia as this creative process comes to a close. üòå\n\nWriting this book has been an immensely rewarding experience. I‚Äôve spent countless hours selecting the right material and striving to present it in a way that‚Äôs accessible and helpful for early-career researchers and students. One of the biggest challenges was deciding what to include and what to leave out, constantly refining the content to ensure it‚Äôs both comprehensive and easy to follow.\nEven though the book is now published, my mind is still buzzing with ideas. There‚Äôs always that lingering thought of what else could be added to make it even better. But at some point, you have to step back and trust that what you‚Äôve created will serve its purpose. And now, it‚Äôs time to see how it will resonate with you, the readers.\n\nAs I reflect on this journey, I‚Äôm filled with gratitude for all the support and feedback I‚Äôve received along the way. Your encouragement has been invaluable in shaping this book into what it is today.\n\nSo, what‚Äôs next? While this project may be wrapping up, my passion for writing and sharing knowledge is far from over. Stay tuned for more updates and future projects! In the meantime, I invite you to dive into the book, explore its content, and share your thoughts. Your insights will be crucial as I continue to learn and grow.\nThank you for being a part of this journey. I hope the book serves as a valuable resource for your research and studies. Let‚Äôs see where this new chapter takes us!\nHappy reading! üìö‚ú®\n\n\n\n\n  SUBSCRIBE\n    Subscribe for the latest tutorials and insights in Data Science and R Programming!\n    \n    \n    \n     \n\n\n\nLatest content"
  }
]