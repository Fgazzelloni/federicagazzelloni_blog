{
  "hash": "d451042fe120da10325f97a72874bbbb",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Understanding Plain Vanilla - from Scratch\"\ndate: '2024-06-28'\nsummary: \"This is a simple introduction to Artificial Neural Networks (ANN) from scratch using R outside the library boxes | used in Ch10 ISLR Book-Club.\"\nimage: featured.png\nslug: ann-plain-vanilla\ncategories:\n  - rstats\n  - modeling\n  - machine-learning\n  - artificial-neural-networks\nmargin-header: |\n     ![By giphy.com](https://i.giphy.com/media/KX5nwoDX97AtPvKBF6/giphy.gif)\n---\n\n\n## Overview\n\nIn the world of artificial intelligence, understanding the fundamentals of neural networks is essential. One of the simplest yet powerful architectures is the **Artificial Neural Network (ANN) with a single hidden layer, often referred to as a \"plain vanilla\" network**. This blog post aims to provide a comprehensive guide to building such a network from scratch, focusing on key concepts like activation functions, weights, and the backpropagation algorithm outside the library boxes.\n\n## Example of construction of Plain vanilla network architecture\n\nANN's (Artificial Neural Networks) is the simplest implementation of deep learning model architectures with only `one hidden layer`.\n\n<center>![](images/10_02_single-layer.png){width=\"400\"}</center>\n\nIn the image above, we have some predictors $x_i$ that are fed into the hidden layer, which then transforms them into a new set of features $h_k(x)$, which are then used to predict the response variable $y$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrm(list=ls())\nsuppressMessages(library(tidyverse))\ntheme_set(theme_minimal())\n```\n:::\n\n\n### Build synthetic data:\n\n-   `Predictors` as `Uniform` distributed variables ranging between `[-2, 2]`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(100)\nx <- runif(60, min=-2, max=2)\n```\n:::\n\n\n-   `Response` Variable as function of the predictors:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny <- function(x) {\n  Y = (cos(2*x + 1))\n  return(Y)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- tibble(y=y(x),x)\nhead(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 2\n       y       x\n   <dbl>   <dbl>\n1  0.859 -0.769 \n2  0.591 -0.969 \n3  0.152  0.209 \n4 -0.829 -1.77  \n5  0.733 -0.126 \n6  0.645 -0.0649\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndata %>% summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       y                  x           \n Min.   :-0.99978   Min.   :-1.77447  \n 1st Qu.:-0.59934   1st Qu.:-0.89194  \n Median : 0.15559   Median :-0.04092  \n Mean   : 0.02628   Mean   : 0.01437  \n 3rd Qu.: 0.62358   3rd Qu.: 0.88138  \n Max.   : 0.99930   Max.   : 1.95826  \n```\n\n\n:::\n:::\n\n\n### EDA - Exploratory Data Analysis\n\nLet's visualize our synthetic data:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata %>%\n  ggplot(aes(x, y)) +\n  geom_line() +\n  geom_point() +\n  geom_hline(yintercept=0, \n             linetype=\"dashed\", color=\"grey\")\n```\n\n::: {.cell-output-display}\n![Synthetic Data](index_files/figure-html/unnamed-chunk-6-1.png){fig-align='center' width=576}\n:::\n:::\n\n\n### Parameters estimation\n\nNow that we have the data we attempt to replicate the distribution of this data with a model using artificial neural network technique with a single hidden layer. \n\nFormula of the model: $$f(X)=\\beta_0+\\sum_{k=1}^K{\\beta_kh_k(X)}$$\n\nWhere, $h_k$ is the hidden layer, $k=1,...,K$ the number of activations, $\\beta_0,\\beta_1,...,\\beta_K$ the coefficients , and $w_{10},...,w_{kp}$ the weights.\n\nThe hidden layer computes a number of activations.\n\n#### Number of activations\n\nInitialize the number of hidden neurons $k$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhidden_neurons = 5\n```\n:::\n\n\n#### Activation Function\n\nIt is a non-linear transformation of the input layers $X_1,X_2,...,X_p$ which transform to $h_k(X)$ while learning during the training of the network.\n\n$$A_k=h_k(X)=g(z)$$\n\n$g(z)$ is a function used in `logistic regression` to **convert** a `linear function` **into probabilities** between zero and one.\n\n#### Type of Activation functions\n\n-   `Sigmoid` function\n\n$$g(z)=\\frac{e^z}{1+e^z}=\\frac{1}{1+ e^{-z}}$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsigmoid <- function(x) {\n  z = (1 / (1 + exp(-x)))\n  return(z)\n}\n```\n:::\n\n\n-   `ReLU` (Rectified Linear Unit) function\n\n$$g(z) = max(0, z)$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrelu <- function(x) {\n  z = ifelse(x < 0, 0, x)\n  return(z)\n}\n```\n:::\n\n\n\n-   `SoftPlus` Function\n\n$$g(z) = \\log(1 + e^z)$$\n\nA smooth approximation to the ReLU function. It is firstly introduced in 2001. `Softplus` is an alternative to traditional functions because it is differentiable and its derivative is easy to demonstrate. source: <https://sefiks.com/2017/08/11/softplus-as-a-neural-networks-activation-function/>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsoftplus <- function(x) {\n  z = log(1 + exp(x))\n  return(z)\n}\n```\n:::\n\n\n-   Other types such as:\n    -   $x^2$, or polynomials/splines\n    -   Hyperbolic tanh $tanh(x) = (e^x – e^-x) / (e^x + e^-x)$\n\nLet's compare the `activation functions`:\n\n::: {.cell layout-ncol=\"3\"}\n\n```{.r .cell-code}\ndata %>%\n  mutate(z=sigmoid(x)) %>%\n  ggplot() +\n  geom_line(aes(x, z)) +\n  ylim(0, 1)\n```\n\n::: {.cell-output-display}\n![Sigmoid function](index_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n\n```{.r .cell-code}\ndata %>%\n  ggplot() +\n  geom_line(aes(x, relu(x) * 1/2.4))\n```\n\n::: {.cell-output-display}\n![ReLU function](index_files/figure-html/unnamed-chunk-11-2.png){width=672}\n:::\n\n```{.r .cell-code}\ndata %>%\n  ggplot() +\n  geom_line(aes(x, softplus(x)))\n```\n\n::: {.cell-output-display}\n![SoftPlus function](index_files/figure-html/unnamed-chunk-11-3.png){width=672}\n:::\n:::\n\n\n::::{.columns}\n\n:::{.column}\n\nAnd now look at `Sigmoid vs ReLU` functions:\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata %>%\n  ggplot() +\n  geom_line(aes(x, sigmoid(x))) +\n  # relu resized for comparison\n  geom_line(aes(x, relu(x) * 1/2.4))+\n  labs(y=\"Sigmoid vs ReLU\")\n```\n:::\n\n\n:::\n\n:::{.column}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Sigmoid vs ReLU](index_files/figure-html/unnamed-chunk-13-1.png){fig-align='center' width=576}\n:::\n:::\n\n\n:::\n\n::::\n\n\n**Our model is a model in the model:**\n\n$$f(X)=\\beta_0+\\sum_{k=1}^K{\\beta_kh_k(X)}$$ $$A_k=h_k(X)=g(z)=g(w_{k0}+\\sum_{j=1}^p{w_{kj}X_j})$$\n\n$$f(X)=\\beta_0+\\sum_{k=1}^K{\\beta_kg(w_{k0}+\\sum_{wkj}^p{X_j})}$$\n\n#### Weights\n\nRandomly initializing the `weights` as i.i.d. $N(0,1)$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nw1 = matrix(rnorm(2*hidden_neurons), nrow=hidden_neurons, ncol=2)\nw2 = matrix(rnorm(hidden_neurons + 1), nrow=1, ncol=(hidden_neurons + 1))\n```\n:::\n\n\nThe constant term $w_{k0}$ will shift the inflection point, we are going from a linear function to a non-linear one.\n\nThe model derives five new features by computing five different linear combinations of X, and then squashes each through an activation function $g(·)$ to transform it.\n\n$$y=\\text{intercept} + \\text{slope }X$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# use the simulation in the manipulate-data.R file\ndata %>%\n  ggplot(aes(x, y)) +\n  geom_point(shape=21, stroke=0.5, fill=\"grey\", color=\"grey20\") +\n  geom_line(linewidth=0.2) +\n  geom_smooth(method = \"lm\", color=\"steelblue\", se=F) +\n  geom_line(aes(x, sigmoid(y)), linetype=\"dashed\", color=\"steelblue\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n### FeedForward\n\nFunction to obtain predicted outputs.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfeedForward <- function(x, w1, w2, activation) {\n  output <- rep(0, length(x))\n\n  for (i in 1:length(x)) {\n    a1 = w1 %*% matrix(rbind(1, x[i]), ncol=1)\n    z1 = activation(a1)\n    a2 = w2 %*% matrix(rbind(1, z1), ncol=1)\n    output[i] = a2\n  }\n\n  return(output)\n}\n```\n:::\n\n\n### Derivative Activation\n\n> `Backpropagation algorithm` multiplies the derivative of the activation function.\n\n<center>![](images/10_derivative.png){width=\"400\"}</center>\n\nSo, it is fundamental to define the derivative of the activation function needed for computing the gradient.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nderivativeActivation <- function(x) {\n  g = (sigmoid(x) * (1 - sigmoid(x)))\n  return(g)\n}\n```\n:::\n\n\n### Model Error\n\nFunction for computing model error.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodelError <- function(x, y, w1, w2, activation) {\n  # Predictions\n  preds <- feedForward(x, w1, w2, activation)\n\n  # Error calculation\n  SSE <- sum((y - preds) ** 2)\n\n  return (SSE)\n}\n```\n:::\n\n\n### Back-Propagation\n\nFunction for computing the gradients.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbackPropagation <- function(x, y, w1, w2, activation, derivativeActivation) {\n  preds <- feedForward(x, w1, w2, activation) #predicted values\n\n  derivCost <- -2 * (y - preds) #Derivative of the cost function (first term)\n\n  dW1 <- matrix(0, ncol=2, nrow=nrow(w1)) #Gradient for w1\n  dW2 <- matrix(rep(0, length(x) * (dim(w2)[2])), nrow=length(x)) #Gradient matrix for w2\n\n  # Computing the Gradient for W2\n  for (i in 1:length(x)) {\n    a1 = w1 %*% matrix(rbind(1, x[i]), ncol=1)\n    da2dW2 = matrix(rbind(1, activation(a1)), nrow=1)\n    dW2[i,] = derivCost[i] * da2dW2\n  }\n\n  # Computing the gradient for W1\n  for (i in 1:length(x)) {\n    a1 = w1 %*% matrix(rbind(1, x[i]), ncol=1)\n    da2da1 = derivativeActivation(a1) * matrix(w2[,-1], ncol=1)\n    da2dW1 = da2da1 %*% matrix(rbind(1, x[i]), nrow=1)\n    dW1 = dW1 + derivCost[i] * da2dW1\n  }\n\n  # Storing gradients for w1, w2 in a list\n  gradient <- list(dW1, colSums(dW2))\n\n  return (gradient)\n}\n```\n:::\n\n\n### Stochastic Gradient Descent\n\nDefining our `Stochastic Gradient Descent algorithm` which will adjust our weight matrices.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSGD <- function(x, y, w1, w2, activation, derivative, learnRate, epochs) {\n  SSEvec <- rep(NA, epochs) # Empty array to store SSE values after each epoch\n  SSEvec[1] = modelError(x, y, w1, w2, activation)\n\n  for (j in 1:epochs) {\n    for (i in 1:length(x)) {\n      gradient <- backPropagation(x[i], y[i], w1, w2, activation, derivative)\n      # Adjusting model parameters for a given number of epochs\n      w1 <- w1 - learnRate * gradient[[1]]\n      w2 <- w2 - learnRate * gradient[[2]]\n    }\n    SSEvec[j+1] <- modelError(x, y, w1, w2, activation) \n    # Storing SSE values after each iteration\n    }\n    # Beta vector holding model parameters\n    B <- list(w1, w2)\n    result <- list(B, SSEvec)\n    return(result)\n}\n```\n:::\n\n\n### Modeling\n\nRunning the SGD function to obtain our optimized model and parameters:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- SGD(x, y(x), w1, w2, \n             activation = sigmoid, \n             derivative = derivativeActivation,\n             learnRate = 0.01, \n             epochs = 200)\n```\n:::\n\n\nObtaining our adjusted SSE's for each epoch:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSSE <- model[[2]]\n```\n:::\n\n\n### Model Visualization\n\nPlotting the SSE from each epoch vs number of epochs\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_data <- tibble(x=seq(0, 200, 1), SSE)\n\nggplot(model_data,aes(x, SSE)) +\n  geom_line(linewidth=0.1)+\n  geom_point(shape=21, \n             stroke=0.2, \n             fill=alpha(\"steelblue\", 0.3),\n             color=\"brown\") +\n  labs(title=\"Model SSE by Number of Epochs\",\n       x = \"Epochs\", y = \"Error\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\n### Parameters optimization\n\nExtracting our `new parameters` from our model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_w1 <- model[[1]][[1]]\nnew_w2 <- model[[1]][[2]]\n```\n:::\n\n\nComparing our `old weight` matrices against the `new ones`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow=c(1,2))\nplot(w1,new_w1)\nabline(0,1)\n\nplot(w2,new_w2)\nabline(0,1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n\n### New Predictions\n\nObtaining our new predictions using our optimized parameters.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny_pred <- feedForward(x, new_w1, new_w2, sigmoid)\n```\n:::\n\n\nPlotting training data against our model predictions\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata %>%\n  mutate(y_pred=y_pred) %>%\n  pivot_longer(cols = c(y, y_pred)) %>%\n  ggplot(aes(x, value, group=name, color=name)) +\n  geom_point(shape=21, stroke=0.5) +\n  geom_line() +\n  scale_color_discrete(type = c(\"steelblue\", \"red\")) +\n  labs(title= \"Target Response vs. Predictions\",\n       x=\"Observations\", \n       y=\"Responses\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n:::\n\n\n## Resources\n\n1.  The code used for this example is customized from `tristanoprofetto` github repository: <https://github.com/tristanoprofetto/neural-networks/blob/main/ANN/Regressor/feedforward.R>\n2.  StatQuest: Neural Networks Pt. 1: Inside the Black Box <https://www.youtube.com/watch?v=CqOfi41LfDw>\n3.  Other-Resources: <https://towardsdatascience.com/the-mostly-complete-chart-of-neural-networks-explained-3fb6f2367464>\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}