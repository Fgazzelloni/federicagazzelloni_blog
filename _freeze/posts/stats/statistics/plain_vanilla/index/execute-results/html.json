{
  "hash": "9d891da7e23296ff823474b2b9717412",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Understanding Plain Vanilla - from Scratch\"\ndate: '2024-06-28'\nsummary: \"This is a simple introduction to Artificial Neural Networks (ANN) from scratch using R outside the library boxes | used in Ch10 ISLR Book-Club.\"\nimage: featured.png\nslug: ann-plain-vanilla\ncategories:\n  - rstats\n  - modeling\n  - machine-learning\n  - artificial-neural-networks\nmargin-header: |\n     ![By giphy.com](https://i.giphy.com/media/KX5nwoDX97AtPvKBF6/giphy.gif)\n---\n\n\n## Overview\n\nIn the world of artificial intelligence, understanding the fundamentals of neural networks is essential. One of the simplest yet powerful architectures is the **Artificial Neural Network (ANN) with a single hidden layer, often referred to as a \"plain vanilla\" network**. This blog post aims to provide a comprehensive guide to building such a network from scratch, focusing on key concepts like activation functions, weights, and the backpropagation algorithm outside the library boxes.\n\n## Example of construction of Plain vanilla network architecture\n\nANN's (Artificial Neural Networks) is the simplest implementation of deep learning model architectures that mimic the human brain's neural network. The simplest form of ANN is a single-layer network, also known as a \"plain vanilla\" network. This network consists of an input layer, a hidden layer, and an output layer. The hidden layer transforms the input data into a new set of features, which are then used to predict the response variable.\n\n<center>![](images/10_02_single-layer.png){width=\"400\"}</center>\n\nIn the image above, we have some predictors $x_i$ that are fed into the hidden layer, which then transforms them into a new set of features $h_k(x)$, which are then used to predict the response variable $y$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrm(list=ls())\nsuppressMessages(library(tidyverse))\ntheme_set(theme_minimal())\n```\n:::\n\n\n### Build synthetic data\n\nLet's create a synthetic dataset to demonstrate the construction of a plain vanilla network. We will generate a dataset with 60 observations and two predictors, `x` and `y`, using the following steps:\n\n-   `Predictors` as `Uniform` distributed variables ranging between `[-2, 2]`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(100)\nx <- runif(60, min=-2, max=2)\n```\n:::\n\n\n-   `Response` Variable as function of the predictors:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny <- function(x) {\n  Y = (cos(2*x + 1))\n  return(Y)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- tibble(y=y(x),x)\nhead(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 2\n       y       x\n   <dbl>   <dbl>\n1  0.859 -0.769 \n2  0.591 -0.969 \n3  0.152  0.209 \n4 -0.829 -1.77  \n5  0.733 -0.126 \n6  0.645 -0.0649\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndata %>% summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       y                  x           \n Min.   :-0.99978   Min.   :-1.77447  \n 1st Qu.:-0.59934   1st Qu.:-0.89194  \n Median : 0.15559   Median :-0.04092  \n Mean   : 0.02628   Mean   : 0.01437  \n 3rd Qu.: 0.62358   3rd Qu.: 0.88138  \n Max.   : 0.99930   Max.   : 1.95826  \n```\n\n\n:::\n:::\n\n\n### EDA - Exploratory Data Analysis\n\nLet's visualize our synthetic data:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata %>%\n  ggplot(aes(x, y)) +\n  geom_line() +\n  geom_point() +\n  geom_hline(yintercept=0, \n             linetype=\"dashed\", color=\"grey\")\n```\n\n::: {.cell-output-display}\n![Synthetic Data](index_files/figure-html/unnamed-chunk-6-1.png){fig-align='center' width=576}\n:::\n:::\n\n\n### Parameters estimation\n\nNow that we have the data, we attempt to replicate the distribution of this data with a model using artificial neural network technique with a single hidden layer. The model will have the following parameters:\n\nModel Formula:\n\n$$f(X)=\\beta_0+\\sum_{k=1}^K{\\beta_kh_k(X)}$$\n\nWhere, $h_k$ is the hidden layer, $k=1,...,K$ the number of activations, $\\beta_0,\\beta_1,...,\\beta_K$ the coefficients , and $w_{10},...,w_{kp}$ the weights.\n\nThe hidden layer computes a number of activations.\n\n#### Number of activations\n\nInitialize the number of hidden neurons $k$, which is the number of activations in the hidden layer:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhidden_neurons = 5\n```\n:::\n\n\nThe number of hidden neurons is a hyperparameter that needs to be tuned. The more neurons, the more complex the model, but it can also lead to overfitting. You can think of a neuron as a connection between the input and output layers. The more neurons, the more connections, and the more complex the model.\n\nWe have set to have 5 hidden neurons in this example. This means that the hidden layer will compute 5 different linear combinations of the input $X$. This linear combination is then squashed through an activation function $g(·)$ to transform it.\n\nThe function that takes the input $X$ and produces an output $A_k$, the activation.\n\n#### Activation Function\n\nThe activation function is a non-linear transformation of the input layers $X_1,X_2,...,X_p$ which transform to $h_k(X)$ while learning during the training of the network. It is a function that decides whether a neuron should be activated or not by calculating the weighted sum and further adding bias with it. The purpose of the activation function is to introduce non-linearity into the output of a neuron.\n\n$$A_k=h_k(X)=g(z)$$\n\n$g(z)$ is a function used in `logistic regression` to **convert** a `linear function` **into probabilities** between zero and one.\n\nTo be more explicit, the activation function is a function that takes the input signal and generates an output signal, but takes into account a `threshold`, meaning that it will only be activated if the signal is above a certain threshold.\n\nWe have specified 5 different activation functions to compare their performance, and we will use the `sigmoid` function as the activation function in this example.\n\n#### Type of Activation functions\n\nThere are several types of activation functions, each with its own characteristics, but all have in common that they introduce non-linearity into the first level of output provided.\n\nSome of the most common types of activation functions are:\n\n-   `Sigmoid` function:\n\n$$g(z)=\\frac{e^z}{1+e^z}=\\frac{1}{1+ e^{-z}}$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsigmoid <- function(x) {\n  z = (1 / (1 + exp(-x)))\n  return(z)\n}\n```\n:::\n\n\n-   `ReLU` (Rectified Linear Unit) function:\n\n$$g(z) = max(0, z)$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrelu <- function(x) {\n  z = ifelse(x < 0, 0, x)\n  return(z)\n}\n```\n:::\n\n\n-   `SoftPlus` Function\n\n$$g(z) = \\log(1 + e^z)$$\n\nThis is a smooth approximation to the ReLU function. Firstly introduced in 2001, `Softplus` is an alternative to traditional functions because it is differentiable and its derivative is easy to demonstrate (see source: <https://sefiks.com/2017/08/11/softplus-as-a-neural-networks-activation-function/>).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsoftplus <- function(x) {\n  z = log(1 + exp(x))\n  return(z)\n}\n```\n:::\n\n\n-   Other types are:\n    -   Polynomials/Splines: $x^2$\n    -   Hyperbolic tanh: $tanh(x) = (e^x – e^-x) / (e^x + e^-x)$\n\nLet's compare the `activation functions`:\n\n\n::: {.cell layout-ncol=\"3\"}\n\n```{.r .cell-code}\ndata %>%\n  mutate(z=sigmoid(x)) %>%\n  ggplot() +\n  geom_line(aes(x, z)) +\n  ylim(0, 1)\n```\n\n::: {.cell-output-display}\n![Sigmoid function](index_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n\n```{.r .cell-code}\ndata %>%\n  ggplot() +\n  geom_line(aes(x, relu(x) * 1/2.4))\n```\n\n::: {.cell-output-display}\n![ReLU function](index_files/figure-html/unnamed-chunk-11-2.png){width=672}\n:::\n\n```{.r .cell-code}\ndata %>%\n  ggplot() +\n  geom_line(aes(x, softplus(x)))\n```\n\n::: {.cell-output-display}\n![SoftPlus function](index_files/figure-html/unnamed-chunk-11-3.png){width=672}\n:::\n:::\n\n\n::: columns\nAnd now look at how the `Sigmoid` differs from the `ReLU` function:\n\n::: column\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata %>%\n  ggplot() +\n  geom_line(aes(x, sigmoid(x))) +\n  # relu resized for comparison\n  geom_line(aes(x, relu(x) * 1/2.4))+\n  labs(y=\"Sigmoid vs ReLU\")\n```\n:::\n\n:::\n\n::: column\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Sigmoid vs ReLU](index_files/figure-html/unnamed-chunk-13-1.png){fig-align='center' width=576}\n:::\n:::\n\n:::\n:::\n\n**Our model is a model in the model:**\n\n$$f(X)=\\beta_0+\\sum_{k=1}^K{\\beta_kh_k(X)}$$ $$A_k=h_k(X)=g(z)=g(w_{k0}+\\sum_{j=1}^p{w_{kj}X_j})$$\n\n$$f(X)=\\beta_0+\\sum_{k=1}^K{\\beta_kg(w_{k0}+\\sum_{wkj}^p{X_j})}$$\n\nAs you might have noticed in the formula above, the model is a linear combination of the input $X$ and the weights $w_{kj}$, which are adjusted during the training process. The activation function $g(z)$ is applied to the linear combination of the input and weights to transform the output.\n\nLet's have a look at the weights and how they are initialized.\n\n#### Weights Initialization\n\nThe weights are the parameters of the model that are adjusted during the training process. They can be considered as the coefficients of the hidden layer model.\n\nThey are `initialized` randomly, and the model is trained to adjust these weights during the training process. The weights are adjusted using the `backpropagation algorithm`, which computes the `gradient of the loss function` with respect to the weights. Then, the weights are updated using the `gradient descent algorithm`. We will see how this is done in the next section.\n\nThe weights are initialized randomly to break the symmetry and prevent the model from getting stuck in a local minimum. In this case we use a normal distribution with a mean of 0 and a standard deviation of 1 to initialize the weights.\n\nRandomly initializing the `weights` as i.i.d. $W \\sim N(0,1)$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nw1 = matrix(rnorm(2*hidden_neurons), \n            nrow=hidden_neurons, \n            ncol=2)\nw2 = matrix(rnorm(hidden_neurons + 1), \n            nrow=1, \n            ncol=(hidden_neurons + 1))\n```\n:::\n\n\nThe constant term $w_{k0}$ will shift the inflection point, and transform a linear function to a non-linear one. The weights are adjusted during the training process to minimize the error between the predicted and actual values.\n\nThe model derives five new features by computing five different linear combinations of $X$, and then squashes each through an activation function $g(·)$ to transform it.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata %>%\n  ggplot(aes(x, y)) +\n  geom_point(shape=21, \n             stroke=0.5, \n             fill=\"grey\", \n             color=\"grey20\") +\n  geom_line(linewidth=0.2) +\n  geom_smooth(method = \"lm\", \n              color=\"steelblue\", \n              se=F) +\n  geom_line(aes(x, sigmoid(y)), \n            linetype=\"dashed\", \n            color=\"steelblue\")\n```\n\n::: {.cell-output-display}\n![In this figure is shown the attempt of the 'linear' and 'sigmoid' functions to fit our original data. The final model function, able to replicate the original pattern is the result of a continous adaptation and re-calibration of the coeffcients in the model.](index_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n### FeedForward\n\nThe meaning of `feedforward` is used to describe the process of moving the input data through the network to obtain the predicted output. The `feedforward` process is the first step in the training process of the neural network.\n\nHere is a function that computes the output of the model given the inputs: `data`, `weights`, and number of `activations`. It computes the output by multiplying the input data by the weights and applying the activation function to the result. It is a matrix multiplication (`%*%`), which is a common operation in unsupervised learning algorithms.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfeedForward <- function(x, w1, w2, activation) {\n  output <- rep(0, length(x))\n\n  for (i in 1:length(x)) {\n    a1 = w1 %*% matrix(rbind(1, x[i]), ncol=1)\n    z1 = activation(a1)\n    a2 = w2 %*% matrix(rbind(1, z1), ncol=1)\n    output[i] = a2\n  }\n\n  return(output)\n}\n```\n:::\n\n\n### Derivative Activation Function\n\nNow, that we have the `feedforward` function, we need to compute the derivative of the activation function. The `backpropagation algorithm` multiplies the derivative of the activation function.\n\n> `Backpropagation algorithm` multiplies the derivative of the activation function.\n\nHere is a recap of the definition of derivative formula, which is applied any time the output released by the activation function is met in the network. And so, a new minimum is found. It will be more clear through the end of the post.\n\n<center>![](images/10_derivative.png){width=\"400\"}</center>\n\nSo, it is fundamental to define the derivative of the activation function needed for computing the gradient. For this example, we will use the derivative of the `sigmoid` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nderivativeActivation <- function(x) {\n  g = (sigmoid(x) * (1 - sigmoid(x)))\n  return(g)\n}\n```\n:::\n\n\n### Model Error\n\nFunction for computing model error is the sum of squared errors (SSE) between the predicted and actual values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodelError <- function(x, y, w1, w2, activation) {\n  # Predictions\n  preds <- feedForward(x, w1, w2, activation)\n  # Error calculation\n  SSE <- sum((y - preds) ** 2)\n  return (SSE)\n}\n```\n:::\n\n\n### Back-Propagation\n\nSo, this is the time for computing the gradients.\n\n> What are the gradients?\n\nThe gradients are the **derivatives of the cost function** with respect to the weights. The `backpropagation algorithm` computes the gradient of the loss function with respect to the weights.\n\nThe gradients are then used to update the weights using the `gradient descent algorithm`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbackPropagation <- function(x, y, w1, w2, \n                            activation, derivativeActivation) {\n  #predicted values\n  preds <- feedForward(x, w1, w2, activation) \n  #Derivative of the cost function (first term)\n  derivCost <- -2 * (y - preds) \n  #Gradients for the weights\n  dW1 <- matrix(0, ncol=2, nrow=nrow(w1)) \n  dW2 <- matrix(rep(0, length(x) * (dim(w2)[2])), nrow=length(x)) \n\n  # Computing the Gradient for W2\n  for (i in 1:length(x)) {\n    a1 = w1 %*% matrix(rbind(1, x[i]), ncol=1)\n    da2dW2 = matrix(rbind(1, activation(a1)), nrow=1)\n    dW2[i,] = derivCost[i] * da2dW2\n  }\n\n  # Computing the gradient for W1\n  for (i in 1:length(x)) {\n    a1 = w1 %*% matrix(rbind(1, x[i]), ncol=1)\n    da2da1 = derivativeActivation(a1) * matrix(w2[,-1], ncol=1)\n    da2dW1 = da2da1 %*% matrix(rbind(1, x[i]), nrow=1)\n    dW1 = dW1 + derivCost[i] * da2dW1\n  }\n\n  # Storing gradients for w1, w2 in a list\n  gradient <- list(dW1, colSums(dW2))\n\n  return (gradient)\n}\n```\n:::\n\n\n### Stochastic Gradient Descent\n\nDefining our `Stochastic Gradient Descent algorithm` which will adjust our weight matrices.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSGD <- function(x, y, w1, w2, activation, derivative, learnRate, epochs) {\n  SSEvec <- rep(NA, epochs) # Empty array to store SSE values after each epoch\n  SSEvec[1] = modelError(x, y, w1, w2, activation)\n\n  for (j in 1:epochs) {\n    for (i in 1:length(x)) {\n      gradient <- backPropagation(x[i], y[i], w1, w2, activation, derivative)\n      # Adjusting model parameters for a given number of epochs\n      w1 <- w1 - learnRate * gradient[[1]]\n      w2 <- w2 - learnRate * gradient[[2]]\n    }\n    SSEvec[j+1] <- modelError(x, y, w1, w2, activation) \n    # Storing SSE values after each iteration\n    }\n    # Beta vector holding model parameters\n    B <- list(w1, w2)\n    result <- list(B, SSEvec)\n    return(result)\n}\n```\n:::\n\n\n### Modeling\n\nRunning the SGD function to obtain our optimized model and parameters:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- SGD(x, y(x), w1, w2, \n             activation = sigmoid, \n             derivative = derivativeActivation,\n             learnRate = 0.01, \n             epochs = 200)\n```\n:::\n\n\nObtaining our adjusted SSE's for each epoch:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSSE <- model[[2]]\n```\n:::\n\n\n### Model Visualization\n\nPlotting the SSE from each epoch vs number of epochs\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_data <- tibble(x=seq(0, 200, 1), SSE)\n\nggplot(model_data,aes(x, SSE)) +\n  geom_line(linewidth=0.1)+\n  geom_point(shape=21, \n             stroke=0.2, \n             fill=alpha(\"steelblue\", 0.3),\n             color=\"brown\") +\n  labs(title=\"Model SSE by Number of Epochs\",\n       x = \"Epochs\", y = \"Error\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\n### Parameters optimization\n\nExtracting our `new parameters` from our model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_w1 <- model[[1]][[1]]\nnew_w2 <- model[[1]][[2]]\n```\n:::\n\n\nComparing our `old weight` matrices against the `new ones`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow=c(1,2))\nplot(w1,new_w1)\nabline(0,1)\n\nplot(w2,new_w2)\nabline(0,1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n\n### New Predictions\n\nObtaining our new predictions using our optimized parameters.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny_pred <- feedForward(x, new_w1, new_w2, sigmoid)\n```\n:::\n\n\nPlotting training data against our model predictions\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata %>%\n  mutate(y_pred=y_pred) %>%\n  pivot_longer(cols = c(y, y_pred)) %>%\n  ggplot(aes(x, value, group=name, color=name)) +\n  geom_point(shape=21, stroke=0.5) +\n  geom_line() +\n  scale_color_discrete(type = c(\"steelblue\", \"red\")) +\n  labs(title= \"Target Response vs. Predictions\",\n       x=\"Observations\", \n       y=\"Responses\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n:::\n\n\n## Resources\n\n1.  The code used for this example is customized from `tristanoprofetto` github repository: <https://github.com/tristanoprofetto/neural-networks/blob/main/ANN/Regressor/feedforward.R>\n2.  StatQuest: Neural Networks Pt. 1: Inside the Black Box <https://www.youtube.com/watch?v=CqOfi41LfDw>\n3.  Other-Resources: <https://towardsdatascience.com/the-mostly-complete-chart-of-neural-networks-explained-3fb6f2367464>\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}